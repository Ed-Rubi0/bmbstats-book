[
["index.html", "bmbstats: bootstrap magnitude-based statistics for sports scientists Welcome R and R packages License", " bmbstats: bootstrap magnitude-based statistics for sports scientists Mladen Jovanovic 2020-05-25 Welcome The aim of this book is to provide an overview of the three classes of tasks in the statistical modeling: description, prediction and causal inference (Hernán, Hsu, and Healy 2019). Statistical inference is often required for all three tasks. Short introduction to frequentist null-hypothesis testing, Bayesian estimation and bootstrap are provided. Special attention is given to the practical significance with the introduction of magnitude-based estimators and statistical inference by using the concept of smallest effect size of interest (SESOI). Measurement error is discussed with the particular aim of interpreting individual change scores. In the second part of this book, common sport science problems are introduced and analyzed with the bmbstats package. This book, as well as the bmbstats package are currently in development phase. Please be free to contribute pull request at GitHub bmbstats package https://github.com/mladenjovanovic/bmbstats bmbstats book https://github.com/mladenjovanovic/bmbstats-book R and R packages This book is fully reproducible and was written in R (Version 4.0.0; R Core Team 2020) and the R-packages bayestestR (Version 0.6.0; Makowski, Ben-Shachar, and Lüdecke 2019), bmbstats (Version 0.0.0.9000; Jovanović 2020), bookdown (Version 0.18; Xie 2016), boot (Version 1.3.25; Davison and Hinkley 1997), carData (Version 3.0.3; Fox, Weisberg, and Price 2019), caret (Version 6.0.86; Kuhn 2020), cowplot (Version 1.0.0; Wilke 2019), directlabels (Version 2020.1.31; Hocking 2020), dplyr (Version 0.8.5; Wickham, François, et al. 2020), effects (Version 4.1.4; Fox and Weisberg 2018; Fox 2003; Fox and Hong 2009), forcats (Version 0.5.0; Wickham 2020), ggplot2 (Version 3.3.0; Wickham 2016), ggridges (Version 0.5.2; Wilke 2020), ggstance (Version 0.3.4; Henry, Wickham, and Chang 2020), kableExtra (Version 1.1.0; Zhu 2019), knitr (Version 1.28; Xie 2015), lattice (Version 0.20.41; Sarkar 2008), markdown (Version 1.1; Allaire et al. 2019), Metrics (Version 0.1.4; Hamner and Frasco 2018), minerva (Version 1.5.8; Albanese et al. 2012a), pdp (Version 0.7.0; B. M. Greenwell 2017a), psych (Version 1.9.12.31; Revelle 2019), purrr (Version 0.3.4; Henry and Wickham 2020), readr (Version 1.3.1; Wickham, Hester, and Francois 2018), rpart (Version 4.1.15; Therneau and Atkinson 2019), stringr (Version 1.4.0; Wickham 2019), tibble (Version 3.0.1; Müller and Wickham 2020), tidyr (Version 1.1.0; Wickham and Henry 2020), tidyverse (Version 1.3.0; Wickham, Averick, et al. 2019), vip (Version 0.2.2; Greenwell, Boehmke, and Gray 2020), and visreg (Version 2.6.1; Breheny and Burchett 2017). License This work, as a whole, is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The code contained in this book is simultaneously available under the MIT license; this means that you are free to use it in your own packages, as long as you cite the source. References "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The real world is very complex and uncertain. In order to help in understanding it and to predict its behavior, we create maps and models (Page 2018; Weinberg and McCann 2019). One such tool are statistical models, representing a simplification of the complex and ultimately uncertain reality, in the hope of describing it, understanding it, predicting its behavior, and help in making decisions and interventions (Hernán, Hsu, and Healy 2019; Lang, Sweet, and Grandfield 2017; McElreath 2015; Pearl and Mackenzie 2018). In the outstanding statistics book “Statistical Rethinking” (McElreath 2015, 19), the author stresses the distinction between Large World and Small World, described initially by Leonard Savage (Binmore 2011; Gigerenzer, Hertwig, and Pachur 2015; Savage 1972): \"All statistical modeling has these same two frames: the small world of the model itself and the large world we hope to deploy the model in. Navigating between these two worlds remains a central challenge of statistical modeling. The challenge is aggravated by forgetting the distinction. The small world is the self-contained logical world of the model. Within the small world, all possibilities are nominated. There are no pure surprises, like the existence of a huge continent between Europe and Asia. Within the small world of the model, it is important to be able to verify the model’s logic, making sure that it performs as expected under favorable assumptions. Bayesian models have some advantages in this regard, as they have reasonable claims to optimality: No alternative model could make better use of the information in the data and support better decisions, assuming the small world is an accurate description of the real world. The large world is the broader context in which one deploys a model. In the large world, there may be events that were not imagined in the small world. Moreover, the model is always an incomplete representation of the large world, and so will make mistakes, even if all kinds of events have been properly nominated. The logical consistency of a model in the small world is no guarantee that it will be optimal in the large world. But it is certainly a warm comfort.\" Creating “Small Worlds” relies heavily on making and accepting numerous assumptions, both known and unknown, as well as prior expert knowledge, which is ultimately incomplete and fallible. Because all statistical models require subjective choices (Gelman and Hennig 2017), there is no objective approach to make “Large World” inferences. It means that it must be us who make the inference, and claims about the “Large World” will always be uncertain. Additionally, we should treat statistical models and statistical results as being much more incomplete and uncertain than the current norm (Amrhein, Trafimow, and Greenland 2019). We must accept the pluralism of statistical models and models in general (Mitchell 2012, 2002), move beyond subjective-objective dichotomy by replacing it with virtues such as transparency, consensus, impartiality, correspondence to observable reality, awareness of multiple perspectives, awareness of context-dependence, and investigation of stability (Gelman and Hennig 2017). Finally, we need to accept that we must act based on cumulative knowledge rather than solely rely on single studies or even single lines of research (Amrhein, Trafimow, and Greenland 2019). This discussion is the topic of epistemology, scientific inference, and philosophy of science, thus far beyond the scope of the present book (and the author). Nonetheless, it was essential to convey that statistical modeling is a process of creating the “Small Worlds” and deploying it in the “Large World”. There are three main classes of tasks that the statistical model is hoping to achieve: description, prediction, and causal inference (Hernán, Hsu, and Healy 2019). The following example will help in differentiating between these three classes of tasks. Consider a king who is facing a drought who must decide whether to invest resources in rain dances. The queen, upon seeing some rain clouds in the sky, must decide on whether to carry her umbrella or not. Young prince, who likes to gamble during his hunting sessions, is interested in knowing what region of his father’s vast Kingdom receives the most rain. All three would benefit from an empirical study of rain, but they have different requirements of the statistical model. The king requires causality: Do rain dances cause rain? The queen requires prediction: Does it look likely enough to rain for me to ask my servants to get my umbrella? The prince requires simple quantitative summary description: have I put my bets on the correct region? The following sections will provide an overview of the three classes of tasks in the statistical modeling. Data can be classified as being on one of four scales: nominal, ordinal, interval or ratio and description, prediction and causal techniques differ depending on the scales utilized. For the sake of simplicity and big picture overview, only examples using ratio scale are to be considered in this book. References "],
["description.html", "Chapter 2 Description 2.1 Comparing two independent groups 2.2 Comparing dependent groups 2.3 Describing relationship between two variables 2.4 Advanced uses", " Chapter 2 Description Description provides quantitative summary of the acquired data sample. These quantitative summaries are termed descriptive statistics or descriptive estimators and are usually broken down into two main categories: measures of central tendency, and measures of spread / dispersion. The stance taken in this book is that descriptive statistics involve all quantitative summaries (or aggregates) that are used to describe data without making predictive or causal claims. For example, linear regression between two variables can be used as a descriptive tool if the aim is to measure linear association between two variables, but it can be also used in predictive and causal tasks. Effect sizes such as change, percent change or Cohen's d represent descriptive statistics used to compare two or more groups, and are commonly used in causal tasks to estimate average causal effect of the treatment. To provide further explanation of the descriptive statistics, three common descriptive tasks in sport science are given as examples: (1) comparing two independent groups, (2) comparing two dependent groups, (3) measuring association between two variables. 2.1 Comparing two independent groups Imagine we carried collection of body height measurements and we obtained N=100 observations using N=50 female and N=50 male subjects. Collected data is visualized in Figure 2.1. Figure 2.1: Common techniques to visualize independent groups observations. Before any analysis takes place, it is always a good practice to visualize the data first. Ideally, we want to visualize the complete data set, rather than only provide descriptive summaries, such as means. A. Simple scatter-plot with jitter to avoid overlap between the points. B. Mean and standard deviation as error bars. C. Box-plot. Horizontal line represents median, or 50th percentile, whereas boxes represent 25th and 75th percentile. Vertical lines usually represent min and max, although they can extend up to 1.5xIQR (inter-quartile range) with point outside of that interval plotted as outliers. D. Violin plots representing double-side density plots with 25th, 50th and 75th percentile lines. E. Density plots indicating sample distribution. F. Raincloud plot (Allen et al. 2019, 2018) which combine kernel density plots as clouds with accompanying 25th, 50th and 75th percentile lines, mean±SD error bars and jittered points as rain Commonly provided descriptive statistics for each group can be found in the Table 2.1. Mean, median and mode are common measures of central tendencies. Standard deviation (SD), median absolute difference (MAD), inter-quartile range (IQR), min, max and range are common measures of spread or dispersion. Percent coefficient of variation (% CV) is also a measure of dispersion, but standardized1 which allows comparison of variables that are on different scales. Skewness (skew) is usually described as a measure of a symmetry. A perfectly symmetrical data set will have a skewness of 0. Kurtosis measures the tail-heaviness of the distribution. More in depth discussion of descriptive estimators, particularly robust estimators (Rousselet, Pernet, and Wilcox 2017; Wilcox, Peterson, and McNitt-Gray 2018; Wilcox and Rousselet 2017; Wilcox 2016) is beyond the topic of this short overview. Table 2.1: Common descriptive statistics or estimators Estimator Male Female n 50.00 50.00 mean (cm) 175.90 163.18 SD (cm) 9.32 8.20 % CV 5.30 5.02 median (cm) 176.30 164.00 MAD (cm) 9.52 8.86 IQR (cm) 11.24 11.67 mode (cm) 176.26 164.94 min (cm) 154.24 145.59 max (cm) 193.90 181.12 range (cm) 39.66 35.53 skew 0.08 0.08 kurtosis -0.53 -0.69 2.1.1 Sample mean as the simplest statistical model In the Introduction of this book, statistical models are defined as “Small Worlds” or simplifications of the complex and uncertain reality. From this perspective, sample mean can be considered the simplest statistical model. With this estimator we are representing all of the data points with one quantitative summary (i.e. aggregate). However, how do we choose an estimate that represents the sample the best? Estimate that has the minimal error is selected as the optimal representative. Error is defined using a loss function that penalizes difference between the model estimate or prediction (\\(\\hat{y_i}\\)) and observations (\\(y_i\\)) (Equation (2.1)). The difference between model prediction (\\(\\hat{y_i}\\)) and observations (\\(y_i\\)) is called residual. \\[ \\begin{equation} Loss \\: function = f(observed, predicted) \\tag{2.1} \\end{equation} \\] Two most common loss functions are absolute loss (also referred to as \\(L1\\)) (Equation (2.2)) and quadratic loss (also referred to as squared errors or \\(L2\\)) (Equation (2.3)). Please refer to section Sample mean as the simplest predictive model in Prediction chapter for more examples. \\[ \\begin{equation} absolute \\: loss = \\mid{\\hat{y_i} - y_i\\mid} \\tag{2.2} \\end{equation} \\] \\[ \\begin{equation} quadratic \\: loss = (\\hat{y_i} - y_i)^2 \\tag{2.3} \\end{equation} \\] Cost function is an aggregate of the loss function (Equation (2.4)). \\[ \\begin{equation} Cost \\: function = f(Loss \\: function (observed, predicted)) \\tag{2.4} \\end{equation} \\] Since loss function is defined on a data point (i.e. \\(y_i\\)), we need to aggregate losses into a single metric. This is done with a cost function, usually using sum or mean. One such cost function is root-mean-square-error (RMSE) (Equation (2.5)). RMSE takes the square root of the mean of the quadratic loss (note the \\((\\hat{y_i} - y_i)^2\\) in the RMSE equation, which represent quadratic loss). RMSE thus represents a measure of the model fit, or how good the model fits the data. Lower RMSE means lower error and thus a better fit. \\[ \\begin{equation} RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}(\\hat{y_i} - y_i)^2} \\tag{2.5} \\end{equation} \\] By using body height data from the female group, we can search for a body height estimate that minimizes the RMSE (Figure 2.2). That body height estimate would be considered the best representative of the sample, and thus the simplest statistical model. Figure 2.2: Sample mean as the simplest statistical model. A. Dashed line represents the estimate, in this case the mean of the sample. Vertical line represent residuals between estimate and observed values. B. Each estimate has a RMSE value. Central tendency estimate with the lowest RMSE value is the sample mean. C. Similar to panel A, this panel depicts residuals for a central tendency estimate with higher RMSE As the result of this search, the body height estimate that minimizes the error is 163.18cm, and accompanying RMSE is equal to 8.12cm. As it can be read from the Table 2.1, this optimal body height estimate is equal to calculated sample mean. Standard deviation of the sample is equal to RMSE2. From statistical modeling perspective, sample mean can be considered sample estimate that minimizes the sample SD, and sample SD can be seen as the measure of the model fit. This search for the optimal estimate that minimizes the cost function can be expanded to other statistical models. For example, linear regression can be seen as a search for the line that minimizes RMSE. This approach of estimating model parameters or estimators belongs to the family of ordinary least squares (OLS) methods, although there are other approaches such as maximum likelihood estimation (MLE) which will be discussed in [Statistical inference] section (Foreman 2014). The solutions to some of these models can be found analytically3, but for some there is no analytic solution and computational approaches must be utilized. These computation approaches are referred to as optimization algorithms. The example given here involves only one parameter that needs to be optimized, in this case body height estimate, but real-life problems involve numerous parameters. The simple search through parameters state-space would take forever when it comes to problems involving more than only a few parameters. Algorithms that solve this computational problems are numerous, out of which the most popular ones are gradient descent, and Markov Chain Monte-Carlo (MCMC), which is utilized in Bayesian inference (will be discussed in [Bayesian perspective] section). The take-home message from this short interlude is that even the simple descriptive statistics can be seen as statistical models. If we take another cost function, for example mean absolute error (MAE) (Equation (2.6)) and if we optimize so that the sample central tendency estimate minimizes MAE, we will get median estimator. \\[ \\begin{equation} MAE = \\frac{1}{n}\\Sigma_{i=1}^{n}\\mid{\\hat{y_i} - y_i\\mid} \\tag{2.6} \\end{equation} \\] We will expand this discussion about loss functions, cost functions, and performance metrics in Sample mean as the simplest predictive model section. For more information please check the package Metrics (Hamner and Frasco 2018) and the following references (Botchkarev 2019; Chai and Draxler 2014; Willmott and Matsuura 2005; Barron 2019). 2.1.2 Effect Sizes Besides describing groups, we are often interested in comparing them. In order to achieve this task, a collection of estimators termed effect size statistics are utilized. Effect size can be defined in a narrow sense or in a broad sense. Briefly, the narrow sense refers to a family of standardized measures such as Cohen’s d, while the broad sense refers to any measure of interest, standardized or not. The approach to effect size statistics in this book is thus in a broad sense of the definition, in which all group comparison estimators are considered effect sizes statistics. In order to estimate effect sizes, one group needs to be considered baseline or control. The most common effect size statistics can be found in the Table 2.2 where female body height is considered baseline and compared with male body height. Table 2.2: Effect size statistics for estimating differences between two independent groups Difference (cm) SDdiff (cm) % CVdiff % Difference Ratio Cohen’s d CLES OVL 12.73 12.41 97.55 7.8 1.08 -1.45 0.85 0.47 Difference, or mean difference (mean diff) is calculated by subtracting group means. Using body height as an example, the mean diff between males and females is calculated by using the following equation (2.7): \\[ \\begin{equation} \\begin{split} mean_{difference} &amp;= mean_{males} - mean_{females} \\\\ mean_{males} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}male_i \\\\ mean_{females} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}female_i \\end{split} \\tag{2.7} \\end{equation} \\] % CVdiff, or percent coefficient of variation of the difference is the standard deviation of the difference (SDdiff - explained shortly) divided by mean diff (Equation (2.8)): \\[ \\begin{equation} \\%\\;CV_{difference} = 100\\times\\frac{SD_{difference}}{mean_{difference}} \\tag{2.8} \\end{equation} \\] % Difference, or mean percent difference is calculated by dividing mean diff with the mean of the baseline group, in this case the female group, multiplied by 100 (Equation (2.9)): \\[ \\begin{equation} mean_{\\% difference} = 100\\times\\frac{mean_{difference}}{mean_{females}} \\tag{2.9} \\end{equation} \\] Mean ratio, as its name suggests, is simple ratio between the two means (Equation (2.10)): \\[ \\begin{equation} mean_{ratio} = \\frac{mean_{males}}{mean_{females}} \\tag{2.10} \\end{equation} \\] Cohen's d represent standardized effects size and thus preferable effect size statistic. For this reason, Cohen's d is commonly written as ES, short of effect size. Cohen's d for the independent groups is calculated by dividing mean diff (Equation (2.7)) with pooled standard deviation ((2.11)). \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{difference}}{SD_{pooled}} \\tag{2.11} \\end{equation} \\] Pooled standard deviation represents combined standard deviations from two groups (Equation (2.12)). \\[ \\begin{equation} SD_{pooled} = \\sqrt{\\frac{(n_{males} - 1) SD_{males}^2 + (n_{females} - 1) SD_{females}^2}{n_{males}+n_{females} - 2}} \\tag{2.12} \\end{equation} \\] Why Cohen's d should be used instead of other effect size estimators can be demonstrated by a simple example, coming from a study by Buchheit and Rabbani (2014). In this study, authors examined the relationship between the performance in the YoYo Intermittent Recovery Test Level 1 (YoYoIR1) and the 30-15 Intermittent Fitness Test (30-15IFT), and compared the sensitivity of both tests to the training. Although this study used two dependent groups (Pre-training and Post-training), the rationale can be applied to the topic of estimating effect sizes between the two independent groups. Table 2.3 contains Pre-training results and the effect sizes estimated with percent change4 and Cohen's d. Table 2.3: Training intervention effect sizes for YoYoIR1 and 30-15IFT. Modified from Buchheit and Rabbani (2014) Test Pre-training % Change Cohen’s d YoYoIR1 1031 ± 257 m 35 % 1.2 30-15IFT 17.4 ± 1.1 kmh-1 7 % 1.1 Since YoYoIR1 and 30-15IFT utilize different scales (total meters covered and velocity reached respectively), percent change estimator is not a good choice to compare the effect sizes between the two tests5. Since Cohen's d is standardized estimator, it should be used when comparing tests or measures that are at different scales. After estimating effect sizes, the question that naturally follows up is the question of magnitude. In other words - “how big is the effect?”. Since Cohen's d is standardized estimator, it allows for establishment of qualitative magnitude thresholds. Based on the original work by Cohen (Cohen 1988), Hopkins (Hopkins 2006; Hopkins et al. 2009) suggested the following magnitudes of effect (Table (2.4). According to the Table (2.4, the body height difference between males and females would be considered large, as well as changes in both YoYoIR1 and 30-15IFT. Table 2.4: Magnitudes of effect Magnitude of effect Trivial Small Moderate Large Very Large Nearly Perfect Cohen’s d 0 - 0.2 0.2 - 0.6 0.6 - 1.2 1.2 - 2.0 2.0 - 4.0 &gt; 4.0 Cohen's d, as well as associated magnitudes of effect, are commonly hard to interpret by non-statistically trained professionals (e.g. coaches). McGraw and Wong (1992) suggested common language effect size (CLES) estimator instead, which could be more intuitive to understand. CLES represents the probability that an observation sampled at random from one group will be greater than an observation sampled at random from other group. For example, if we take random male and random female from our two groups and repeat that 100 times6, how many times a male would be taller than a female (Figure 2.3)? Figure 2.3: Drawing random 100 pairs to estimate probability of males being taller than females. A. Scatterplot of 100 pairs drawn at random from two samples. Since we are comparing paired males and females, lines can be drawn between each of 100 draws. Blue line indicates taller male, while orange line indicates taller female. B. Distribution of the difference between males and females for each of 100 pairs drawn By using simple counting from 100 random paired samples, males are taller in 85 cases, or 85%. By using probability, that is equal to 0.85. In other words, if I blindfoldedly, randomly select a male and a female from the two groups and if I bet that the male is taller, I would be correct 85% of the time. CLES can be estimated using brute-force computational method, or algebraic method. Brute-force method involves generating all possible pair-wise combinations from two groups, and in our example that is equal to \\(50 \\times 50 = 2500\\) cases, and then simply counting in how many cases males are taller than females. This method can become very computationally intensive for groups with large sample number. Algebraic method, on the other hand, assumes normal distribution of the observations in the groups, and estimates standard deviation of the difference (SDdiff) (Equation (2.13)). Note that standard deviation of the all pairwise differences estimated with brute-force method would be very similar to algebraically derived SDdiff. \\[ \\begin{equation} SD_{difference} = \\sqrt{SD_{males}^{2} + SD_{females}^{2}} \\tag{2.13} \\end{equation} \\] Algebraically, CLES is then derived assuming normal distribution (where mean of the distribution is equal to mean diff between the groups, and standard deviation of the distribution is equal to SDdiff) by calculating probability of the difference scores higher than zero (see Figure 2.3B for a visual representation). Table 2.2 contains algebraically computed CLES estimate. CLES equivalent is utilized as a performance metric in class prediction tasks, termed area under curve (AUC), where 0.5 is a predictive performance equal to a random guess, and 1 is perfect predictive separation between the two classes (James et al. 2017; Kuhn and Johnson 2018). Overlap (OVL) estimator represents the overlap between the two sample distributions. Providing that samples are identical, the OVL is equal to 1. Providing there is complete separation between the two samples, then OVL is equal to 0 (Figure 2.4A). OVL can be estimated with brute-force computational methods (which doesn’t make assumptions regarding sample distribution) and with algebraic methods that make normality assumptions. Since Cohen's d, CLES and OVL are mathematically related, it is possible to convert one to another (assuming normal distribution of the samples and equal SD between the two groups for the OVL estimation). Figure 2.4B depicts relationship between the Cohen's d, CLES, and OVL. Figure 2.4C depicts relationship between the CLES and OVL. Figure 2.4: Relationship between the Cohen's d, CLES, and OVL. A. Visual display of the samples of varying degrees of separations, and calculated Cohen's d, CLES, and OVL. B. Relationship between the CLES and OVL to the Cohen's d. C. Relationship between the CLES and OVL Table 2.5 contains Cohen's d magnitudes of effect with accompanying estimated CLES and OVL thresholds. Table 2.5: Magnitudes of effect for CLES and OVL estimated using Cohen's d Magnitude of effect Trivial Small Moderate Large Very Large Nearly Perfect Cohen’s d 0.0 - 0.2 0.2 - 0.6 0.6 - 1.2 1.2 - 2.0 2.0 - 4.0 &gt; 4.0 CLES 0.50 - 0.56 0.56 - 0.66 0.66 - 0.80 0.80 - 0.92 0.92 - 1.00 1.00 OVL 1.00 - 0.92 0.92 - 0.76 0.76 - 0.55 0.55 - 0.32 0.32 - 0.05 0.00 2.1.3 The Smallest Effect Size Of Interest According to Cohen (1988), the qualitative magnitude thresholds from Table 2.5 are “arbitrary conventions, recommended for use only when no better basis for estimating the effect size is available” (p. 12). But what if practitioners a priori know what is the minimal important effect size and are interested in judging the practical or clinical significance (Sainani 2012) of the results (in this case difference between the groups)? In other words, the smallest effect size of interest (SESOI)7. There is no single way to approach definition and estimation of SESOI, but it usually tends to be based on either the known measurement error (ME) (e.g. the minimum detectable effect size), or the effect size that is large enough to be practically meaningful (e.g. the minimal important difference, or the smallest worthwhile change) (Anvari and Lakens 2019; Hopkins 2004, 2015; King 2011; Lakens, Scheel, and Isager 2018; Turner et al. 2015; Swinton et al. 2018; Caldwell and Cheuvront 2019). In this book, statistical models and estimators that utilize SESOI are referred to as magnitude-based. To introduce magnitude-based estimators, consider ±2.5cm to be body height SESOI8, or the difference that would be practically significant. In other words, individuals with height difference within ±2.5cm would be considered practically equivalent (from the minimal important effect perspective), or it might be hard to detect this difference with a quick glance (from minimum detectable effect perspective). The simplest magnitude-based statistics would be mean diff divided by SESOI (Difference to SESOI) (Equation (2.14)). This estimator, similar to other standardized estimators (e.g. Cohen's d) allows comparison of variables at different scales, but it would also give more insight into differences from practical significance perspective. \\[ \\begin{equation} Difference\\;to\\;SESOI = \\frac{mean_{difference}}{SESOI_{upper} - SESOI_{lower}} \\tag{2.14} \\end{equation} \\] Second magnitude-based statistic is SDdiff divided by SESOI (SDdiff to SESOI) (Equation (2.15)). This estimator, similar to % CVdiff, would answer how variable are the differences compared to SESOI. \\[ \\begin{equation} SDdiff\\;to\\;SESOI = \\frac{SD_{difference}}{SESOI_{upper} - SESOI_{lower}} \\tag{2.15} \\end{equation} \\] Similarly, CLES estimator can become magnitude-based by utilizing SESOI. Rather than being interested in probability of a random male being taller than a random female (out of the two sample groups), we might be interested in estimating how probable are lower, equivalent, and higher (or usually defined as harmful, trivial, and beneficial) differences defined by SESOI. Practically equivalent (trivial) differences are differences ranging from \\(SESOI_{lower}\\) to \\(SESOI_{upper}\\), while everything over \\(SESOI_{upper}\\) is higher (or beneficial) difference and everything lower than \\(SESOI_{lower}\\) is lower (or harmful) difference. Using brute-force computational method and drawing all pair-wise combinations from the two groups (50x50 = 2500 cases), and using ±2.5cm SESOI as a practically equivalent difference9, we can estimate probabilities of lower (pLower), equivalent (pEquivalent) and higher difference (pHigher) by calculating proportion of cases within each magnitude band (Figure 2.5). Figure 2.5: Pairwise comparison of males and females to estimate probability of lower, equivalent, and higher magnitude of difference. A. Scatterplot of all pair-wise combinations (50x50 = 2500), drawn at random out of two samples. Since we are comparing paired males and females, lines can be drawn between each of 2500 draws. Blue line indicates males taller than females higher than SESOI, equivalent lines indicates pairs with a height difference less or equal to SESOI, while orange line indicates females taller than males higher than SESOI. B. Distribution of the differences between males and females for all 2500 pair-wise combinations. Grey band indicates SESOI. Surface of the distribution over SESOI (blue color) indicates probability of randomly selected male being taller than a randomly selected female (pHigher), with a height difference of at least SESOI magnitude. Surface of the distribution under SESOI (orange color) indicates probability of randomly selected female being taller than a randomly selected female (pLower), with a height difference of at least SESOI magnitude. Grey surface area indicates probability of randomly selecting male and female with a height difference within SESOI band (pEquivalent) Table 2.6 contains estimated probabilities of observing lower, equivalent, and higher differences in height between the randomly selected male and female using brute-force computational method and algebraic method. These estimates answer the following question “If I compare random male and random female from my sample, how probable are lower/equivalent/higher magnitudes of difference in height?”. Asking such a magnitude-based question regarding the random individual difference represents a form of prediction question and predictive task. In this book, such questions are answered with magnitude-based prediction approaches. Table 2.6: Estimated probabilities of observing lower, equivalent, and higher differences in height Method pLower pEquivalent pHigher brute-force 0.11 0.096 0.794 algebraic 0.11 0.095 0.795 It is common to represent means as systematic component or fixed effect (e.g. mean difference), and variability around the mean (i.e. SDdiff) as stochastic component or random effect. It is unfortunate that the common statistical modeling and analysis, particularly in sport science, takes the stance of approaching and treating between-individual variation as random error. The approach suggested in this book complements group-based or average-based statistics with magnitude-based predictions that aim to help in answering individual-based questions, common to sport practitioners. Table 2.7 contains discussed magnitude-based estimators that can complement common effect size statistics (Table 2.2) when comparing two independent groups. Table 2.7: Magnitude-based effect size statistics for estimating difference between two independent groups SESOI lower (cm) SESOI upper (cm) Difference to SESOI SDdiff to SESOI pLower pEquivalent pHigher -2.5 2.5 2.55 2.48 0.11 0.1 0.79 2.2 Comparing dependent groups As an example of dependent or paired groups descriptive analysis, let’s consider the simple Pre-test and Post-test design. We have given training intervention to a group of N=20 males involving bench-press training. Training intervention involved performing bench pressing two times a week for 16 weeks. One-repetition-maximum (1RM) in the bench press was performed before (Pre-test) and after (Post-test) training intervention. Table 2.8 contains individual Pre-test and Post-test scores, as well as the Change in the bench press 1RM. Table 2.8: Individual Pre and Post scores, as well as Change in the bench press 1RM Athlete Pre-test (kg) Post-test (kg) Change (kg) Athlete 01 111.89 116.55 4.66 Athlete 02 96.04 100.67 4.64 Athlete 03 106.29 109.60 3.31 Athlete 04 99.74 100.97 1.23 Athlete 05 92.44 110.93 18.48 Athlete 06 96.39 96.93 0.54 Athlete 07 93.82 114.54 20.72 Athlete 08 105.87 113.40 7.53 Athlete 09 82.61 79.65 -2.96 Athlete 10 103.66 95.64 -8.01 Athlete 11 86.23 77.18 -9.05 Athlete 12 100.60 114.75 14.14 Athlete 13 93.16 90.87 -2.29 Athlete 14 111.42 115.27 3.85 Athlete 15 98.59 89.73 -8.86 Athlete 16 108.85 114.92 6.07 Athlete 17 93.81 91.05 -2.76 Athlete 18 93.83 92.14 -1.69 Athlete 19 107.94 116.72 8.78 Athlete 20 101.65 91.81 -9.84 The results of this simple Pre-test and Post-test design can be described in multiple ways. Here, I will present the three most common approaches. 2.2.1 Describing groups as independent The simplest analysis involve descriptive statistics assuming groups as independent. Table 2.9 contains descriptive statistics applied to Pre-test, Post-test and Change scores as independent. Figure 2.6 visualizes the scores using three raincloud plots. Table 2.9: Descriptive analysis of the Pre-test, Post-test, and Change as independent samples Estimator Pre-test Post-test Change n 20.00 20.00 20.00 mean (kg) 99.24 101.67 2.42 SD (kg) 8.06 12.85 8.71 % CV 8.12 12.64 359.18 median (kg) 99.16 100.82 2.27 MAD (kg) 8.42 15.71 7.60 IQR (kg) 12.16 22.97 9.25 mode (kg) 96.36 113.59 2.89 min (kg) 82.61 77.18 -9.84 max (kg) 111.89 116.72 20.72 range (kg) 29.28 39.54 30.57 skew -0.17 -0.30 0.45 kurtosis -0.89 -1.32 -0.65 Figure 2.6: Raincloud plots of the Pre-test, Post-test and Change scores in the bench press 1RM. A. Distribution of the Pre-test and Post-test scores. B. Distribution of the Change score 2.2.2 Effect Sizes Table 2.10 contains the most common effect size estimators utilized when describing change in the Pre-Post paired design. The terminology utilized in this book differentiates between the difference which is used in independent groups and the change which is used in paired or dependent groups Table 2.10: Effect size statistics for estimating change in two dependent groups Change (kg) SDchange (kg) % CVchange % Change Ratio Cohen’s d CLES OVL 2.42 8.71 359.18 2.33 1.02 0.3 0.56 0.88 Change, or mean change is calculated by taking average of the change score (Equation (2.16)). Change score is simple difference between Pre-test and Post-test. \\[ \\begin{equation} \\begin{split} mean_{change} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(post_{i}-pre_{i}) \\\\ mean_{change} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}change_{i} \\\\ change_{i} &amp;= post_{i}-pre_{i} \\end{split} \\tag{2.16} \\end{equation} \\] SDchange, or standard deviation of the change is a simple standard deviation of the change (Equation (2.17)). It represents a measure of dispersion of the change scores. \\[ \\begin{equation} SD_{change} = \\sqrt{\\frac{1}{n-1}\\Sigma_{i=1}^{n}(change_i -mean_{change})^2} \\tag{2.17} \\end{equation} \\] % CVchange, or percent coefficient of variation of the change is the SDchange divided by mean change (Equation (2.18)). \\[ \\begin{equation} \\%\\;CV_{change} = 100\\times\\frac{SD_{change}}{mean_{change}} \\tag{2.18} \\end{equation} \\] % Change, or Mean percent change is calculated by taking a mean of the ratio between the change and the Pre-test, multiplied by 100 (Equation (2.19)). \\[ \\begin{equation} mean_{\\% change} = 100\\times\\frac{1}{n}\\Sigma_{i}^{n}\\frac{change_{i}}{pre_{i}} \\tag{2.19} \\end{equation} \\] Mean ratio represents mean of the Post-test to Pre-test scores ratios (Equation (2.20)). \\[ \\begin{equation} mean_{ratio} = \\frac{1}{n}\\Sigma_{i}^{n}\\frac{post_{i}}{pre_{i}} \\tag{2.20} \\end{equation} \\] Cohen's d represents standardized effect size of the change. In the paired design, Cohen's d is calculated by dividing mean change with standard deviation of the Pre-test scores (SDpre) (Equation (2.21)). \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{change}}{SD_{pre}} \\tag{2.21} \\end{equation} \\] CLES for the paired groups represents probability of observing positive change. OVL, equally to the independent groups, represents overlap between the Pre-test and Post-test scores. Magnitude-based effect size estimators involve the use of SESOI and can be found on Table 2.11. Similarly to magnitude-based effect size estimators with the independent groups, magnitude-based effect size estimators with the paired group involve Change to SESOI, SDchange to SESOI as well as proportions of lower (pLower), equivalent (pEquivalent) and higher (pHigher) change scores. Table 2.11: Magnitude-based effect size statistics for estimating change between two dependent groups SESOI (kg) Change to SESOI SDchange to SESOI pLower pEquivalent pHigher ±5 0.24 0.87 0.2 0.42 0.38 Figure 2.7 depicts visually how proportions of lower, equivalent, and higher change scores are estimated. Same as with two independent groups, these proportions can be estimated using the brute-force method (i.e. simple counting of the change scores withing lower, trivial, and higher zones), or algebraic where SDchange is utilized and assumption of the normally distributed change scores is made. Figure 2.7: Visual analysis of the dependent groups scores using SESOI. A. Scatter plot of Pre-test and Post-test scores. Green line indicates change higher than SESOI upper, grey line indicates change within SESOI band, and red line indicates negative change lower than SESOI lower. B. Distribution of the change scores. Green area represents proportion of change scores higher than SESOI upper, red area represents proportion of negative change scores lower than SESOI lower, and grey area indicates equivalent change, which is within SESOI band It might be tempting to claim that this intervention is causing changes in the bench press 1RM, but we should be vary of doing that. It is important to keep in mind that the effect size estimators are used only descriptively without any causal connotation. To make causal claims, further criteria needs to be taken into account. This is discussed in more details in the Causal inference section of this book. 2.3 Describing relationship between two variables So far, we have dealt with single variable descriptive statistics. However, we are often interested in relationship or association between two variables. One of these variables takes the role of the dependent variable (outcome or target variable) and the other of the independent variable (or predictor variable). Let’s assume we tested N=30 female soccer athletes by using two tests: (1) YoYoIR1 test (expressed in meters), and (2) maximum aerobic speed (MAS) test (expressed in km/h)10. Variables in this example represent observations in each test (Table 2.12). Table 2.12: Results of YoYoIR1 and MAS tests for N=30 female soccer athletes Athlete YoYoIR1 (m) MAS (km/h) Athlete 01 1640 15.5 Athlete 02 1080 15.0 Athlete 03 1440 15.0 Athlete 04 1200 15.0 Athlete 05 960 14.5 Athlete 06 1120 15.0 Athlete 07 1000 14.5 Athlete 08 1440 15.0 Athlete 09 640 14.0 Athlete 10 1360 15.0 Athlete 11 760 14.5 Athlete 12 1240 15.0 Athlete 13 1000 15.0 Athlete 14 1600 15.5 Athlete 15 1160 15.0 Athlete 16 1520 15.0 Athlete 17 1000 14.5 Athlete 18 1000 14.5 Athlete 19 1480 15.5 Athlete 20 1280 15.0 Athlete 21 1200 14.5 Athlete 22 1200 14.5 Athlete 23 1200 15.0 Athlete 24 1120 14.5 Athlete 25 1560 15.5 Athlete 26 1120 14.5 Athlete 27 1640 15.5 Athlete 28 1280 15.0 Athlete 29 1040 14.5 Athlete 30 880 14.0 Descriptive statistics for YoYoIR1 and MAS test results can be found in the Table 2.13. Table 2.13: Descriptive statistics for YoYoIR1 and MAS test results Estimator YoYoIR1 MAS n 30.00 30.00 mean 1205.33 14.85 SD 255.96 0.42 % CV 21.24 2.82 median 1200.00 15.00 MAD 296.52 0.74 IQR 410.00 0.50 mode 1131.68 15.00 min 640.00 14.00 max 1640.00 15.50 range 1000.00 1.50 skew -0.02 -0.11 kurtosis -0.68 -0.72 Visual analysis in Figure 2.8 depicts the association between these two tests using scatter plot. Figure 2.8: Scatter plot between two variables. Dashed line represents linear regression line Table 2.14 contains common estimators of the association between two variables. All estimators except maximum information coefficient (MIC) (Albanese et al. 2012b; Reshef et al. 2011) assumes linear relationship between two variables. It is thus important to visually analyze the association (see Figure 2.8) before trusting numerical estimators. Table 2.14: Common estimators of the association between two variables Pearson r R-squared MIC 0.86 0.74 0.55 The Pearson product-moment correlation coefficient (Pearson's r) is a measure of the strength of the linear relationship between two variables (Equation (2.22)). \\[ \\begin{equation} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} \\tag{2.22} \\end{equation} \\] Pearson's r is standardized measure that can take values ranging from -1 to +1, where 0 indicates no relationship, and -1 and +1 indicates perfect relationship. Negative Pearson's r value represents negative association (i.e. as one variable increases the other decreases), while positive Pearson's r value represents positive association (i.e., as one variable increases so does the other). R-squared (\\(R^2\\)) represents variance explained, i.e. how much the model explains variance in the target variable. In this example the model is linear regression. R-squared is standardized measure of association that can take values ranging from zero (no association, or no variance explained) to 1 (perfect association, or all variance explained). R-squared, as its name suggests, represents Pearson’s r squared, but for more complex models it can be calculated using variances or mean squares (MS) (Equation (2.22)): \\[ \\begin{equation} \\begin{split} R^2 &amp;= \\frac{MS_{model}}{MS_{total}} \\\\ MS_{model} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(\\hat y_i - \\overline y)^2 \\\\ MS_{total} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(y_i - \\overline y)^2 \\end{split} \\tag{2.23} \\end{equation} \\] Maximal information coefficient (MIC) is a novel measure of the strength of the linear or non-linear association between two variables and belongs to the maximal information-based non-parametric exploration (MINE) class of statistics (Albanese et al. 2012b; Reshef et al. 2011). MIC is standardized measure of association that can take values ranging from zero (no association) to 1 (perfect association). As opposed to Pearson r, MIC can pick up non-linear association between two variables. Statistical model, or machinery underlying Pearson r and R-squared is linear regression. Similar to a sample mean (see section Sample mean as the simplest statistical model), linear regression can be seen as optimization algorithm that tries to find a line that passes through the data with the minimal error.11 A solution to this problem can be found computationally or analytically12. Either way, the coefficients (or parameters) that need to be estimated in this example with two variables are intercept (\\(\\hat{\\beta}_0\\)), slope coefficient (\\(\\hat{\\beta}_1\\)), and residual error (\\(\\hat{\\epsilon}\\)) (Equation (2.24)). \\[ \\begin{equation} \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{\\epsilon} \\tag{2.24} \\end{equation} \\] Table 2.15 contains estimates for intercept, slope, and residual error. Residual error (\\(\\epsilon\\)) is estimated by using residual standard error (RSE), which is similar to already discussed RMSE, but rather than dividing sum of square errors by \\(n\\) observations, it is divided by \\(n-p\\) (Equation (2.25)). The \\(p\\) is the number of model parameters, in this case 2 (intercept and one slope coefficient). \\[ \\begin{equation} RSE = \\sqrt{\\frac{1}{n-p}\\Sigma_{i=1}^{n}(y_i -\\hat{y_i})^2} \\tag{2.25} \\end{equation} \\] Table 2.15: Linear regression estimates for intercept, slope coefficient, and RSE when MAS is the target variable an YoYoIR1 is the predictor Intercept (km/h) Slope RSE (km/h) 13.16 0.0014 0.22 Estimated parameters in the Table 2.15 can be written using the linear equation format (Equation (2.26)). \\[ \\begin{equation} MAS = 13.16 + 0.0014 \\times YoYoIR1 \\pm 0.22 \\: km/h \\tag{2.26} \\end{equation} \\] Slope coefficient of 0.0014 can be interpreted the following way: if YoYoIR1 increases by 500m, then MAS would increase by 500 x 0.0014 or 0.7km/h. Although measures of association between two variables, such as Pearson's r and R-squared, are symmetrical (meaning it doesn’t matter which variable is predictor or target), one cannot reverse the linear regression equation to get YoYoIR1 from MAS as done in the Equation (2.25). \\[ \\begin{equation} \\begin{split} MAS &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times YoYoIR1 \\\\ YoYoIR1 &amp;= \\frac{-\\hat{\\beta}_0 + MAS}{\\hat{\\beta}_1} \\\\ YoYoIR1 &amp;= -\\frac{\\hat{\\beta}_0}{\\hat{\\beta}_1} + \\frac{1}{\\hat{\\beta}_1}\\times MAS \\\\ YoYoIR1 &amp;= -9385.59 + 713.19 \\times MAS \\end{split} \\tag{2.27} \\end{equation} \\] It can be seen that the reverse parameters from (2.25) differ from the parameters in the Table 2.16 which are estimated using YoYoIR1 as the target variable an MAS as the predictor variable. Table 2.16: Linear regression estimates for intercept, slope coefficient, and RSE when YoYoIR1 is the target variable an MAS is the predictor Intercept (m) Slope RSE (m) -6589.82 524.93 133.84 This diference between reversed parameters and correctly estimated can be visually seen as non-identical linear regression lines in the Figure 2.9. Figure 2.9: Regression line differs depending which variable is target or the outcome variable. Dashed grey line represents regression line when MAS is the target variable. Grey line represents regression line when YoYoIR1 is the target variable. Since they are not identical, one cannot reverse the equation to predict YoYoIR1 from MAS score, when such equation is estimated by predicting MAS from YoYoIR1 Unfortunately, this is common practice in sport science. Rather than reversing parameters, one needs to fit, in this case, linear regression model again with the properly defined target and predictor variables. In certain scenarios, such as [Reliability] analysis, we do not know which variable represents predictor and which represents target or outcome. For this reason, different approaches to regression, such as ordinary least products (OLP) are utilized (Ludbrook 2010, 2012, 1997, 2002; Mullineaux, Barnes, and Batterham 1999). These topics will be covered in the second part of this book. 2.3.1 Magnitude-based estimators Similarly to independent and dependent group analysis, with association we might be interested in the practical significance of the results. In order to judge results from practical significance perspective, we need to define SESOI of both variables (i.e. YoYoIR1 and MAS). Using minimal test increment, SESOI for the YoYoIR1 test is defined as ±40m, and SESOI for the MAS test is defined as ±0.5km/h. One question we might ask is whether the YoYoIR1 SESOI is associated with MAS SESOI. This can be answered with the sensitivity estimator (Equation (2.28)). \\[ \\begin{equation} \\begin{split} Sensitivity &amp;= \\frac{(SESOI_{YoYoIR1_{upper}} - SESOI_{YoYoIR1_{lower}})\\times\\hat{\\beta}_1}{SESOI_{MAS_{upper}}-SESOI_{MAS_{lower}}} \\\\ Sensitivity &amp;= \\frac{(40 - -40)\\times 0.0014}{0.5--0.5} \\\\ Sensitivity &amp;= \\frac{(80)\\times 0.0014}{1} \\\\ Sensitivity &amp;= \\frac{0.11}{1} \\\\ Sensitivity &amp;= 0.11 \\end{split} \\tag{2.28} \\end{equation} \\] This means that the change in the YoYoIR1 test equal to SESOI will yield only a small proportion of SESOI in the MAS test. In the case where SESOI of the MAS test is unknown, using known SESOI of the YoYoIR1 test can be used to estimate it. This is done by using estimated \\(\\hat{\\beta}_1\\) (slope coefficient), as demonstrated in the Equation (2.29). \\[ \\begin{equation} \\begin{split} SESOI_{MAS_{upper}} &amp;= \\hat{\\beta}_1\\times SESOI_{YoYoIR1_{upper}} \\\\ SESOI_{MAS_{upper}} &amp;= 0.0014\\times 40 \\\\ SESOI_{MAS_{upper}} &amp;= 0.06 \\: km/h \\\\ \\\\ SESOI_{MAS_{lower}} &amp;= \\hat{\\beta}_1\\times SESOI_{YoYoIR1_{lower}} \\\\ SESOI_{MAS_{lower}} &amp;= 0.0014\\times -40 \\\\ SESOI_{MAS_{lower}} &amp;= -0.06 \\: km/h \\end{split} \\tag{2.29} \\end{equation} \\] Next magnitude-based question might be related to the practically significant strength of the association between two variables. For example, we would like to know if the residuals are higher or lower than the SESOI in the target variable (i.e. MAS, which is equal to ±0.5km/h). Figure 2.10 depicts scatter plot between two variable (panel A) and residuals (panel B) utilizing SESOI in MAS as the grey area. Figure 2.10: Scatter plot between two variables using SESOI to indicate practically significant difference A. Scatterplot with SESOI depicted as grey band around linear regression line. B. Residual plot, where the difference between MAS and linear regression line (model estimate) is plotted against linear regression line (fitted or predicted MAS). SESOI is represented with the grey band. Residuals within SESOI band are of no practical difference. Dashed lines represent upper and lower levels of agreement using RSE and 95% confidence level (or in other words, 95% of the residuals distribution will be within these two dashed lines). Magnitude-based estimators of the practically significant strength of the two variable association involve ratio between the SESOI (\\(SESOI_{upper} - SESOI_{lower}\\)) and RSE (SESOI to RSE), and PPER. SESOI to RSE indicates how big are the residuals compared to the SESOI, and thus a metric of the practical strength of the association. Assuming that residuals are being normally distributed, SESOI to RSE over 4 (or \\(2\\times 1.96\\)) would indicate excellent practical strength of the association. If you look at the Table 15, estimated SESOI to RSE in this example is not great, indicating poor practical strength of association. Proportion of practically equivalent residuals (PPER) as a measure of the practical strength of the association revolves around estimating proportions of residuals in the equivalent range, defined as SESOI in the target variable (which is exactly the same as already introduced pEquivalent estimator). PPER can be estimated with the brute-force method by simply counting residuals in the equivalent zone, or using algebraic method and assuming normally distributed residuals (i.e. using RSE of the residuals13). Figure 2.11 graphically depicts how PPER is calculated. Practically significant association between two variables would have PPER equal to 1, which indicates that all residuals are within confines of the SESOI. If you look at the Table 2.17, estimated PPER in this example is almost perfect, indicating great practical strength of the association between YoYoIR1 and MAS tests. Figure 2.11: Residuals of the linear regression model predicting MAS from YoYoIR1 test. Proportion of residuals within SESOI band represent PPER Table 2.17: Magnitude-based estimators of the association between two variables. Association is estimated using linear regression model. MAS is the target variable, and YoYoIR1 is the predictor SESOI YoYoIR1 (m) SESOI MAS (km/h) Sensitivity RSE SESOI MAS to RSE PPER ±40 ±0.5 0.11 0.22 4.57 0.98 Visual inspection from the Figure 2.11 and magnitude-based estimates from the Table 2.17 indicate that using YoYoIR1 test scores, we are able to predict14 MAS test scores with the error within SESOI. But would that be the case if the we want to predict YoYoIR1 from MAS test scores? Predictive performance of such model is depicted on the Figure 2.12 and magnitude-based estimator are enlisted in the Table 2.18. Figure 2.12: Linear regression model estimating association between YoYoIR1 and MAS tests where YoYoIR1 is now the target variable. A. Scatterplot with SESOI depicted as grey band around linear regression line. B. Residual plot, where the difference between YoYoIR1 and linear regression line (model estimate) is plotted against MAS variable. SESOI is represented with the grey band. Residuals within SESOI band are of no practical difference. Proportion of residuals within SESOI band represent PPER Table 2.18: Magnitude-based estimators of the association between two variables. Association is estimated using linear regression model. YoYoIR1 is the target variable, and MAS is the predictor SESOI YoYoIR1 (m) SESOI MAS (km/h) Sensitivity RSE SESOI YoYoIR1 to RSE PPER ±40 ±0.5 6.56 133.84 0.6 0.23 As clearly indicated with this example, when estimating practical association between two variables, it is very important which variable is the target and which is predictor. When it comes to Pearson's r, R-Squared and MIC, this is not the case and results are same regardless of which variable is predictor and which is target. From the analysis performed, it seems that predicting MAS from YoYoIR1 is practically useful and the association is practically significant. Unfortunately, the same is not the case when we try to predict YoYoIR1 from MAS. This might be due different physical traits that determine the test scores. For example, results in the YoYoIR1 test might depend on the traits that include, but are not limited to, same traits important for the MAS test. The purpose of descriptive analysis is only to describe - further analysis involving answering the why questions is in the domain of explanatory modeling and causal inference (which are covered in the Causal inference section), as well as Advanced uses of descriptive modeling, such as latent variable modeling. What is important to remember is that to describe magnitude-based association, it is important to clearly state which variable is the target and which is the predictor. 2.4 Advanced uses Advanced techniques in the descriptive statistics involve dimension reduction, such as principal component analysis (PCA), latent variable modeling, such as factor analysis (FA), or cluster analysis (Beaujean 2014; Borsboom 2008; Borsboom, Mellenbergh, and van Heerden 2003; Everitt and Hothorn 2011; Finch and French 2015; Kabacoff 2015). These techniques are beyond the scope of this book and the interested readers are directed to references provided. References "],
["prediction.html", "Chapter 3 Prediction 3.1 Overfitting 3.2 Cross-Validation 3.3 Bias-Variance decomposition and trade-off 3.4 Interpretability 3.5 Magnitude-based prediction estimators 3.6 Practical example: MAS and YoYoIR1 prediction", " Chapter 3 Prediction In many disciplines there is a near-exclusive use of the statistical models for causal inference15 and the assumption that models with high explanatory power are inherently of high predictive power (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017; Hernán, Hsu, and Healy 2019). There is a constant tug-of-war between prediction versus explanation, and experts are leaning on one side or the other. Some experts warn against over-reliance on explanatory models with poor predictive power (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017), whereas some warn against over-reliance on predictive models that lack causal explanatory power that can guide intervention (Hernán, Hsu, and Healy 2019; Pearl and Mackenzie 2018; Pearl, Glymour, and Jewell 2016; Pearl 2019). It is thus important to differentiate between the two and take into account the research question that we are trying to answer. In this book, I define predictive modeling by using definition from Galit Shmueli “as the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations” (Shmueli 2010, 291). Usually this predictive statistical model is treated as a black box. Black box approach implies that we are not really interested in underlying mechanism and relationships between the predictor variables, only in predictive performance of the model (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017) Linear regression model from Describing relationship between two variables section already introduced predictive question (“If I know someone’s YoYoIR1 score, what would be his or her MAS score? Is the prediction within SESOI?”) to complement the association one (“How is YoYoIR1 associated with MAS?”). This section will continue this quest and introduce essential concepts and caveats of the predictive analysis needed to answer predictive questions. 3.1 Overfitting To explain a few caveats with predictive modeling, let’s take slightly more complex example (although we will come back to YoYoIR1 and MAS relationship later). Imagine we know the true relationship between back squat relative 1RM (BS)16 and vertical jump height during a bodyweight squat jump (SJ; measured in cm). This true relationship is usually referred to as data generating process (DGP) (Carsey and Harden 2013) and one of the aims of causal inference tasks is to uncover parameters and mechanism of DGP from the acquired sample17. With predictive tasks this aim is of no direct interest, but rather reliable prediction regarding new or unseen observations. DGP is usually unknown, but with simulations, such as this one, DGP is known and it is used to generate the sample data. Simulation is thus excellent teaching tool, since one can play with the problem and understand how the statistical analysis works, since the true DGP is known and can be compared with estimates (Carsey and Harden 2013; Hopkins 2007; Rousselet, Pernet, and Wilcox 2019). DGP is assumed to consist of systematic component \\(f(x)\\) and stochastic component \\(\\epsilon\\) (Equation (3.1)). \\[ \\begin{equation} Y = f(X) + \\epsilon \\tag{3.1} \\end{equation} \\] Systematic component is assumed to be fixed in the population (constant from sample to sample) and captures the true relationship \\(f(X)\\) among variables in the population (e.g. this can also be termed signal), while stochastic component represents random noise or random error, that varies from sample to sample, although its distribution remains the same. Random error is assumed to be normally distributed with mean of 0 and standard deviation which represents estimated parameter (either with RMSE or RSE). Thus, RMSE or RSE are estimates of \\(\\epsilon\\). In our example, the relationship between SJ and BS is expressed with the following Equation (3.2). \\[ \\begin{equation} \\begin{split} SJ &amp;= 30 + 15\\times BS\\times\\sin(BS)+\\epsilon \\\\ \\epsilon &amp;\\sim \\mathcal{N}(0,\\,2) \\end{split} \\tag{3.2} \\end{equation} \\] Systematic component in the DGP is represented with \\(30 + 15\\times BS\\times\\sin(BS)\\), and stochastic component is represented with the known random error (\\(\\epsilon\\)) that is normally distributed with the mean equal to zero and standard deviation equal to 2cm (\\(\\mathcal{N}(0,\\,2)\\)). This random error can be termed irreducible error (James et al. 2017), since it is inherent to the true DGP. As will be demonstrated shortly, models that perform better than this irreducible error are said to overfit. In other words, models are jumping to the noise. The objective of causal inference or explanatory modeling is to estimate the \\(f(X)\\) (estimate is indicated with the hat symbol: \\(\\hat{f}(x)\\)) or to understand the underlying DGP. With the predictive analysis, the goal is to find the best estimate of \\(Y\\) or \\(\\hat{y}\\). The underlying DGP is treated as a black box. To demonstrate a concept of overfitting, we are going to generate two samples (N=35 observations) from the DGP with BS ranging from 0.8 to 2.5. These samples are training and testing sample (Figure 3.1). Training sample is used to train the prediction model, while testing sample will be used as a holdout sample for evaluating model performance on the unseen data. Figure 3.1: Two samples simulated from the known DGP. Black line represents systematic component of the DGP and it is equal for both training and testing samples. Observations vary in the two samples due stochastic component in the DGP Model used to predict SJ from BS will be polynomial linear regression. Equation (3.3) explains first, second, and third degree polynomial linear regression function and provides a form for n-degree polynomials. Please, note that first degree polynomial function represents simple linear regression. \\[ \\begin{equation} \\begin{split} \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\hat{\\beta}_2 x_i^2 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\hat{\\beta}_2 x_i^2 + \\hat{\\beta}_3 x_i^3 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\dots + \\hat{\\beta}_n x_i^n \\end{split} \\tag{3.3} \\end{equation} \\] Increasing polynomial degrees increases the flexibility of the polynomial regression model, and thus can represent tuning parameter that we can select based on the model performance. In other words, we might be interested in finding polynomial degree that minimized model error (or maximize model fit). Figure 3.2 contains model performance on the training data for polynomial degrees ranging from 1 to 20. Figure 3.2: Model fit with varying polynomial degrees. More degrees equals better model fit As can be seen from the Figure 3.2, the more flexible the model (or the higher the polynomial degree) the better it fits the data. But how do these models perform on the unseen, testing data sample? In order to quantify model performance, RMSE metric is used. Figure 3.3 demonstrates performance of the polynomial regression model on the training and testing data sample across different polynomial degrees. Figure 3.3: Testing and training errors across varying polynomial degrees. Model error is estimated with the RMSE metric, while polynomial degree represents tuning or flexibility parameter of the model. As can be noted from the figure, better training performance doesn’t imply better testing performance. Vertical dashed line represents the polynomial degree at which testing error is lowest. Polynomial degrees on the right of the vertical dashed line are said to overfit the data, while polynomial degree on the left are said to underfit the data As can be seen from the Figure 3.3, models with higher polynomial degrees tend to overfit (indicated by performance better than the known irreducible error \\(\\epsilon\\) visualized with the horizontal line at 2cm). Performance on the training data sample improves as the polynomial degrees increase, which is not the case with the performance on the testing data sample. There is clearly the best polynomial degree that has the best predictive performance on the unseen data. Polynomial degrees on the left of the vertical dashed line are said to underfit, while polynomial degrees on the right are said to overfit. The take home message is that predictive performance on the training data can be too optimistic, and for evaluating predictive performance of the model, unseen data must be used, otherwise the model might overfit. 3.2 Cross-Validation In order to evaluate predictive performance of the model, researchers usually remove some percent of data to be used as a testing or holdout sample. Unfortunately, this is not always possible (although it is recommended, particularly to evaluate final model performance, especially when there are multiple models and model tuning). One solution to these problems is cross-validation technique (James et al. 2017; Kuhn and Johnson 2018; Yarkoni and Westfall 2017). There are numerous variations of the cross-validation, but the simplest one is n-fold cross validation (Figure 15). N-fold cross validation involve splitting the data into 5 to 10 equal folds and using one fold as a testing or hold-out sample while performing model training on the other folds. This is repeated over N-iteration (in this case 5 to 10) and the model performance is averaged to get cross-validated model performance. Figure 3.4: Cross-Validation With predictive analysis and machine learning, different model’s tuning parameters are evaluated (as well as multiple different models) to estimate the one that gives the best predictive performance. It is thus important to utilize techniques such as cross-validation to avoid overfitting and too optimistic model selection. Certain models, such as lasso, ridge regression, and elastic-net implement regularization parameters that penalizes the model complexity and are used as a tuning variable (James et al. 2017; Kuhn and Johnson 2018; Yarkoni and Westfall 2017). This is useful in situations when there are a lot of predictors, and it is easy to overfit the model. Selecting the best regularization parameter that has the best cross-validated performance helps in simplifying the model and avoiding the overfit. These topics are beyond the scope of this book, and interested readers are directed to references provided. 3.2.1 Sample mean as the simplest predictive model We have already discussed in Sample mean as the simplest statistical model section that sample mean can be considered simplest model that describes a particular sample with the lowest RMSE. But can it be used for prediction? Here is an example to demonstrate both sample mean as a predictive model, as well as to demonstrate cross-validation technique. Let’s assume that we have collected N=10 observations: 15, 19, 28, 28, 30, 57, 71, 88, 95, 97. Sample mean is equal to 52.8. If we assume that the sample mean represents our prediction for the observations, we can easily calculate prediction error for each observation, which is simple difference (column Error in the Table 3.1). Table 3.1: Sample mean as prediction with associated prediction errors Observed Predicted Error Absolute Error Squared Error 15 52.8 37.8 37.8 1428.84 19 52.8 33.8 33.8 1142.44 28 52.8 24.8 24.8 615.04 28 52.8 24.8 24.8 615.04 30 52.8 22.8 22.8 519.84 57 52.8 -4.2 4.2 17.64 71 52.8 -18.2 18.2 331.24 88 52.8 -35.2 35.2 1239.04 95 52.8 -42.2 42.2 1780.84 97 52.8 -44.2 44.2 1953.64 Besides simple difference, Table 3.1 provides errors (or losses) using two common loss functions (see Sample mean as the simplest statistical model section in Description chapter): absolute loss (column Absolute Error) and quadratic loss (column Squared Error). We need to aggregate these errors or losses into a single metric using the cost function. If we calculate the mean of the prediction errors (column Error in the Table 3.1), we are going to get 0. This is because the positive and negative errors cancel each other out for the sample mean estimator. This error estimator is often referred to as mean bias error (MBE) or simply bias and is often used in validity and reliability analysis (see [Validity] and [Reliability] sections). If we take the mean of the absolute prediction errors (column Absolute error in the Table 3.1) we are going to get mean absolute error (MAE) estimator, which is in this example equal to 28.8. If we take the mean of the squared prediction errors (column Squared error in the Table 3.1) we are going to get mean square error (MSE) estimator, often called variance in the case of describing sample dispersion. In this example MSE is equal to 964.36. To bring back MSE to the same scale with the observation scale, square root of the MSE is taken. This represents root mean square error (RMSE), which is equal to 31.05. As explained in Sample mean as the simplest statistical model section, sample mean represents statistical model of the central tendency with the lowest RMSE. Aforementioned error estimators, MBE, MAE, MSE, and RMSE can be considered different cost functions. Which one should be used18? As always, it depends (Chai and Draxler 2014). Assuming Gaussian normal distribution of the errors, MSE and RMSE have very useful mathematical properties that can be utilized in modeling stochastic or random components (i.e. random error propagation and prediction error decomposition in Bias-Variance decomposition and trade-off section). This property will be utilized thorough this book and particularly in the Example of randomized control trial section when estimating random or stochastic component of the treatment effect. I personally prefer to report multiple estimators, including error estimators, which is also a strategy suggested by Chai and Draxler (2014). The are, of course, other loss and cost functions that could be used. For example, one might only use the maximal error (MaxErr) and minimal error (MinErr), rather than average. Discussion and review of these different metrics is beyond the scope of this book (for more information please check the package Metrics (Hamner and Frasco 2018) and the following references (Botchkarev 2019; Chai and Draxler 2014; Willmott and Matsuura 2005; Barron 2019)). Figure 3.5 visualize the most common loss functions that are used in both model training and as performance metrics. It is important to keep in mind that in the case of OLS regression, MSE (or RMSE) is minimized. It is thus important to make a distinction between cost function used in the optimization and model training (i.e. in OLS, parameters of the model are found so that MSE is minimized; in some machine-learning models Huber loss or Rigde loss19 is minimized; see Figure 3.5) versus cost function used as a performance metric (e.g. reporting pEquivalent, MaxErr or MinErr for OLS models). Figure 3.5: Common loss functions. A. Trellis plot of each loss function. B. Plot of all loss functions on a common scale. Huber loss is a combination of absolute and quadratic loss. The take-away point is to understand that there are multiple performance metrics that can be utilized and it is best to report multiple of them. Estimates for MBE, MAE, MSE20, and RMSE represent training sample predictive performance since sample mean is estimated using this very sample. We are interested how sample mean, as predictive model, perform on the unseen data. Cross-validation is one method to estimate model performance on unseen data. Table 3.2 contains 3-fold cross-validation sample used to train the model (in this case estimate sample mean) and to evaluate the performance on unseen data. Table 3.2: Example cross-validation using sample mean as the prediction model Fold Training sample mean Testing Sample MBE MAE RMSE MinErr MaxErr 1 15 19 28 – 30 – 71 88 95 – 49.43 – – – 28 – 57 – – – 97 -11.24 25.52 30.44 -47.57 21.43 2 15 19 – 28 – 57 – 88 – 97 50.67 – – 28 – 30 – 71 – 95 – -5.33 27.00 28.81 -44.33 22.67 3 – – 28 28 30 57 71 – 95 97 58.00 15 19 – – – – – 88 – – 17.33 37.33 37.73 -30.00 43.00 Since this is a small sample, we can repeat cross-validation few times. This is called repeated cross-validation. Let’s repeat 3-folds cross-validation for 5 repeats (Table 3.3). Table 3.3: Example repeated cross-validation using sample mean as the prediction model Repeat Fold Training sample mean Testing Sample MBE MAE RMSE MinErr MaxErr 1 1 15 19 28 – 30 – 71 88 95 – 49.43 – – – 28 – 57 – – – 97 -11.24 25.52 30.44 -47.57 21.43 1 2 15 19 – 28 – 57 – 88 – 97 50.67 – – 28 – 30 – 71 – 95 – -5.33 27.00 28.81 -44.33 22.67 1 3 – – 28 28 30 57 71 – 95 97 58.00 15 19 – – – – – 88 – – 17.33 37.33 37.73 -30.00 43.00 2 1 – 19 28 – 30 57 71 88 – – 48.83 15 – – 28 – – – – 95 97 -9.92 37.25 38.83 -48.17 33.83 2 2 15 – 28 28 – 57 – – 95 97 53.33 – 19 – – 30 – 71 88 – – 1.33 27.50 28.45 -34.67 34.33 2 3 15 19 – 28 30 – 71 88 95 97 55.38 – – 28 – – 57 – – – – 12.87 14.50 19.39 -1.63 27.37 3 1 – 19 28 28 30 57 71 88 – – 45.86 15 – – – – – – – 95 97 -23.14 43.71 44.66 -51.14 30.86 3 2 15 19 – – 30 – – 88 95 97 57.33 – – 28 28 – 57 71 – – – 11.33 18.17 21.84 -13.67 29.33 3 3 15 – 28 28 – 57 71 – 95 97 55.86 – 19 – – 30 – – 88 – – 10.19 31.62 31.94 -32.14 36.86 4 1 – 19 28 – 30 57 71 88 – 97 55.71 15 – – 28 – – – – 95 – 9.71 35.90 36.37 -39.29 40.71 4 2 15 – – 28 30 – 71 – 95 97 56.00 – 19 28 – – 57 – 88 – – 8.00 24.50 28.19 -32.00 37.00 4 3 15 19 28 28 – 57 – 88 95 – 47.14 – – – – 30 – 71 – – 97 -18.86 30.29 33.41 -49.86 17.14 5 1 15 19 – 28 – 57 – 88 95 97 57.00 – – 28 – 30 – 71 – – – 14.00 23.33 24.26 -14.00 29.00 5 2 15 – 28 28 30 – 71 88 95 – 50.71 – 19 – – – 57 – – – 97 -6.95 28.10 32.60 -46.29 31.71 5 3 – 19 28 – 30 57 71 – – 97 50.33 15 – – 28 – – – 88 95 – -6.17 35.00 35.92 -44.67 35.33 To calculate cross-validated prediction performance metrics, average of testing MBE, MAE, RMSE, MinErr, and MaxErr is calculated and reported as cvMBE, cvMAE, cvRMSE, cvMinErr, and cvMaxErr (Table 3.4). Prediction performance metrics don’t need to be averaged across cross-validation samples and can be instead estimated by binding (or pooling) all cross-validated samples together (i.e. target variable and predicted target variable). More about this at the end of this chapter in Practical example: MAS and YoYoIR1 prediction section. Table 3.4: Cross-validated prediction performance metrics (estimators) cvMBE cvMAE cvRMSE cvMinErr cvMaxErr 0.21 29.32 31.52 -35.29 31.37 As can be seen from the Table 3.4, all performance metrics estimated using repeated cross-validation are larger than the when estimated using the training data (full sample). Utilizing cross-validated estimates of performance (or error) should be used over training estimates when discussing predictive performance of the models. Unfortunately, this is almost never the case in sport science literature, where prediction is never estimated on unseen data and the model performance estimates can suffer from over-fitting. Using sample mean as predictive model represents simplistic example, although this type of model is usually referred to as baseline model. Baseline models are used as benchmarks or anchor when discussing performance of more elaborate and complex predictive models. We will come back to these at the end of this section when we will perform predictive analysis of the linear regression model used in Magnitude-based estimators section for predicting MAS from YoYoIR1 (and vice versa) tests. 3.3 Bias-Variance decomposition and trade-off Prediction error can be decomposed into two components, reducible and irreducible error21. Reducible error is the error that can be reduced with a better model, while irreducible error is the unknown error inherent to the DGP itself (James et al. 2017). Reducible error can be further divided into \\(Bias^2\\) and \\(Variance\\) (Figure 3.6 and Equation (3.4)). \\[ \\begin{equation} \\begin{split} Prediction \\; error &amp;= Reducible \\; error + Irreducible \\; error \\\\ Prediction \\; error &amp;= (Bias^2 + Variance) + Irreducible \\; error \\end{split} \\tag{3.4} \\end{equation} \\] \\(Bias^2\\) represents constant or systematic error, which is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model (James et al. 2017). \\(Variance\\) represents variable or random error, and refers to the amount by which model parameters would change if we estimated it by using a different training data set (James et al. 2017). Figure 3.6: Bias and variance decomposition of the error. A. Visual representation of \\(Bias^2\\) and \\(Variance\\) using the shooting target. B. Error can be decomposed to \\(Bias^2\\), \\(Variance\\), and \\(Irreducible \\: error\\), where \\(Bias^2\\) represents constant or systematic error and \\(Variance\\) represents variable or random error To understand this concept, we need to run a simulation using known relationship between BS and SJ (see Figure 3.1, Figure 3.2, and Equation (3.2)). Prediction error decomposition to bias and variance is done for a single data point. To do that we need to differentiate between the following variables: \\(x\\) predictors. In our case we only have one \\(x\\) predictor - \\(BS\\) and we will use \\(BS = 1.75\\) for this simulation \\(y_{true}\\) value for a particular \\(x\\) values. In our case \\(SJ\\) variable represents target variable \\(y\\), which is equal to \\(SJ = 30 + 15\\times BS\\times\\sin(BS)\\) or \\(SJ=55.83\\)cm. Both \\(x\\) and \\(y_{true}\\) values are constant across simulations \\(y_{observed}\\) represents observed \\(y\\) which differs from \\(y_{true}\\) due stochastic component \\(\\epsilon\\). This implies that \\(y_{observed}\\) randomly varies across simulations. The equation for \\(y_{observed}\\) is: \\(SJ = 30 + 15\\times BS\\times\\sin(BS) + \\mathcal{N}(0,\\,2)\\). Error \\(\\epsilon\\) represents irreducible error, because it is inherent to DGP and it is equal to \\(\\mathcal{N}(0,\\,2)\\). \\(y_{predicted}\\) represents model prediction using the training sample of \\(x\\) and \\(y_{observed}\\) values. For every simulation (N=200 simulations in total), the whole sample of N=35 observations is generated from the known DGP. This includes \\(x\\), \\(y_{true}\\), and \\(y_{observed}\\) variables. Polynomial regression models (from 1 to 20 polynomial degrees) are being fitted (or trained) using \\(x\\) predictors (in our case \\(BS\\) variable) and \\(y_{observed}\\) as our target variable. True \\(y\\) (\\(y_{true}\\)) is thus unknown to the model, but only to us. After models are trained, we estimate \\(y_{predicted}\\) using \\(BS = 1.75\\), for which the \\(y_{true}\\) is equal to \\(SJ=55.83\\)cm. Table 3.5 contains results of the first 10 simulations for 2nd degree polynomial model. Table 3.5: Results of first 10 simulations for 2nd degree polynomial linear regression model sim model x y_true y_observed y_predicted 1 2 1.75 55.83 54.65 55.27 2 2 1.75 55.83 53.80 55.34 3 2 1.75 55.83 56.03 55.34 4 2 1.75 55.83 55.71 55.68 5 2 1.75 55.83 55.52 54.91 6 2 1.75 55.83 57.55 55.81 7 2 1.75 55.83 52.03 55.08 8 2 1.75 55.83 56.35 55.92 9 2 1.75 55.83 54.05 54.64 10 2 1.75 55.83 58.16 55.57 To estimate reducible error, MSE estimator is used (Equation (3.5). \\[ \\begin{equation} Reducible \\: error = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{predicted_{j,x=1.75}} - y_{true_{x=1.75}})^2 \\tag{3.5} \\end{equation} \\] Reducible error can be decomposed to \\(Bias^2\\) and \\(Variance\\), or systematic error and variable or random error. \\(Bias^2\\) is squared difference between \\(y_{true}\\) and mean of \\(y_{predicted}\\) across simulations (Equation (3.6)). \\[ \\begin{equation} Bias^2 = (\\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{predicted_{j, x=1.75}}) - y_{true_{x=1.75}})^2 \\tag{3.6} \\end{equation} \\] \\(Variance\\) represents, pretty much, SD of the of the \\(y_{predicted}\\) and it is an estimate of how much the predictions vary across simulations (Equation (3.7)). \\[ \\begin{equation} Variance = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(\\bar{y_{predicted_{x=1.75}}} - y_{predicted_{j, x=1.75}})^2 \\tag{3.7} \\end{equation} \\] Reducible error is thus equal to the sum of \\(Bias^2\\) and \\(Variance\\). If you remember from Description section, \\(Bias^2\\) and \\(Variance\\) are nothing more than measure of central tendency (i.e. systematic or constant error) and measure of spread (i.e. variable or random error). Figure 3.6 also illustrates this concept. Irreducible error is estimated using MSE as well (Equation MSE-irreducible-error). \\[ \\begin{equation} Irreducible \\: error = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{observed_{x=1.75}} - y_{true_{x=1.75}})^2 \\tag{3.8} \\end{equation} \\] Since we know that the stochastic error in the DGP is normally distributed with SD=2cm, expected irreducible error should be around 4cm (this is because MSE is mean squared error, and RMSE, which is equivalent to SD, is calculated by doing square root of MSE). This might not be exactly 4cm due sampling error which is the topic of [Statistical inference] section. As explained in Equation (3.4), \\(Prediction \\: error\\) is equal to sum of \\(Bias^2\\), \\(Variance\\) and \\(Irreducible \\: error\\). Table contains estimated aforementioned errors using all 200 simulations for 2nd degree polynomial linear regression. Table 3.6: Calculated errors for all 200 simulations for 2nd degree polynomial linear regression model x y_true Prediction error Bias^2 Variance Irreducible error 2 1.75 55.83 5.17 0.09 0.21 4.87 This decomposition of errors is one useful mathematical property when using squared erors that I alluded to in the Cross-Validation section when discussing prediction error metrics. If we perfrom this analysis for each degree of polynomial fit, we will estimate prediction error, as well as \\(Bias^2\\) and \\(Variance\\) across model complexity (i.e. polynomial degrees). This is visualized in the Figure 3.7. Figure 3.7: Bias and Variance error decomposition. \\(Prediction \\: error\\) is indicated with the black line, and is decomposed to \\(Bias^2\\), \\(Variance\\), and \\(Irreducible \\: error\\). These are represents with areas of different color Beside decomposition of \\(Prediction \\: error\\) to \\(Bias^2\\), \\(Variance\\) and \\(Irreducible \\: error\\), it is important to notice the trade-off between \\(Bias^2\\) and \\(Variance\\). Linear regression models with lower polynomial degree, particularly 1st degree which is simple linear regression, has higher \\(Bias^2\\) due imposed linearity of the model (we can say that linear regression model is more biased). As \\(Bias^2\\) decreases with more flexible models (i.e. higher polynomial degree), \\(Variance\\) increase due model being too jumpy across simulations. To achieve best prediction (or the lower \\(Prediction \\: error\\)) a balance between \\(Bias^2\\) and \\(Variance\\) needs to be found, both within a particular model and across models. The free lunch theorem (Kuhn and Johnson 2018; Yarkoni and Westfall 2017) states that there is no single model that is the best across all different sets of problems. One needs to evaluate multiple models22 to estimate which one is the best for a particular problem at hand. To estimate \\(Bias^2\\) and \\(Variance\\), true DGP must be known. Unfortunately, we do not know these for the real world problems, only for simulations. But even when we do not know \\(y_{true}\\) values (and thus \\(Irreducible \\: error\\)), concepts of \\(Bias^2\\) and \\(Variance\\) can be applied in cross-validation samples (particularly when using multiple repeats) and can estimated using \\(y_{observed}\\). I will provide one such analysis in Practical example: MAS and YoYoIR1 prediction section. 3.4 Interpretability As explained, predictive models put predictive performance over explanation of the underlying DGP mechanism (which is treated as a black box). However, sometimes we might be interested in which predictor is the most important, how do predictions change when particular predictor changes, or why23 model made a particular prediction for a case of interest (Kuhn and Johnson 2018; Molnar 2018; Ribeiro, Singh, and Guestrin 2016). Model interpretability can be defined as the degree to which a human can understand the cause of a decision (Miller 2017; Molnar 2018; Biecek and Burzykowski 2019). Some models are more inherently interpretable (e.g. linear regression) and some are indeed very complex and hard to interpret (e.g. random forest or neural networks). For this reason, there are model-agnostic techniques that can help increase model interpretability. Excellent book and R (R Core Team 2020) package by Christoph Molnar (Molnar, Bischl, and Casalicchio 2018; Molnar 2018) demonstrates a few model-agnostic interpretation techniques. One such technique is estimating which predictor is the most important (variable importance). One method for estimating variable importance involves perturbing one predictor and estimating the change in model performance. The predictor whose perturbing causes the biggest change in model performance can be considered the most important (Kuhn and Johnson 2018; Molnar 2018). There are others approaches for estimating variable importance (B. M. Greenwell 2017b). Those interested in further details can also check vip (Greenwell, Boehmke, and Gray 2020) R (R Core Team 2020) package. One might be interested in how the predicted outcome changes when particular predictor changes. Techniques such as partial dependence plot (PDP), individual conditional expectation (ICE) and accumulated local effects (ALE) can be helpful in interpreting the effect of particular predictor on predicted outcome (Molnar, Bischl, and Casalicchio 2018; Molnar 2018; Goldstein et al. 2013; Zhao and Hastie 2019; B. M. Greenwell 2017a). Similar techniques are utilized in [Prediction as a complement to causal inference] section. Interested readers are also directed toward visreg (Breheny and Burchett 2017) and effects (Fox and Weisberg 2018; Fox 2003; Fox and Hong 2009) R (R Core Team 2020) packages for more information about visualizing models. It is important to keep in mind that these model-agnostic explanations should not be automatically treated as causal explanations (Pearl and Mackenzie 2018; Pearl 2019), but as mere association and descriptive analysis that can still be useful in understanding and interpreting the underlying predictive black-box. They are not without problems, such as correlated variables, interactions and other issues (Altmann et al. 2019). According to Judea Pearl (Pearl and Mackenzie 2018; Pearl 2019), prediction models should belong to the first level of ladder of causation24, which represents simple “curve fitting”. Although in under special conditions these techniques can have causal interpretation (Zhao and Hastie 2019). The distinctions, similarities and issues between predictive modeling, machine learning and causal inference is currently hot topic in debates between machine learning specialists, statisticians and philosophers of science and it is beyond the scope of this book to delve into the debate. Interested readers are directed towards work by Miguel Hernan (Hernán 2017, 2016, 2018; Hernán and Robins, n.d.; Hernán, Hsu, and Healy 2019), Judea Pearl (Pearl and Mackenzie 2018; Pearl, Glymour, and Jewell 2016; Pearl 2019, 2009), Samantha Kleinberg (Kleinberg 2015, 2018) and others (Watts et al. 2018; Saddiki and Balzer 2018; Kleinberg, Liang, and Mullainathan 2017; Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017). The next Causal Inference section introduces the causal inference as a specific task of statistical modeling. 3.5 Magnitude-based prediction estimators Similar to the magnitude-based estimators from Describing relationship between two variables section, one can utilize target variable SESOI to get magnitude-based estimates of predictive performance of the model. Rather than utilizing RSE as an estimate of the model fit in the training data, one can utilize cross-validated RMSE (cvRMSE), SESOI to cvRMSE, as well as cross-validated proportion of practically equivalent residuals (cvPPER) estimators. Continuing with the squat jump height and relative squat 1RM example, one can assume that the SESOI in the squat jump is ±1cm. For the sake of example, we can feature engineer (Kuhn and Johnson 2018, 2019) relative squat 1RM variable to include all 20 degree polynomials. This way, we have created 20 predictor variables. To avoid overfitting, elastic-net model (Friedman, Hastie, and Tibshirani 2010) implemented in the caret R package (Kuhn and Johnson 2018; Kuhn et al. 2018) is utilized, as well as repeated cross-validation involving 3 splits repeated 10 times. Predictive model performance is evaluated by using cvRMSE, together with magnitude-based performance estimators (SESOI to cvRMSE and cvPPER). Elastic-net model represents regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, or alpha and lambda tuning parameters (Friedman, Hastie, and Tibshirani 2010; James et al. 2017; Kuhn and Johnson 2018). Total of nine combinations of tuning parameters is evaluated using aforementioned repeated cross-validation, and the model with minimal cvRMSE is selected as the best one. Performance metrics of the best model are further reported. Table 3.7 contains cross-validated best model performance metrics together with model performance on the training data set. Table 3.7: Common predictive metrics and magnitude-based predictive metrics. Metrics starting with cv indicate cross-validated performance metrics. Metrics without cv indicate performance metrics on the training data set, which is often more optimistic SESOI (cm) cvRMSE (cm) SESOI to cvRMSE cvPPER RMSE (cm) SESOI to RMSE PPER ±1 2.19 0.91 0.34 1.99 1.01 0.38 Utilizing apriori known SESOI gives us practical anchor to evaluate predictive model performance. Reported SESOI to cvRMSE (0.91) as well as cvPPER (0.34) indicate very poor predictive performance of the model. In practical terms, utilizing relative squat 1RM doesn’t produce practically meaningful predictions given SESOI of ±1cm and the model as well as the data sample utilized. Model performance can be visualized using the training data set (Figure 3.8). PPER estimator, for both cross-validate estimate and training data performance estimate, utilized SD of the residuals and provided SESOI. Grey band on panels A and B on Figure 3.8 represents SESOI, and as can be visually inspected, model residuals are much wider than the SESOI, indicating poor practical predictive performance. Figure 3.8: Model performance on the training data set. A. Model with the lowest cvRMSE is selected. SESOI is depicted as grey band around the model prediction (blue line). B. Residuals scatter plot. Residuals outside of the SESOI band (grey band) indicate prediction which error is practically significant. PPER represents proportion of residuals inside the SESOI band Predictive tasks are focusing on providing the best predictions on the novel or unseen data without much concern about the underlying DGP. Predictive model performance can be evaluated by using magnitude-based approach to give insights into practical significance of the predictions. These magnitude-based prediction estimators, can be used to complement explanatory or causal inference tasks, rather than relying solely on the group-based and average-based estimators. This topic is further discussed in the [Prediction as a complement to causal inference] section. 3.6 Practical example: MAS and YoYoIR1 prediction In Describing relationship between two variables we have used two physical performance tests, MAS and YoYoIR1, to showcase relationship or association between two variables. Besides mere association, we have stepped into the domain of prediction by utilizing magnitude-based estimators such as PPER and SESOI to RMSE. As you have learned so far in this section, these predictions were made on the training data set. Let’s implement concepts learned so far to estimate predictive performance on the unseen data. Why is this important? Although very simple model, we are interested in predicting MAS from YoYoIR1 test score for a new or unseen individual. For this reason, it is important to get estimates of model performance on the unseen athletes. 3.6.1 Predicting MAS from YoYoIR1 Let’s first estimate predictive performance when predicting MAS scores from the YoYoIR1 score using MAS SESOI ±0.5km/h and linear regression. Figure 3.9 consists of two panels. Panel A depicts scatter plot between YoYoIR1 and MAS scores (black line represents linear model fit). Panel B depicts \\(y_{predicted}\\) (predicted or fitted MAS using simple linear regression; i.e. the black line on the panel A) against the model residuals \\(y_residual = y_{predicted} - y_{observed}\\), or Predicted MAS - MAS. The data points represent model performance on the full training data set. Figure 3.9: Scatter plot for simple linear regression between MAS and YoYoIR1 using the full training data sample. A. Scatter plot between MAS and YoYoIR1 scores. Black line indicates model prediction. B. Scatter plot between \\(y_{predicted}\\) (fitted or predicted MAS) against model residual \\(y_{residual} = y_{predicted} - y_{observed}\\), or Predicted MAS - MAS. Dotted lines indicate Levels of Agreement (LOA; i.e. upper and lower threshold that contain 95% of residuals distribution) and grey band indicates SESOI. Blue line indicate linear regression fit of the residuals and is used to indicate issues with the model (residuals) Predictive performance for the full training data set is enlisted in the Table 3.8. Table 3.8: Predictive performance using the full training data set MBE MAE RMSE PPER SESOI.to.RMSE R.squared MinErr MaxErr MaxAbsErr 0 0.17 0.21 0.98 4.73 0.74 -0.44 0.39 0.44 But as already explained, these are not predictive performance estimators for the unseen data. To estimate how the model performs on the unseen data (i.e. unseen athletes in this case), cross-validation is performed using 3 folds and 5 repeats. Estimated predictive performance for every cross-validation sample is enlisted in the Table 3.9. Table 3.9: Predictive performance for every repeated cross-validated sample fold MBE MAE RMSE PPER SESOI to RMSE R-squared MinErr MaxErr MaxAbsErr 1 Fold1.Rep1 -0.13 0.27 0.29 0.90 3.42 0.62 -0.46 0.38 0.46 2 Fold1.Rep2 -0.04 0.13 0.14 1.00 7.15 0.87 -0.22 0.15 0.22 3 Fold1.Rep3 0.13 0.24 0.27 0.93 3.74 0.79 -0.27 0.49 0.49 4 Fold1.Rep4 -0.17 0.25 0.28 0.91 3.52 0.41 -0.51 0.26 0.51 5 Fold1.Rep5 0.08 0.20 0.24 0.95 4.20 0.77 -0.28 0.46 0.46 6 Fold2.Rep1 0.09 0.15 0.19 0.99 5.18 0.87 -0.27 0.37 0.37 7 Fold2.Rep2 0.05 0.20 0.24 0.95 4.16 0.71 -0.31 0.41 0.41 8 Fold2.Rep3 -0.14 0.18 0.22 0.98 4.65 0.81 -0.38 0.18 0.38 9 Fold2.Rep4 -0.02 0.13 0.17 1.00 5.92 0.75 -0.30 0.33 0.33 10 Fold2.Rep5 -0.07 0.21 0.27 0.93 3.76 0.27 -0.50 0.33 0.50 11 Fold3.Rep1 0.04 0.13 0.16 1.00 6.27 0.60 -0.20 0.31 0.31 12 Fold3.Rep2 -0.01 0.20 0.24 0.95 4.21 0.54 -0.45 0.34 0.45 13 Fold3.Rep3 0.02 0.15 0.21 0.98 4.74 0.51 -0.44 0.35 0.44 14 Fold3.Rep4 0.21 0.23 0.28 0.93 3.57 0.83 -0.06 0.53 0.53 15 Fold3.Rep5 -0.01 0.17 0.19 0.99 5.29 0.82 -0.32 0.34 0.34 As explained in Cross-Validation section, to calculate overall cross-validated performance, the mean is calculated for the performance metrics in the Table 3.9. Besides reporting the mean as the summary for predictive performances across cross-validated samples, SD, min, and max can be reported too. Another method of summarizing predictive performance over cross-validated samples would be to bind or pool all \\(y_{observed}\\) and \\(y_{predicted}\\) scores from the test samples together and then calculate overall predictive performance metrics. These pooled cross_validated \\(y_{observed}\\) and \\(y_{predicted}\\) can also be visualized using the residuals plot (Panel C in Figure 3.10). Figure 3.10: Residuals plot. A. Model residuals using the training data. This is exactly the same as panel B in Figure 3.9. B. Model residuals using the cross-validated training data. C. Model residuals using the cross-validated testing data. As can be seen from the panels B and C in Figure 3.10, since we have used 5 repeats of the 3-fold cross-validations, each \\(y_{observed}\\) will have 5 assigned \\(y_{predicted}\\) scores. These can be used to estimate \\(Bias^2\\) and \\(Variance\\) as explained in the Bias-Variance decomposition and trade-off section. Since we do not know the \\(y_{true}\\) scores, we can only estimate \\(Bias^2\\) and \\(Variance\\) of the model for each \\(y_{observed}\\) using cross-validated \\(y_{predicted}\\). This is, of course, only possible if multiple repeats of cross-validation are performed. It bears repeating that this \\(Bias^2\\) and \\(Variance\\) decomposition of the prediction error using cross-validated \\(y_{predicted}\\) and \\(y_{observed}\\) are NOT the same as when using simulation and known DGP as done in Bias-Variance decomposition and trade-off section. But these can be useful diagnostic tools for checking where the model fails (e.g. what particular observation might be problematic or outlier, as well how does \\(Bias^2\\) and \\(Variance\\) changes across \\(y_{observed}\\) continuum). These two concepts are depicted on Figure 3.11 and Figure 3.12. Figure 3.11: Prediction error (\\(MSE\\)), \\(Bias^2\\), and \\(Variance\\) across repeated cross-validated testing data. X-axis on the panels represents observation index, as in \\(y_{i, observed}\\) Figure 3.12: Prediction error (\\(MSE\\)), \\(Bias^2\\) and \\(Variance\\) across repeated cross-validated testing data. X-axis on the panels represent \\(y_{observed}\\). Since there might be multiple equal \\(y_{observed}\\), min and max are used and represent ribbon over mean (indicated by line) Since \\(Bias\\) and \\(Variance\\) represent a quantitative summary of the residuals across cross-validations, the residuals and predicted observations across cross-validation testing folds can be visualized in more details as depicted on Figures 3.13 and 3.14. Figure 3.13: Testing prediction residuals across cross-validation folds summarized with cross-bars for every observation. Cross-bars represent ranges of testing residuals for each observation, while horizontal bar represent mean residual. The length of the bar represents \\(Variance\\), while distance between horizontal dashed line and horizontal line in the cross-bar (i.e. mean residual) represents \\(Bias\\). Figure 3.14: Testing prediction residuals across cross-validation folds summarized with cross-bars for every observation value. Cross-bars represent ranges of testing residuals for each observation, while horizontal bar represent mean residual. The length of the bar represents \\(Variance\\), while distance between horizontal dashed line and horizontal line in the cross-bar (i.e. mean residual) represents \\(Bias\\). Cross-validated, pooled, and full training data set predictive performance metrics can be found in the Table 3.10. Please note that the pooled predictive performance metrics are in the column testing.pooled. Table 3.10: Predictive performance summary metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 0.00 0.00 0.10 -0.17 0.21 MAE 0.17 0.17 0.19 0.19 0.05 0.13 0.27 RMSE 0.21 0.21 0.23 0.23 0.05 0.14 0.29 PPER 0.98 0.98 0.97 0.96 0.03 0.90 1.00 SESOI to RMSE 4.73 4.83 4.33 4.65 1.12 3.42 7.15 R-squared 0.74 0.75 0.68 0.68 0.18 0.27 0.87 MinErr -0.44 -0.49 -0.51 -0.33 0.13 -0.51 -0.06 MaxErr 0.39 0.42 0.53 0.35 0.10 0.15 0.53 MaxAbsErr 0.44 0.49 0.53 0.41 0.09 0.22 0.53 Summary from the Table 3.10 as well as the individual cross-validated sample predictive performance from the Table 3.9 are visually represented in the Figure 3.15). Figure 3.15: Cross-validated model performance. Dot and line bar indicate mean, min and max of the cross-validated performance. Dotted line indicate model performance on the training data set. As can be seen from the Figure 3.15, cross-validated prediction performance metrics do not differ much25 from the metrics estimated using the full training sample (calculated in Describing relationship between two variables section and in the Table 3.8, and indicated by the dotted horizontal line in the Figure 3.15). For some more complex models, these differences can be much larger and are clear indication of the model over-fitting. Overall, predicting MAS from the YoYoIR1 score, given the data collected, SESOI of ±0.5km/h, and linear regression as a model, is practically excellent. Please note that prediction can be practically useful (given SESOI) (PPER; CV from 0.9 to 1) even when R-squared is relatively low (CV from 0.27 to 0.87). And the vice versa in some cases. That’s the reason why we need to utilize magnitude-based estimators as a complement of contemporary estimators such as R-squared, RSE, and RMSE. 3.6.2 Predicting YoYoIR1 from MAS As shown in Describing relationship between two variables, predicting YoYoIR1 from MAS scores was not practically useful (or precise enough). But for the sake of completeness, let’s perform cross-validated prediction (using 3 folds and 5 repeats). YoYoIR1 SESOI is taken to be ±40m. Figure 3.16 depicts modified Bland-Altman plot for predictions using the full training data set. Visual inspection demonstrates that many points are outside of SESOI band, indicating poor practically significant (or useful) predictions. Figure 3.16: Scatter plot for simple linear regression between YoYoIR1 and MAS using the full training data sample. A. Scatter plot between YoYoIR1 and MAS scores. Black line indicates model prediction. B. Scatter plot between \\(y_{predicted}\\) (fitted or predicted YoYoIR1) against model residual \\(y_{residual} = y_{predicted} - y_{observed}\\), or Predicted YoYoIR1 - YoYoIR1. Dotted lines indicate Levels of Agreement (LOA; i.e. upper and lower threshold that contain 95% of residuals distribution) and grey band indicates SESOI. Blue line indicate linear regression fit of the residuals and is used to indicate issues with the model (residuals) Predictive performance metrics can be found in the Table 3.10. As already expected, predicting YoYoIR1 from the MAS score, given the data collected, SESOI and linear regression model is not precise enough to be practically useful. Please note that the R-squared is very close to R-squared from the Table 3.10, but the PPER is much worse. Another reason to complement contemporary estimators with magnitude-based ones. Table 3.11: Predictive performance summary metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 3.13 1.95 50.33 -96.20 82.14 MAE 104.69 103.04 112.98 112.59 24.99 66.61 154.76 RMSE 129.30 127.55 138.73 135.92 26.46 82.62 175.52 PPER 0.24 0.25 0.23 0.23 0.05 0.17 0.35 SESOI to RMSE 0.62 0.63 0.58 0.61 0.14 0.46 0.97 R-squared 0.74 0.74 0.70 0.70 0.14 0.48 0.88 MinErr -235.93 -261.63 -253.04 -193.59 43.68 -253.04 -76.70 MaxErr 284.07 309.00 323.53 225.37 89.69 51.02 323.53 MaxAbsErr 284.07 309.00 323.53 260.89 44.88 158.47 323.53 In the second part of this book, we will get back to this example and estimate predictive performance using different models besides linear regression (like baseline prediction and regression trees) References "],
["causal-inference.html", "Chapter 4 Causal inference 4.1 Necessary versus sufficient causality 4.2 Observational data 4.3 Potential outcomes or counterfactuals 4.4 Ceteris paribus and the biases 4.5 Subject matter knowledge 4.6 Example of randomized control trial", " Chapter 4 Causal inference Does playing basketball makes one taller? This is a an example of a causal question. Wrestling with the concept of causality, as a philosophical construct is outside the scope of this book (and the author too), but I will define it using the counterfactual theory or potential outcomes perspective (Hernán, Hsu, and Healy 2019; Kleinberg 2015; Pearl and Mackenzie 2018; Angrist and Pischke 2015; Gelman 2011) that define causes in terms of how things would have been different had the cause not occurred, as well as from causality-as-intervention perspective (Gelman 2011), which necessitates clearly defined interventions (Hernán 2018, 2016; Hernán and Taubman 2008). In other words, would someone be shorter if basketball was never trained? There are two broad classes of inferential questions that focus on what if and why: forward causal inference (“What might happen if we do X?”) and reverse causal inference (“What causes Y? Why?”) (Gelman 2011). Forward causation is more clearly defined problem, where the goal is to quantify the causal effect of treatment. Questions of forward causation are most directly studied using randomization (Gelman 2011) and are answered from the above mentioned causality-as-intervention and counterfactual perspectives. Reverse causation is more complex and it is more related to explaining the causal chains using the system-variable approach. Article by Gelman (2011) provides great overview of the most common causal perspectives, out of which I will mostly focus on forward causation. 4.1 Necessary versus sufficient causality Furthermore, we also need to distinguish between four kinds of causation (Pearl and Mackenzie 2018; Kleinberg 2015): necessary causation, sufficient causation and neither or both. For example, if someone says that A causes B, then: If A is necessary for B, it means that if A never happened (counterfactual reasoning), then B will never happen. Or, in other words, B can never happen without A. But sufficient causality also means that A can happen without B happening. If A is sufficient for B, it means that if you have A, you will always have B. In other words, B always follows A. However, sometimes B can happen without A If A is neither sufficient nor necessary for B, then sometimes when A happens B will happen. B can also happen without A. If A is both necessary and sufficient for B, then B will always happen after A, and B will never happen without A. Table 4.1 contains summary of the above necessary and sufficient causality. In all four types of causation, the concept of counterfactual reasoning is invoked. Table 4.1: Four kinds of causation Cause Necessary Sufficient Neither Both A happens B might happen B always happen B might happen B always happen A doesn’t happen B never happens B might happen B might happen B never happens Although the causal inference is a broad area of research, philosophical discussion and conflicts, there are a few key concepts that need to be introduced to get the big picture and understand the basics behind the aims of causal inference. Let’s start with an example involving the aforementioned question whether playing basketball makes one taller. 4.2 Observational data In order to answer this question, we have collected height data (expressed in cm) for the total of N=30 athletes, of which N=15 play basketball, and N=15 don’t play basketball (Table 4.2). Playing basketball can be considered intervention or treatment, in which causal effect we are interested in. Basketball players are considered intervention group or treatment group and those without the treatment are considered comparison group or control group Table 4.2: Height in the treatment and control groups Athlete Treatment Height (cm) Athlete 27 Basketball 214 Athlete 01 Basketball 214 Athlete 25 Basketball 211 Athlete 19 Basketball 210 Athlete 03 Basketball 207 Athlete 21 Basketball 200 Athlete 23 Basketball 199 Athlete 15 Basketball 198 Athlete 17 Basketball 193 Athlete 07 Basketball 192 Athlete 29 Basketball 192 Athlete 13 Basketball 191 Athlete 05 Basketball 191 Athlete 11 Basketball 184 Athlete 09 Basketball 180 Athlete 02 Control 189 Athlete 28 Control 183 Athlete 06 Control 181 Athlete 14 Control 180 Athlete 04 Control 179 Athlete 12 Control 176 Athlete 18 Control 176 Athlete 08 Control 173 Athlete 26 Control 173 Athlete 24 Control 170 Athlete 30 Control 168 Athlete 20 Control 168 Athlete 16 Control 165 Athlete 10 Control 165 Athlete 22 Control 163 Using descriptive estimators introduced in the Description section, one can quickly calculate the group mean and SD as well as their difference (Table 4.3). But does mean difference between basketball and control represent average causal effect (ACE)26? No, unfortunately not! Table 4.3: Descriptive analysis of the groups Mean (cm) SD (cm) Basketball 198.59 10.86 Control 174.04 7.54 Difference 24.55 13.22 4.3 Potential outcomes or counterfactuals To explain why this is the case, we need to imagine alternate counterfactual reality. What is needed are two potential outcomes: \\(Height_{0}\\), which represents height of the person if one doesn’t train basketball, and \\(Height_{1}\\) which represents height of the person if basketball is being played (Table 4.4). As can be guessed, the Basketball group has known \\(Height_{1}\\), but unknown \\(Height_{0}\\) and vice versa for the Control group. Table 4.4: Counterfactuals of potential outcomes that are unknown Athlete Treatment Height_0 (cm) Height_1 (cm) Height (cm) Causal Effect (cm) Athlete 27 Basketball ??? 214 214 ??? Athlete 01 Basketball ??? 214 214 ??? Athlete 25 Basketball ??? 211 211 ??? Athlete 19 Basketball ??? 210 210 ??? Athlete 03 Basketball ??? 207 207 ??? Athlete 21 Basketball ??? 200 200 ??? Athlete 23 Basketball ??? 199 199 ??? Athlete 15 Basketball ??? 198 198 ??? Athlete 17 Basketball ??? 193 193 ??? Athlete 07 Basketball ??? 192 192 ??? Athlete 29 Basketball ??? 192 192 ??? Athlete 13 Basketball ??? 191 191 ??? Athlete 05 Basketball ??? 191 191 ??? Athlete 11 Basketball ??? 184 184 ??? Athlete 09 Basketball ??? 180 180 ??? Athlete 02 Control 189 ??? 189 ??? Athlete 28 Control 183 ??? 183 ??? Athlete 06 Control 181 ??? 181 ??? Athlete 14 Control 180 ??? 180 ??? Athlete 04 Control 179 ??? 179 ??? Athlete 12 Control 176 ??? 176 ??? Athlete 18 Control 176 ??? 176 ??? Athlete 08 Control 173 ??? 173 ??? Athlete 26 Control 173 ??? 173 ??? Athlete 24 Control 170 ??? 170 ??? Athlete 30 Control 168 ??? 168 ??? Athlete 20 Control 168 ??? 168 ??? Athlete 16 Control 165 ??? 165 ??? Athlete 10 Control 165 ??? 165 ??? Athlete 22 Control 163 ??? 163 ??? Unfortunately, these potential outcomes are unknown, and thus individual causal effects are unknown as well. We just do not know what might have happened to individual outcomes in counterfactual world (i.e. alternate reality). A good control group serves as a proxy to reveal what might have happened on average to the treated group in the counterfactual world where they are not treated. Since the basketball data is simulated, the exact DGP is known (the true systematic or main causal effect of playing basketball on height is exactly zero), which again demonstrates the use of simulations as a great learning tool, in this case understanding the underlying causal mechanisms (Table 4.5). Individual causal effect in this case is the difference between two potential outcomes: \\(Height_{1}\\) and \\(Height_{0}\\). Table 4.5: Simulated causal effects and known counterfactuals Athlete Treatment Height_0 (cm) Height_1 (cm) Height (cm) Causal Effect (cm) Athlete 27 Basketball 214 214 214 0.12 Athlete 01 Basketball 214 214 214 0.00 Athlete 25 Basketball 212 211 211 -1.10 Athlete 19 Basketball 210 210 210 0.90 Athlete 03 Basketball 208 207 207 -0.07 Athlete 21 Basketball 200 200 200 0.20 Athlete 23 Basketball 198 199 199 0.57 Athlete 15 Basketball 198 198 198 0.18 Athlete 17 Basketball 193 193 193 0.44 Athlete 07 Basketball 193 192 192 -0.09 Athlete 29 Basketball 193 192 192 -0.40 Athlete 13 Basketball 192 191 191 -0.36 Athlete 05 Basketball 191 191 191 -0.15 Athlete 11 Basketball 183 184 184 0.46 Athlete 09 Basketball 179 180 180 0.72 Athlete 02 Control 189 189 189 0.06 Athlete 28 Control 183 184 183 0.41 Athlete 06 Control 181 181 181 0.42 Athlete 14 Control 180 180 180 -0.66 Athlete 04 Control 179 179 179 0.10 Athlete 12 Control 176 176 176 -0.39 Athlete 18 Control 176 175 176 -0.31 Athlete 08 Control 173 173 173 -0.55 Athlete 26 Control 173 174 173 0.77 Athlete 24 Control 170 170 170 0.02 Athlete 30 Control 168 168 168 0.27 Athlete 20 Control 168 168 168 -0.03 Athlete 16 Control 165 165 165 -0.01 Athlete 10 Control 165 164 165 -0.81 Athlete 22 Control 163 163 163 0.00 From Table 4.5, we can state that the mean difference between the groups consists of two components: average causal effect and the selection bias (Angrist and Pischke 2015) (Equation (4.1)). \\[ \\begin{equation} \\begin{split} mean_{difference} &amp;= Average \\; causal\\; effect + Selection\\; bias \\\\ Average \\; causal\\; effect &amp;= \\frac{1}{N_{Basketball}}\\Sigma_{i=1}^{n}(Height_{1i} - Height_{0i}) \\\\ Selection\\; bias &amp;= \\frac{1}{N_{Basketball}}\\Sigma_{i=1}^{n}Height_{0i} - \\frac{1}{N_{Control}}\\Sigma_{i=1}^{n}Height_{0i} \\end{split} \\tag{4.1} \\end{equation} \\] The mean group difference we have observed (24.55cm) is due to average causal effect (0.1cm) and selection bias (24.46cm). In other words, observed mean group difference can be explained solely by selection bias. Since we know the DGP behind the basketball data, we know that there is no systematic causal effect of playing basketball on height. On top of the selection bias involved in the example above, other confounders might be involved, such as age, sex, race, experience and others, some of which can be measured and some might be unknown. These are also referred to as the third variable which confounds the causal relationship between treatment and the outcome. In this example, all subjects from the Basketball group might be older males, whereas all the subjects from the Control group might be be younger females, and this can explain the group differences, rather than causal effect of playing basketball. 4.4 Ceteris paribus and the biases It is important to understand that, in order to have causal interpretation, comparisons need to be made under ceteris paribus conditions (Angrist and Pischke 2015), which is Latin for other things equal. In the basketball example above, we cannot make causal claim that playing basketball makes one taller, since comparison between the groups is not done in the ceteris paribus conditions due to the selection bias involved. We also know this since we know the DGP behind the observed data. Causal inference thus aims to achieve ceteris paribus conditions needed to make causal interpretations by careful considerations of the known and unknown biases involved (Angrist and Pischke 2015; Hernán 2017, 2016; Hernán and Robins, n.d.; Hernán, Hsu, and Healy 2019; Lederer et al. 2019; Rohrer 2018; Shrier and Platt 2008). According to Hernan et al. (Hernán 2017; Hernán and Robins, n.d.), there are three types of biases involved in causal inference: confounding, selection bias and measurement bias. Confounding is the bias that arises when treatment and outcome share causes. This is because treatment was not randomly assigned (Hernán 2017; Hernán and Robins, n.d.). For example, athletes that are naturally taller might be choosing to play basketball due to success and enjoyment over their shorter peers. On the other hand, it might be some hidden confounder that motivates to-be-tall athletes to choose basketball. Known and measured confounders from the observational studies can be taken into account to create ceteris paribus conditions when estimating causal effects (Angrist and Pischke 2015; Hernán 2017; Hernán and Robins, n.d.; Lederer et al. 2019; Rohrer 2018; Shrier and Platt 2008). 4.4.1 Randomization The first line of defence against confounding and selection bias is to randomly assign athletes to treatment, otherwise known as randomized trial or randomized experiment. Random assignment makes comparison between groups ceteris paribus providing the sample is large enough to ensure that differences in the individual characteristics such as age, sex, experience and other potential confounders are washed out (Angrist and Pischke 2015). In other words, random assignment works not by eliminating individual differences but rather by ensuring that the mix of the individuals being compared is the same, including the ways we cannot easily measure or observe (Angrist and Pischke 2015). In case the individuals from the basketball example were randomly assigned, given the known causal DGP, then the mean difference between the groups would be more indicative of the causal effect of playing basketball on height (Table 4.6). Table 4.6: Randomized participants Athlete Treatment Height (cm) Athlete 01 Basketball 214 Athlete 25 Basketball 211 Athlete 19 Basketball 210 Athlete 03 Basketball 207 Athlete 23 Basketball 199 Athlete 15 Basketball 198 Athlete 13 Basketball 191 Athlete 02 Basketball 189 Athlete 28 Basketball 184 Athlete 14 Basketball 180 Athlete 04 Basketball 179 Athlete 12 Basketball 176 Athlete 26 Basketball 174 Athlete 24 Basketball 170 Athlete 16 Basketball 165 Athlete 27 Control 214 Athlete 21 Control 200 Athlete 29 Control 193 Athlete 07 Control 193 Athlete 17 Control 193 Athlete 05 Control 191 Athlete 11 Control 183 Athlete 06 Control 181 Athlete 09 Control 179 Athlete 18 Control 176 Athlete 08 Control 173 Athlete 30 Control 168 Athlete 20 Control 168 Athlete 10 Control 165 Athlete 22 Control 163 If we calculate the mean differences in this randomly assigned basketball treatment (Table 4.7), we can quickly notice that random assignment washed out selection bias involved with the observational study, and that the mean difference is closer to the known systematic (or average or expected) causal effect. The difference between estimated systematic causal effect using mean group difference from the randomized trial and the true causal effect is due to the sampling error which will be explained in the [Statistical inference] section. Table 4.7: Descriptive summary of randomized participants Mean (cm) SD (cm) Basketball 189.91 16.15 Control 182.65 14.44 Difference 7.25 21.66 Apart from creating ceteris paribus conditions, randomization generates a good control group that serves as a proxy to reveal what might have happened to the treated group in the counterfactual world where they are not treated, since \\(Height_0\\) is not known for the basketball group. Creating those conditions with randomized trial demands careful considerations and balance checking since biases can crawl inside the causal interpretation. The logic of randomized trial is simple, yet the logistics can be quite complex. For example, a sample of sufficient size might not be practically feasible, and imbalances in the known confounders can be still found in the groups, thus demanding further control and adjustment in the analysis (e.g. using ANCOVA instead of ANOVA, adjusting for confounders in the linear regression by introducing them as interactions) in order to create ceteris paribus conditions needed to evaluate causal claims. Belief effect can sneak in, for example, if the treatment group knows they are being treated, or if researchers motivate treatment groups harder, since they expect and hope for better outcomes. For this reason, blinding both the subjects and researches can be considered, as well as providing placebo treatment to the Control group. In sport science research blinding and providing placebo can be problematic. For example, if our intervention is a novel training method or a technology, both researchers and subjects will expect better outcomes which can bias causal interpretations. 4.5 Subject matter knowledge One of the main problems with randomized trials is that it cannot be done in most real life settings, either due to the ethical or practical reasons. For example, if studying effects of smoking on baby mortality and birth defects, which parent would accept being in the treatment group. Or if studying effects of resistance training on injury risk in football players, which professional organization would allow random assignment to the treatment that is lesser than the known best practices and can predispose athletes to the injuries or sub-par preparation? For this reason, reliance on observation studies is the best we can do. However, in order to create ceteris paribus conditions necessary to minimize bias in the causal interpretations, expert subject-matter knowledge is needed, not only to describe the causal structure of the system under study, but also to specify the causal questions and identify relevant data sources (Hernán, Hsu, and Healy 2019). Imagine asking the following causal question: “Does training load lead to overuse injuries in professional sports”. It takes expert subject matter knowledge to specify the treatment construct (i.e. “training load”), to figure out how should be measured, as well as to quantify the measurement error which can induce measurement bias, to state over which time period the treatment is done, as well as to specify the outcome construct (i.e. “overuse-injuries”), and to define the variables and constructs that confound and define the causal network underlying such a question. This subject matter is fallible of course, and the constructs, variables and the causal network can be represented with pluralistic models that represents “Small World” maps of the complex “Large World”, in which we are hoping to deploy the findings (please refer to the Introduction for more information about this concept). Drawing assumptions that underly causal structure using direct acyclical graphs (DAGs) (Hernán 2017; Hernán and Robins, n.d.; Pearl and Mackenzie 2018; Rohrer 2018; Saddiki and Balzer 2018; Shrier and Platt 2008; Textor et al. 2017) represents a step forward in acknowledging the issues above, by providing transparency of the assumptions involved and bridging the subjective - objective dichotomy. 4.6 Example of randomized control trial Let’s consider the following example. We are interested in estimating causal effect of the plyometric training on the vertical jump height. To estimate causal effect, randomized control trial (RCT) is utilized. RCT utilizes two groups: Treatment (N=15) and Control (N=15), measured two times: Pre-test and Post-test. Treatment group received plyometric training over the course of three months, while Control group continued with normal training. The results of RCT study can be found in the Table 4.8. To estimate practical significance of the treatment effect, SESOI of ±2.5cm is selected to indicate minimal change of the practical value. It is important to have “well defined interventions” (Hernán 2018, 2016; Hernán and Taubman 2008), thus the question that should be answered is as follows: “Does plyometric training added to normal training improves vertical jump height over period of three months?” Table 4.8: Randomized control trial data Athlete Group Pre-test (cm) Post-test (cm) Change (cm) Athlete 19 Treatment 45.86 60.74 14.88 Athlete 27 Treatment 44.67 59.04 14.37 Athlete 01 Treatment 37.98 52.27 14.29 Athlete 03 Treatment 41.36 52.70 11.35 Athlete 25 Treatment 39.99 50.09 10.09 Athlete 23 Treatment 47.84 57.35 9.51 Athlete 21 Treatment 36.94 46.20 9.26 Athlete 15 Treatment 46.51 54.85 8.33 Athlete 17 Treatment 44.37 51.33 6.96 Athlete 07 Treatment 41.12 46.59 5.47 Athlete 05 Treatment 38.07 42.84 4.77 Athlete 29 Treatment 43.17 47.74 4.58 Athlete 13 Treatment 50.02 54.07 4.05 Athlete 11 Treatment 44.80 48.27 3.47 Athlete 09 Treatment 46.89 49.38 2.49 Athlete 18 Control 42.88 43.55 0.67 Athlete 02 Control 49.57 50.11 0.54 Athlete 12 Control 42.95 43.47 0.52 Athlete 28 Control 46.65 46.90 0.25 Athlete 08 Control 41.66 41.64 -0.01 Athlete 22 Control 36.52 36.20 -0.32 Athlete 04 Control 44.42 43.33 -1.09 Athlete 16 Control 37.64 36.44 -1.20 Athlete 10 Control 37.58 36.35 -1.23 Athlete 20 Control 38.98 37.31 -1.67 Athlete 30 Control 39.07 37.31 -1.76 Athlete 14 Control 45.18 43.35 -1.83 Athlete 06 Control 45.54 42.84 -2.70 Athlete 26 Control 41.54 38.69 -2.85 Athlete 24 Control 40.13 36.82 -3.31 Descriptive summary statistics for Treatment and Control groups are enlisted in the Table 4.9, and visually depicted in the Figure 4.1. Table 4.9: RCT summary using mean ± SD Group Pre-test (cm) Post-test (cm) Change (cm) Treatment 43.31 ± 3.93 51.56 ± 5.03 8.26 ± 4.16 Control 42.02 ± 3.77 40.95 ± 4.31 -1.07 ± 1.31 Figure 4.1: Visual analysis of RCT using Treatment and Control groups. A and B. Raincloud plot of the Pre-test and Post-test scores for Treatment and Control groups. Blue color indicates Control group and orange color indicates Treatment group. C and D. Raincloud plot of the change scores for the Treatment and Control groups. SESOI is indicated with a grey band Further analysis might involve separate dependent groups analysis for both Treatment and Control (Table 4.10), or in other words, the analysis of the change scores. To estimate Cohen's d, pooled SD of the Pre-test scores in both Treatment and Control is utilized. (see Equation (2.11)). Table 4.10: Descriptive analysis of the change scores for Treatment and Control groups independently Estimator Treatment Control Mean change (cm) 8.26 -1.07 SDchange (cm) 4.16 1.31 SDpre-test pooled (cm) 3.85 3.85 Cohen’s d 2.14 -0.28 SESOI lower (cm) -2.50 -2.50 SESOI upper (cm) 2.50 2.50 Change to SESOI 1.65 -0.21 SDchange to SESOI 0.83 0.26 pLower 0.00 0.14 pEquivalent 0.08 0.86 pHigher 0.92 0.00 Figure 4.2 depicts same information as Figure 4.1 but organized differently and conveying different comparison. Figure 4.2: Visual analysis of RCT using Treatment and Control groups. A and B. Scatter plot of Pre-test and Post-test scores for Treatment and Control groups. Green line indicates change higher than SESOI upper, grey line indicates change within SESOI band, and red line indicates negative change lower than SESOI lower. C. Distribution of the change scores for Treatment (orange) and Control (blue) groups. Grey rectangle indicates SESOI band. But we are not that interested in independent analysis of Treatment and Control groups, but rather on their differences and understanding of the causal effect of the treatment (i.e. understanding and estimating parameters of the underlying DGP). As stated, treatment effect consists of two components: systematic component or main effect (i.e. expected or average causal effect), and stochastic component or random effect (i.e. that varies between individuals) (see Figure 4.3). As already explained, Control group serves as a proxy to what might have happened to the Treatment group in the counterfactual world, and thus allows for casual interpretation of the treatment effect. There are two effects at play with this RCT design: treatment effect and non-treatment effect. The latter captures all effects not directly controlled by a treatment, but assumes it affects both groups equally (Figure 4.3). For example, if we are treating kids for longer period of time, non-treatment effect might be related to the growth and associated effects. Another non-treatment effect is measurement error (discussed in more details in [Measurement Error] section). Figure 4.3: Treatment and Non-treatment effects of intervention. Both treatment and non-treatment effects consists of two components: systematic and random. Treatment group experiences both treatment and non-treatment effects, while Control group experiences only non-treatment effects. The following equation captures the essence of estimating Treatment effects from Pre-test and Post-test scores in the Treatment and Control groups (Equation (4.2)): \\[ \\begin{equation} \\begin{split} Treatment_{post} &amp;= Treatment_{pre} + Treatment \\; Effect + NonTreatment \\; Effect \\\\ Control_{post} &amp;= Control_{pre} + NonTreatment \\; Effect \\\\ \\\\ NonTreatment \\; Effect &amp;= Control_{post} - Control_{pre} \\\\ Treatment \\; Effect &amp;= Treatment_{post} - Treatment_{pre} - NonTreatment \\; Effect \\\\ \\\\ Treatment \\; Effect &amp;= (Treatment_{post} - Treatment_{pre}) - (Control_{post} - Control_{pre}) \\\\ Treatment \\; Effect &amp;= Treatment_{change} - Control_{change} \\end{split} \\tag{4.2} \\end{equation} \\] From the Equation (4.2), the differences between the changes in Treatment and Control groups can be interpreted as the estimate of the causal effect of the treatment. More precisely, average causal effect or expected causal effect represent systematic treatment effect. This is estimated using difference between mean Treatment change and mean Control change. Table 4.11 contains descriptive statistics of the change score differences. Panel C in the Figure 4.2 depicts distribution of the change scores and reflect the calculus in the Table 4.11 graphically. Table 4.11: Descriptive statistics of the change score differences Mean difference (cm) Cohen’s d Difference to SESOI pLower diff pEquivalent diff pHigher diff 9.32 7.14 1.86 -0.13 -0.78 0.91 Cohen's d in the Table 4.11 is calculated by using the Equation (4.3) and it estimates standardized difference between change scores in Treatment and the Control groups. \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{treatment\\; group \\; change} - mean_{control\\; group \\;change}}{SD_{control\\; group \\; change}} \\tag{4.3} \\end{equation} \\] Besides estimating systematic component of the treatment (i.e. the difference between the mean change in Treatment and Control groups), we might be interested in estimating random component and proportions of lower, equivalent and higher effects compared to SESOI (pLower, pEquivalent, and pHigher). Unfortunately, differences in pLower, pEquivalent, and pHigher from Table 4.11 don’t answer this question, but rather the expected difference in proportions compared to Control (e.g. the expected improvement of 0.91 in observing proportion of higher change outcomes compared to Control). Since the changes in Treatment group are due both to the treatment and non-treatment effects (equation 29), the average treatment effect (systematic component) represents the difference between the mean changes in Treatment and Control groups (Table 4.11). In the same manner, the variance of the change scores in the Treatment group are due to the random component of the treatment and non-treatment effects. Assuming normal (Gaussian) distribution of the random components, the SD of the treatment effects (\\(SD_{TE}\\))27 is estimated using the following Equation (4.4). \\[ \\begin{equation} \\begin{split} \\epsilon_{treatment \\;group \\;change} &amp;= \\epsilon_{treatment \\; effect} + \\epsilon_{nontreatment \\; effect} \\\\ \\epsilon_{control \\;group \\;change} &amp;= \\epsilon_{nontreatment \\; effect} \\\\ \\epsilon_{treatment \\; effect} &amp;= \\epsilon_{treatment \\;group \\;change} - \\epsilon_{control \\;group \\;change} \\\\ \\\\ \\epsilon_{treatment \\; effect} &amp;\\sim \\mathcal{N}(0,\\,SD_{TE}) \\\\ \\epsilon_{nontreatment \\; effect} &amp;\\sim \\mathcal{N}(0,\\,SD_{NTE}) \\\\ \\epsilon_{treatment \\;group \\;change} &amp;\\sim \\mathcal{N}(0,\\,SD_{treatment \\;group \\;change}) \\\\ \\epsilon_{control \\;group \\;change} &amp;\\sim \\mathcal{N}(0,\\,SD_{control \\;group \\;change}) \\\\ \\\\ SD_{TE} &amp;= \\sqrt{SD_{treatment \\;group \\;change}^2 - SD_{control \\;group \\;change}^2} \\end{split} \\tag{4.4} \\end{equation} \\] This neat mathematical solution is due to assumption of Gaussian error, assumption that random treatment and non-treatment effects are equal across subjects (see [Ergodicity] section for more details about this assumption), and the use of squared errors. This is one beneficial property of using squared errors that I alluded to in the section Cross-Validation section. Thus, the estimated parameters of the causal treatment effects in the underlying DGP are are summarized with the following Equation (4.5). This treatment effect is graphically depicted in the Figure 4.4. \\[ \\begin{equation} \\begin{split} Treatment \\; effect &amp;\\sim \\mathcal{N}(Mean_{TE},\\,SD_{TE}) \\\\ \\\\ Mean_{TE} &amp;= Mean_{treatment \\;group \\;change} - Mean_{control \\;group \\;change} \\\\ \\\\ SD_{TE} &amp;= \\sqrt{SD_{treatment \\;group \\;change}^2 - SD_{control \\; group \\; change}^2} \\end{split} \\tag{4.5} \\end{equation} \\] Figure 4.4: Graphical representation of the causal Treatment effect. Green area indicates proportion of higher than SESOI treatment effects, red indicates proportion of negative and lower than SESOI treatment effects, and grey indicates treatment effects that are within SESOI. Mean of treatment effect distribution represents average (or expected) causal effect or systematic treatment effect. SD of treatment effect distribution represents random systematic effect or \\(SD_{TE}\\) Using SESOI, one can also estimate the proportion of lower, equivalent and higher changes (responses) caused by treatment. The estimates of the causal treatment effects, with accompanying proportions of responses are enlisted in the Table 4.12. Table 4.12: Estimates of the causal treatment effects Average causal effect (cm) Random effect (cm) SESOI (cm) Average causal effect to SESOI SESOI to random effect pLower pEquivalent pHigher 9.32 3.95 ±2.5 1.86 1.27 0 0.04 0.96 Therefore, we can conclude that plyometric training over three months period, on top of the normal training, cause improvements in vertical jump height (in the sample collected; generalizations beyond sample are discussed in the [Statistical inference] section). The expected improvement (i.e. average causal effect or systematic effect) is equal to 9.32cm, with 0, 4, and 96% of athletes having lower, trivial and higher improvements. References "],
["references.html", "References", " References Albanese, Davide, Michele Filosi, Roberto Visintainer, Samantha Riccadonna, Giuseppe Jurman, and Cesare Furlanello. 2012a. “Minerva and Minepy: A c Engine for the Mine Suite and Its R, Python and Matlab Wrappers.” Bioinformatics, bts707. ———. 2012b. “Minerva and Minepy: A C Engine for the MINE Suite and Its R, Python and MATLAB Wrappers.” Bioinformatics, bts707. Allaire, JJ, Jeffrey Horner, Yihui Xie, Vicent Marti, and Natacha Porte. 2019. Markdown: Render Markdown with the c Library ’Sundown’. https://CRAN.R-project.org/package=markdown. Allen, Micah, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and Rogier Kievit. 2018. “Raincloudplots Tutorials and Codebase.” Zenodo. https://doi.org/10.5281/zenodo.1402959. Allen, Micah, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and Rogier A. Kievit. 2019. “Raincloud Plots: A Multi-Platform Tool for Robust Data Visualization.” Wellcome Open Research 4 (April): 63. https://doi.org/10.12688/wellcomeopenres.15191.1. Altmann, Thomas, Jakob Bodensteiner, Cord Dankers, Thommy Dassen, Nikolas Fritz, Sebastian Gruber, Philipp Kopper, Veronika Kronseder, Moritz Wagner, and Emanuel Renkl. 2019. Limitations of Interpretable Machine Learning Methods. Amrhein, Valentin, David Trafimow, and Sander Greenland. 2019. “Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis If We Don’t Expect Replication.” The American Statistician 73 (sup1): 262–70. https://doi.org/10.1080/00031305.2018.1543137. Angrist, Joshua David, and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton ; Oxford: Princeton University Press. Anvari, Farid, and Daniel Lakens. 2019. “Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest,” March. https://doi.org/10.31234/osf.io/syp5a. Barron, Jonathan T. 2019. “A General and Adaptive Robust Loss Function.” arXiv:1701.03077 [Cs, Stat], April. http://arxiv.org/abs/1701.03077. Beaujean, A. Alexander. 2014. Latent Variable Modeling Using R: A Step by Step Guide. New York: Routledge/Taylor &amp; Francis Group. Biecek, Przemyslaw, and Tomasz Burzykowski. 2019. Predictive Models: Explore, Explain, and Debug. Binmore, Ken. 2011. Rational Decisions. Fourth Impression edition. Princeton, NJ: Princeton University Press. Borsboom, Denny. 2008. “Latent Variable Theory.” Measurement: Interdisciplinary Research &amp; Perspective 6 (1-2): 25–53. https://doi.org/10.1080/15366360802035497. Borsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2003. “The Theoretical Status of Latent Variables.” Psychological Review 110 (2): 203–19. https://doi.org/10.1037/0033-295X.110.2.203. Botchkarev, Alexei. 2019. “A New Typology Design of Performance Metrics to Measure Errors in Machine Learning Regression Algorithms.” Interdisciplinary Journal of Information, Knowledge, and Management 14: 045–076. https://doi.org/10.28945/4184. Breheny, Patrick, and Woodrow Burchett. 2017. “Visualization of Regression Models Using Visreg.” The R Journal 9 (2): 56–71. Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–215. Buchheit, Martin, and Alireza Rabbani. 2014. “The 3015 Intermittent Fitness Test Versus the Yo-Yo Intermittent Recovery Test Level 1: Relationship and Sensitivity to Training.” International Journal of Sports Physiology and Performance 9 (3): 522–24. https://doi.org/10.1123/ijspp.2012-0335. Caldwell, Aaron R., and Samuel N. Cheuvront. 2019. “Basic Statistical Considerations for Physiology: The Journal Temperature Toolbox.” Temperature, June, 1–30. https://doi.org/10.1080/23328940.2019.1624131. Carsey, Thomas, and Jeffrey Harden. 2013. Monte Carlo Simulation and Resampling Methods for Social Science. 1 edition. Los Angeles: Sage Publications, Inc. Chai, T., and R. R. Draxler. 2014. “Root Mean Square Error (RMSE) or Mean Absolute Error (MAE)? Arguments Against Avoiding RMSE in the Literature.” Geoscientific Model Development 7 (3): 1247–50. https://doi.org/10.5194/gmd-7-1247-2014. Cohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates. Davison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/. Everitt, Brian, and Torsten Hothorn. 2011. An Introduction to Applied Multivariate Analysis with R. Use R! New York: Springer. Finch, W. Holmes, and Brian F. French. 2015. Latent Variable Modeling with R. New York: Routledge, Taylor &amp; Francis Group. Foreman, John W. 2014. Data Smart: Using Data Science to Transform Information into Insight. Hoboken, New Jersey: John Wiley &amp; Sons. Fox, John. 2003. “Effect Displays in R for Generalised Linear Models.” Journal of Statistical Software 8 (15): 1–27. http://www.jstatsoft.org/v08/i15/. Fox, John, and Jangman Hong. 2009. “Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package.” Journal of Statistical Software 32 (1): 1–24. http://www.jstatsoft.org/v32/i01/. Fox, John, and Sanford Weisberg. 2018. “Visualizing Fit and Lack of Fit in Complex Regression Models with Predictor Effect Plots and Partial Residuals.” Journal of Statistical Software 87 (9): 1–27. https://doi.org/10.18637/jss.v087.i09. Fox, John, Sanford Weisberg, and Brad Price. 2019. CarData: Companion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1–22. Gelman, Andrew. 2011. “Causality and Statistical Learning.” American Journal of Sociology 117 (3): 955–66. https://doi.org/10.1086/662659. Gelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 967–1033. https://doi.org/10.1111/rssa.12276. Gigerenzer, Gerd, Ralph Hertwig, and Thorsten Pachur. 2015. Heuristics: The Foundations of Adaptive Behavior. Reprint edition. Oxford University Press. Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2013. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” arXiv:1309.6392 [Stat], September. http://arxiv.org/abs/1309.6392. Greenwell, Brandon, Brad Boehmke, and Bernie Gray. 2020. Vip: Variable Importance Plots. https://CRAN.R-project.org/package=vip. Greenwell, Brandon M. 2017a. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://journal.r-project.org/archive/2017/RJ-2017-016/index.html. ———. 2017b. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://doi.org/10.32614/RJ-2017-016. Hamner, Ben, and Michael Frasco. 2018. Metrics: Evaluation Metrics for Machine Learning. https://CRAN.R-project.org/package=Metrics. Henry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr. Henry, Lionel, Hadley Wickham, and Winston Chang. 2020. Ggstance: Horizontal ’Ggplot2’ Components. https://CRAN.R-project.org/package=ggstance. Hernán, M A, and S L Taubman. 2008. “Does Obesity Shorten Life? The Importance of Well-Defined Interventions to Answer Causal Questions.” International Journal of Obesity 32 (S3): S8–S14. https://doi.org/10.1038/ijo.2008.82. Hernán, Miguel A. 2016. “Does Water Kill? A Call for Less Casual Causal Inferences.” Annals of Epidemiology 26 (10): 674–80. https://doi.org/10.1016/j.annepidem.2016.08.016. ———. 2017. “Causal Diagrams: Draw Your Assumptions Before Your Conclusions Course | PH559x | edX.” edX. https://courses.edx.org/courses/course-v1:HarvardX+PH559x+3T2017/course/. ———. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/AJPH.2018.304337. Hernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” CHANCE 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578. Hernán, Miguel A., and JM Robins. n.d. Causal Inference. Boca Raton: Chapman &amp; Hall/CRC. Hocking, Toby Dylan. 2020. Directlabels: Direct Labels for Multicolor Plots. https://CRAN.R-project.org/package=directlabels. Hopkins, Will G. 2004. “How to Interpret Changes in an Athletic Performance Test,” November, 2. ———. 2006. “New View of Statistics: Effect Magnitudes.” https://www.sportsci.org/resource/stats/effectmag.html. Hopkins, Will G. 2015. “Individual Responses Made Easy.” Journal of Applied Physiology 118 (12): 1444–6. https://doi.org/10.1152/japplphysiol.00098.2015. ———. 2007. “Understanding Statistics by Using Spreadsheets to Generate and Analyze Samples.” Sportscience.org. https://www.sportsci.org/2007/wghstats.htm. Hopkins, William G., Stephen W. Marshall, Alan M. Batterham, and Juri Hanin. 2009. “Progressive Statistics for Studies in Sports Medicine and Exercise Science:” Medicine &amp; Science in Sports &amp; Exercise 41 (1): 3–13. https://doi.org/10.1249/MSS.0b013e31818cb278. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning: With Applications in R. 1st ed. 2013, Corr. 7th printing 2017 edition. New York: Springer. Jovanović, Mladen. 2020. bmbstats: Bootstrap Magnitude-Based Statistics. Belgrade, Serbia. https://github.com/mladenjovanovic/bmbstats. Kabacoff, Robert. 2015. R in Action: Data Analysis and Graphics with R. Second edition. Shelter Island: Manning. King, Madeleine T. 2011. “A Point of Minimal Important Difference (MID): A Critique of Terminology and Methods.” Expert Review of Pharmacoeconomics &amp; Outcomes Research 11 (2): 171–84. https://doi.org/10.1586/erp.11.9. Kleinberg, Jon, Annie Liang, and Sendhil Mullainathan. 2017. “The Theory Is Predictive, but Is It Complete? An Application to Human Perception of Randomness.” arXiv:1706.06974 [Cs, Stat], June. http://arxiv.org/abs/1706.06974. Kleinberg, Samantha. 2018. Causality, Probability, and Time. ———. 2015. Why: A Guide to Finding and Using Causes. 1 edition. Beijing ; Boston: O’Reilly Media. Kruschke, John K., and Torrin M. Liddell. 2018a. “Bayesian Data Analysis for Newcomers.” Psychonomic Bulletin &amp; Review 25 (1): 155–77. https://doi.org/10.3758/s13423-017-1272-1. ———. 2018b. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin &amp; Review 25 (1): 178–206. https://doi.org/10.3758/s13423-016-1221-4. Kuhn, Max. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret. Kuhn, Max, and Kjell Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Milton: CRC Press LLC. ———. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2016 edition. New York: Springer. Kuhn, Max, Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, et al. 2018. Caret: Classification and Regression Training. Lakens, Daniël, Anne M. Scheel, and Peder M. Isager. 2018. “Equivalence Testing for Psychological Research: A Tutorial.” Advances in Methods and Practices in Psychological Science 1 (2): 259–69. https://doi.org/10.1177/2515245918770963. Lang, Kyle M., Shauna J. Sweet, and Elizabeth M. Grandfield. 2017. “Getting Beyond the Null: Statistical Modeling as an Alternative Framework for Inference in Developmental Science.” Research in Human Development 14 (4): 287–304. https://doi.org/10.1080/15427609.2017.1371567. Lantz, Brett. 2019. Machine Learning with R: Expert Techniques for Predictive Modeling. Lederer, David J., Scott C. Bell, Richard D. Branson, James D. Chalmers, Rachel Marshall, David M. Maslove, David E. Ost, et al. 2019. “Control of Confounding and Reporting of Results in Causal Inference Studies. Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals.” Annals of the American Thoracic Society 16 (1): 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS. Ludbrook, John. 1997. “SPECIAL ARTICLE COMPARING METHODS OF MEASUREMENT.” Clinical and Experimental Pharmacology and Physiology 24 (2): 193–203. https://doi.org/10.1111/j.1440-1681.1997.tb01807.x. ———. 2002. “Statistical Techniques for Comparing Measurers and Methods of Measurement: A Critical Review.” Clinical and Experimental Pharmacology and Physiology 29 (7): 527–36. https://doi.org/10.1046/j.1440-1681.2002.03686.x. ———. 2010. “Linear Regression Analysis for Comparing Two Measurers or Methods of Measurement: But Which Regression?: Linear Regression for Comparing Methods.” Clinical and Experimental Pharmacology and Physiology 37 (7): 692–99. https://doi.org/10.1111/j.1440-1681.2010.05376.x. ———. 2012. “A Primer for Biomedical Scientists on How to Execute Model II Linear Regression Analysis: Model II Linear Regression Analysis.” Clinical and Experimental Pharmacology and Physiology 39 (4): 329–35. https://doi.org/10.1111/j.1440-1681.2011.05643.x. Makowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019. “BayestestR: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” Journal of Open Source Software 4 (40): 1541. https://doi.org/10.21105/joss.01541. McElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 1 edition. Boca Raton: Chapman and Hall/CRC. McGraw, Kenneth O., and S. P. Wong. 1992. “A Common Language Effect Size Statistic.” Psychological Bulletin 111 (2): 361–65. https://doi.org/10.1037/0033-2909.111.2.361. Miller, Tim. 2017. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv:1706.07269 [Cs], June. http://arxiv.org/abs/1706.07269. Mitchell, Sandra. 2012. Unsimple Truths: Science, Complexity, and Policy. Paperback ed. Chicago, Mich.: The Univ. of Chicago Press. Mitchell, Sandra D. 2002. “Integrative Pluralism.” Biology &amp; Philosophy 17 (1): 55–70. https://doi.org/10.1023/A:1012990030867. Molnar, Christoph. 2018. Interpretable Machine Learning. Leanpub. Molnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “Iml: An R Package for Interpretable Machine Learning.” JOSS 3 (26): 786. https://doi.org/10.21105/joss.00786. Mullineaux, David R., Christopher A. Barnes, and Alan M. Batterham. 1999. “Assessment of Bias in Comparing Measurements: A Reliability Example.” Measurement in Physical Education and Exercise Science 3 (4): 195–205. https://doi.org/10.1207/s15327841mpee0304_1. Müller, Kirill, and Hadley Wickham. 2020. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble. Page, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for You. Basic Books. Pearl, Judea. 2009. “Causal Inference in Statistics: An Overview.” Statistics Surveys 3 (0): 96–146. https://doi.org/10.1214/09-SS057. ———. 2019. “The Seven Tools of Causal Inference, with Reflections on Machine Learning.” Communications of the ACM 62 (3): 54–60. https://doi.org/10.1145/3241036. Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. 1 edition. Chichester, West Sussex: Wiley. Pearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. 1 edition. New York: Basic Books. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reshef, D. N., Y. A. Reshef, H. K. Finucane, S. R. Grossman, G. McVean, P. J. Turnbaugh, E. S. Lander, M. Mitzenmacher, and P. C. Sabeti. 2011. “Detecting Novel Associations in Large Data Sets.” Science 334 (6062): 1518–24. https://doi.org/10.1126/science.1205438. Revelle, William. 2019. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier.” arXiv:1602.04938 [Cs, Stat], February. http://arxiv.org/abs/1602.04938. Rohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629. Rousselet, Guillaume A., Cyril R. Pernet, and Rand R. Wilcox. 2017. “Beyond Differences in Means: Robust Graphical Methods to Compare Two Groups in Neuroscience.” European Journal of Neuroscience 46 (2): 1738–48. https://doi.org/10.1111/ejn.13610. Rousselet, Guillaume A, Cyril R Pernet, and Rand R. Wilcox. 2019. “A Practical Introduction to the Bootstrap: A Versatile Method to Make Inferences by Using Data-Driven Simulations.” https://doi.org/10.31234/osf.io/h8ft7. Saddiki, Hachem, and Laura B. Balzer. 2018. “A Primer on Causality in Data Science.” arXiv:1809.02408 [Stat], September. http://arxiv.org/abs/1809.02408. Sainani, Kristin L. 2012. “Clinical Versus Statistical Significance.” PM&amp;R 4 (6): 442–45. https://doi.org/10.1016/j.pmrj.2012.04.014. Sarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with R. New York: Springer. http://lmdvr.r-forge.r-project.org. Savage, Leonard J. 1972. The Foundations of Statistics. 2nd Revised ed. edition. New York: Dover Publications. Shmueli, Galit. 2010. “To Explain or to Predict?” Statistical Science 25 (3): 289–310. https://doi.org/10.1214/10-STS330. Shrier, Ian, and Robert W Platt. 2008. “Reducing Bias Through Directed Acyclic Graphs.” BMC Medical Research Methodology 8 (1). https://doi.org/10.1186/1471-2288-8-70. Swinton, Paul A., Ben Stephens Hemingway, Bryan Saunders, Bruno Gualano, and Eimear Dolan. 2018. “A Statistical Framework to Interpret Individual Response to Intervention: Paving the Way for Personalized Nutrition and Exercise Prescription.” Frontiers in Nutrition 5 (May). https://doi.org/10.3389/fnut.2018.00041. Textor, Johannes, Benito van der Zander, Mark S. Gilthorpe, Maciej Li’skiewicz, and George T. H. Ellison. 2017. “Robust Causal Inference Using Directed Acyclic Graphs: The R Package ‘Dagitty’.” International Journal of Epidemiology, January, dyw341. https://doi.org/10.1093/ije/dyw341. Therneau, Terry, and Beth Atkinson. 2019. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. Turner, Anthony, Jon Brazier, Chris Bishop, Shyam Chavda, Jon Cree, and Paul Read. 2015. “Data Analysis for Strength and Conditioning Coaches: Using Excel to Analyze Reliability, Differences, and Relationships.” Strength and Conditioning Journal 37 (1): 76–83. https://doi.org/10.1519/SSC.0000000000000113. Watts, Duncan J, Emorie D Beck, Elisa Jayne Bienenstock, Jake Bowers, Aaron Frank, Anthony Grubesic, Jake Hofman, Julia Marie Rohrer, and Matthew Salganik. 2018. “Explanation, Prediction, and Causality: Three Sides of the Same Coin?” October. https://doi.org/10.31219/osf.io/u6vz5. Weinberg, Gabriel, and Lauren McCann. 2019. Super Thinking: The Big Book of Mental Models. New York: Portfolio/Penguin. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr. ———. 2020. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, and Lionel Henry. 2020. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr. Wickham, Hadley, Jim Hester, and Romain Francois. 2018. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr. Wikipedia contributors. 2019. “Causal Model.” Wilcox, Rand, Travis J. Peterson, and Jill L. McNitt-Gray. 2018. “Data Analyses When Sample Sizes Are Small: Modern Advances for Dealing with Outliers, Skewed Distributions, and Heteroscedasticity.” Journal of Applied Biomechanics 34 (4): 258–61. https://doi.org/10.1123/jab.2017-0269. Wilcox, Rand R. 2016. Introduction to Robust Estimation and Hypothesis Testing. 4th edition. Waltham, MA: Elsevier. Wilcox, Rand R., and Guillaume A. Rousselet. 2017. “A Guide to Robust Statistical Methods in Neuroscience.” bioRxiv, June. https://doi.org/10.1101/151811. Wilke, Claus O. 2019. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://CRAN.R-project.org/package=cowplot. ———. 2020. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges. Willmott, Cj, and K Matsuura. 2005. “Advantages of the Mean Absolute Error (MAE) over the Root Mean Square Error (RMSE) in Assessing Average Model Performance.” Climate Research 30: 79–82. https://doi.org/10.3354/cr030079. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/. ———. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown. Yarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction over Explanation in Psychology: Lessons from Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122. https://doi.org/10.1177/1745691617693393. Zhao, Qingyuan, and Trevor Hastie. 2019. “Causal Interpretations of Black-Box Models.” Journal of Business &amp; Economic Statistics, July, 1–10. https://doi.org/10.1080/07350015.2019.1624293. Zhu, Hao. 2019. KableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra. "]
]

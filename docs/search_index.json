[
["index.html", "bmbstats: bootstrap magnitude-based statistics for sports scientists Welcome R and R packages License", " bmbstats: bootstrap magnitude-based statistics for sports scientists Mladen Jovanovic 2020-06-24 Welcome The aim of this book is to provide an overview of the three classes of tasks in the statistical modeling: description, prediction and causal inference (Hernán, Hsu, and Healy 2019). Statistical inference is often required for all three tasks. Short introduction to frequentist null-hypothesis testing, Bayesian estimation and bootstrap are provided. Special attention is given to the practical significance with the introduction of magnitude-based estimators and statistical inference by using the concept of smallest effect size of interest (SESOI). Measurement error is discussed with the particular aim of interpreting individual change scores. In the second part of this book, common sport science problems are introduced and analyzed with the bmbstats package. This book, as well as the bmbstats package are currently in development phase. Please be free to contribute pull request at GitHub bmbstats package https://github.com/mladenjovanovic/bmbstats bmbstats book https://github.com/mladenjovanovic/bmbstats-book R and R packages This book is fully reproducible and was written in R (Version 4.0.0; R Core Team 2020) and the R-packages automatic (Lang et al. 2014), bayestestR (Version 0.6.0; Makowski, Ben-Shachar, and Lüdecke 2019), bmbstats (Version 0.0.0.9000; Jovanović 2020), bookdown (Version 0.18; Xie 2016), boot (Version 1.3.25; A. C. Davison and Hinkley 1997a), carData (Version 3.0.3; Fox, Weisberg, and Price 2019), caret (Version 6.0.86; Kuhn 2020), cowplot (Version 1.0.0; Wilke 2019), directlabels (Version 2020.1.31; Hocking 2020), dplyr (Version 1.0.0; Wickham, François, et al. 2020), effects (Version 4.1.4; Fox and Weisberg 2018; Fox 2003; Fox and Hong 2009), forcats (Version 0.5.0; Wickham 2020), ggplot2 (Version 3.3.2; Wickham 2016), ggridges (Version 0.5.2; Wilke 2020), ggstance (Version 0.3.4; Henry, Wickham, and Chang 2020), hardhat (Version 0.1.3; Vaughan and Kuhn 2020), kableExtra (Version 1.1.0; Zhu 2019), knitr (Version 1.28; Xie 2015), lattice (Version 0.20.41; Sarkar 2008), markdown (Version 1.1; Allaire et al. 2019), Metrics (Version 0.1.4; Hamner and Frasco 2018), minerva (Version 1.5.8; Albanese et al. 2012a), mlr (Version 2.17.1; Bischl et al. 2016, 2017; Lang et al. 2019), mlr3 (Version 0.3.0; Lang et al. 2019), mlrmbo (Bischl et al. 2017), multilabel (Probst et al. 2017), openml (Casalicchio et al. 2017), ParamHelpers (Version 1.14; Bischl et al. 2020), pdp (Version 0.7.0; B. M. Greenwell 2017a), psych (Version 1.9.12.31; Revelle 2019), purrr (Version 0.3.4; Henry and Wickham 2020), readr (Version 1.3.1; Wickham, Hester, and Francois 2018), rpart (Version 4.1.15; Therneau and Atkinson 2019), stringr (Version 1.4.0; Wickham 2019), tibble (Version 3.0.1; Müller and Wickham 2020), tidyr (Version 1.1.0; Wickham and Henry 2020), tidyverse (Version 1.3.0; Wickham, Averick, et al. 2019), vip (Version 0.2.2; Greenwell, Boehmke, and Gray 2020), and visreg (Version 2.6.1; Breheny and Burchett 2017). License This work, as a whole, is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The code contained in this book is simultaneously available under the MIT license; this means that you are free to use it in your own packages, as long as you cite the source. References "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The real world is very complex and uncertain. In order to help in understanding it and to predict its behavior, we create maps and models (Page 2018; Weinberg and McCann 2019). One such tool are statistical models, representing a simplification of the complex and ultimately uncertain reality, in the hope of describing it, understanding it, predicting its behavior, and help in making decisions and interventions (Hernán, Hsu, and Healy 2019; Lang, Sweet, and Grandfield 2017; McElreath 2015; Pearl and Mackenzie 2018). In the outstanding statistics book “Statistical Rethinking” (McElreath 2015, 19), the author stresses the distinction between Large World and Small World, described initially by Leonard Savage (Binmore 2011; Gigerenzer, Hertwig, and Pachur 2015; Savage 1972): \"All statistical modeling has these same two frames: the small world of the model itself and the large world we hope to deploy the model in. Navigating between these two worlds remains a central challenge of statistical modeling. The challenge is aggravated by forgetting the distinction. The small world is the self-contained logical world of the model. Within the small world, all possibilities are nominated. There are no pure surprises, like the existence of a huge continent between Europe and Asia. Within the small world of the model, it is important to be able to verify the model’s logic, making sure that it performs as expected under favorable assumptions. Bayesian models have some advantages in this regard, as they have reasonable claims to optimality: No alternative model could make better use of the information in the data and support better decisions, assuming the small world is an accurate description of the real world. The large world is the broader context in which one deploys a model. In the large world, there may be events that were not imagined in the small world. Moreover, the model is always an incomplete representation of the large world, and so will make mistakes, even if all kinds of events have been properly nominated. The logical consistency of a model in the small world is no guarantee that it will be optimal in the large world. But it is certainly a warm comfort.\" Creating “Small Worlds” relies heavily on making and accepting numerous assumptions, both known and unknown, as well as prior expert knowledge, which is ultimately incomplete and fallible. Because all statistical models require subjective choices (Gelman and Hennig 2017), there is no objective approach to make “Large World” inferences. It means that it must be us who make the inference, and claims about the “Large World” will always be uncertain. Additionally, we should treat statistical models and statistical results as being much more incomplete and uncertain than the current norm (Amrhein, Trafimow, and Greenland 2019). We must accept the pluralism of statistical models and models in general (Mitchell 2012, 2002), move beyond subjective-objective dichotomy by replacing it with virtues such as transparency, consensus, impartiality, correspondence to observable reality, awareness of multiple perspectives, awareness of context-dependence, and investigation of stability (Gelman and Hennig 2017). Finally, we need to accept that we must act based on cumulative knowledge rather than solely rely on single studies or even single lines of research (Amrhein, Trafimow, and Greenland 2019). This discussion is the topic of epistemology, scientific inference, and philosophy of science, thus far beyond the scope of the present book (and the author). Nonetheless, it was essential to convey that statistical modeling is a process of creating the “Small Worlds” and deploying it in the “Large World”. There are three main classes of tasks that the statistical model is hoping to achieve: description, prediction, and causal inference (Hernán, Hsu, and Healy 2019). The following example will help in differentiating between these three classes of tasks. Consider a king who is facing a drought who must decide whether to invest resources in rain dances. The queen, upon seeing some rain clouds in the sky, must decide on whether to carry her umbrella or not. Young prince, who likes to gamble during his hunting sessions, is interested in knowing what region of his father’s vast Kingdom receives the most rain. All three would benefit from an empirical study of rain, but they have different requirements of the statistical model. The king requires causality: Do rain dances cause rain? The queen requires prediction: Does it look likely enough to rain for me to ask my servants to get my umbrella? The prince requires simple quantitative summary description: have I put my bets on the correct region? The following sections will provide an overview of the three classes of tasks in the statistical modeling. Data can be classified as being on one of four scales: nominal, ordinal, interval or ratio and description, prediction and causal techniques differ depending on the scales utilized. For the sake of simplicity and big picture overview, only examples using ratio scale are to be considered in this book. References "],
["description.html", "Chapter 2 Description 2.1 Comparing two independent groups 2.2 Comparing dependent groups 2.3 Describing relationship between two variables 2.4 Advanced uses", " Chapter 2 Description Description provides quantitative summary of the acquired data sample. These quantitative summaries are termed descriptive statistics or descriptive estimators and are usually broken down into two main categories: measures of central tendency, and measures of spread / dispersion. The stance taken in this book is that descriptive statistics involve all quantitative summaries (or aggregates) that are used to describe data without making predictive or causal claims. For example, linear regression between two variables can be used as a descriptive tool if the aim is to measure linear association between two variables, but it can be also used in predictive and causal tasks. Effect sizes such as change, percent change or Cohen's d represent descriptive statistics used to compare two or more groups, and are commonly used in causal tasks to estimate average causal effect of the treatment. To provide further explanation of the descriptive statistics, three common descriptive tasks in sport science are given as examples: (1) comparing two independent groups, (2) comparing two dependent groups, (3) measuring association between two variables. 2.1 Comparing two independent groups Imagine we carried collection of body height measurements and we obtained N=100 observations using N=50 female and N=50 male subjects. Collected data is visualized in Figure 2.1. Figure 2.1: Common techniques to visualize independent groups observations. Before any analysis takes place, it is always a good practice to visualize the data first. Ideally, we want to visualize the complete data set, rather than only provide descriptive summaries, such as means. A. Simple scatter-plot with jitter to avoid overlap between the points. B. Mean and standard deviation as error bars. C. Box-plot. Horizontal line represents median, or 50th percentile, whereas boxes represent 25th and 75th percentile. Vertical lines usually represent min and max, although they can extend up to 1.5xIQR (inter-quartile range) with point outside of that interval plotted as outliers. D. Violin plots representing double-side density plots with 25th, 50th and 75th percentile lines. E. Density plots indicating sample distribution. F. Raincloud plot (Allen et al. 2019, 2018) which combine kernel density plots as clouds with accompanying 25th, 50th and 75th percentile lines, mean±SD error bars and jittered points as rain Commonly provided descriptive statistics for each group can be found in the Table 2.1. Mean, median and mode are common measures of central tendencies. Standard deviation (SD), median absolute difference (MAD), inter-quartile range (IQR), min, max and range are common measures of spread or dispersion. Percent coefficient of variation (% CV) is also a measure of dispersion, but standardized1 which allows comparison of variables that are on different scales. Skewness (skew) is usually described as a measure of a symmetry. A perfectly symmetrical data set will have a skewness of 0. Kurtosis measures the tail-heaviness of the distribution. More in depth discussion of descriptive estimators, particularly robust estimators (Rousselet, Pernet, and Wilcox 2017; Wilcox, Peterson, and McNitt-Gray 2018; Wilcox and Rousselet 2017; Wilcox 2016) is beyond the topic of this short overview. Table 2.1: Common descriptive statistics or estimators Estimator Male Female n 50.00 50.00 mean (cm) 175.90 163.18 SD (cm) 9.32 8.20 % CV 5.30 5.02 median (cm) 176.30 164.00 MAD (cm) 9.52 8.86 IQR (cm) 11.24 11.67 mode (cm) 176.26 164.94 min (cm) 154.24 145.59 max (cm) 193.90 181.12 range (cm) 39.66 35.53 skew 0.08 0.08 kurtosis -0.53 -0.69 2.1.1 Sample mean as the simplest statistical model In the Introduction of this book, statistical models are defined as “Small Worlds” or simplifications of the complex and uncertain reality. From this perspective, sample mean can be considered the simplest statistical model. With this estimator we are representing all of the data points with one quantitative summary (i.e. aggregate). However, how do we choose an estimate that represents the sample the best? Estimate that has the minimal error is selected as the optimal representative. Error is defined using a loss function that penalizes difference between the model estimate or prediction (\\(\\hat{y_i}\\)) and observations (\\(y_i\\)) (Equation (2.1)). The difference between model prediction (\\(\\hat{y_i}\\)) and observations (\\(y_i\\)) is called residual. \\[ \\begin{equation} Loss \\: function = f(observed, predicted) \\tag{2.1} \\end{equation} \\] Two most common loss functions are absolute loss (also referred to as \\(L1\\)) (Equation (2.2)) and quadratic loss (also referred to as squared errors or \\(L2\\)) (Equation (2.3)). Please refer to section Sample mean as the simplest predictive model in Prediction chapter for more examples. \\[ \\begin{equation} absolute \\: loss = \\mid{\\hat{y_i} - y_i\\mid} \\tag{2.2} \\end{equation} \\] \\[ \\begin{equation} quadratic \\: loss = (\\hat{y_i} - y_i)^2 \\tag{2.3} \\end{equation} \\] Cost function is an aggregate of the loss function (Equation (2.4)). \\[ \\begin{equation} Cost \\: function = f(Loss \\: function (observed, predicted)) \\tag{2.4} \\end{equation} \\] Since loss function is defined on a data point (i.e. \\(y_i\\)), we need to aggregate losses into a single metric. This is done with a cost function, usually using sum or mean. One such cost function is root-mean-square-error (RMSE) (Equation (2.5)). RMSE takes the square root of the mean of the quadratic loss (note the \\((\\hat{y_i} - y_i)^2\\) in the RMSE equation, which represent quadratic loss). RMSE thus represents a measure of the model fit, or how good the model fits the data. Lower RMSE means lower error and thus a better fit. \\[ \\begin{equation} RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}(\\hat{y_i} - y_i)^2} \\tag{2.5} \\end{equation} \\] By using body height data from the female group, we can search for a body height estimate that minimizes the RMSE (Figure 2.2). That body height estimate would be considered the best representative of the sample, and thus the simplest statistical model. Figure 2.2: Sample mean as the simplest statistical model. A. Dashed line represents the estimate, in this case the mean of the sample. Vertical line represent residuals between estimate and observed values. B. Each estimate has a RMSE value. Central tendency estimate with the lowest RMSE value is the sample mean. C. Similar to panel A, this panel depicts residuals for a central tendency estimate with higher RMSE As the result of this search, the body height estimate that minimizes the error is 163.18cm, and accompanying RMSE is equal to 8.12cm. As it can be read from the Table 2.1, this optimal body height estimate is equal to calculated sample mean. Standard deviation of the sample is equal to RMSE2. From statistical modeling perspective, sample mean can be considered sample estimate that minimizes the sample SD, and sample SD can be seen as the measure of the model fit. This search for the optimal estimate that minimizes the cost function can be expanded to other statistical models. For example, linear regression can be seen as a search for the line that minimizes RMSE. This approach of estimating model parameters or estimators belongs to the family of ordinary least squares (OLS) methods, although there are other approaches such as maximum likelihood estimation (MLE) which will be discussed in Statistical inference section (Foreman 2014). The solutions to some of these models can be found analytically3, but for some there is no analytic solution and computational approaches must be utilized. These computation approaches are referred to as optimization algorithms. The example given here involves only one parameter that needs to be optimized, in this case body height estimate, but real-life problems involve numerous parameters. The simple search through parameters state-space would take forever when it comes to problems involving more than only a few parameters. Algorithms that solve this computational problems are numerous, out of which the most popular ones are gradient descent, and Markov Chain Monte-Carlo (MCMC), which is utilized in Bayesian inference (will be discussed in Bayesian perspective section). The take-home message from this short interlude is that even the simple descriptive statistics can be seen as statistical models. If we take another cost function, for example mean absolute error (MAE) (Equation (2.6)) and if we optimize so that the sample central tendency estimate minimizes MAE, we will get median estimator. \\[ \\begin{equation} MAE = \\frac{1}{n}\\Sigma_{i=1}^{n}\\mid{\\hat{y_i} - y_i\\mid} \\tag{2.6} \\end{equation} \\] We will expand this discussion about loss functions, cost functions, and performance metrics in Sample mean as the simplest predictive model section. For more information please check the package Metrics (Hamner and Frasco 2018) and the following references (Botchkarev 2019; Chai and Draxler 2014; Willmott and Matsuura 2005; Barron 2019). 2.1.2 Effect Sizes Besides describing groups, we are often interested in comparing them. In order to achieve this task, a collection of estimators termed effect size statistics are utilized. Effect size can be defined in a narrow sense or in a broad sense. Briefly, the narrow sense refers to a family of standardized measures such as Cohen’s d, while the broad sense refers to any measure of interest, standardized or not. The approach to effect size statistics in this book is thus in a broad sense of the definition, in which all group comparison estimators are considered effect sizes statistics. In order to estimate effect sizes, one group needs to be considered baseline or control. The most common effect size statistics can be found in the Table 2.2 where female body height is considered baseline and compared with male body height. Table 2.2: Effect size statistics for estimating differences between two independent groups Difference (cm) SDdiff (cm) % CVdiff % Difference Ratio Cohen’s d CLES OVL 12.73 12.41 97.55 7.8 1.08 1.45 0.85 0.47 Difference, or mean difference (mean diff) is calculated by subtracting group means. Using body height as an example, the mean diff between males and females is calculated by using the following equation (2.7): \\[ \\begin{equation} \\begin{split} mean_{difference} &amp;= mean_{males} - mean_{females} \\\\ mean_{males} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}male_i \\\\ mean_{females} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}female_i \\end{split} \\tag{2.7} \\end{equation} \\] % CVdiff, or percent coefficient of variation of the difference is the standard deviation of the difference (SDdiff - explained shortly) divided by mean diff (Equation (2.8)): \\[ \\begin{equation} \\%\\;CV_{difference} = 100\\times\\frac{SD_{difference}}{mean_{difference}} \\tag{2.8} \\end{equation} \\] % Difference, or mean percent difference is calculated by dividing mean diff with the mean of the baseline group, in this case the female group, multiplied by 100 (Equation (2.9)): \\[ \\begin{equation} mean_{\\% difference} = 100\\times\\frac{mean_{difference}}{mean_{females}} \\tag{2.9} \\end{equation} \\] Mean ratio, as its name suggests, is simple ratio between the two means (Equation (2.10)): \\[ \\begin{equation} mean_{ratio} = \\frac{mean_{males}}{mean_{females}} \\tag{2.10} \\end{equation} \\] Cohen's d represent standardized effects size and thus preferable effect size statistic. For this reason, Cohen's d is commonly written as ES, short of effect size. Cohen's d for the independent groups is calculated by dividing mean diff (Equation (2.7)) with pooled standard deviation ((2.11)). \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{difference}}{SD_{pooled}} \\tag{2.11} \\end{equation} \\] Pooled standard deviation represents combined standard deviations from two groups (Equation (2.12)). \\[ \\begin{equation} SD_{pooled} = \\sqrt{\\frac{(n_{males} - 1) SD_{males}^2 + (n_{females} - 1) SD_{females}^2}{n_{males}+n_{females} - 2}} \\tag{2.12} \\end{equation} \\] Why Cohen's d should be used instead of other effect size estimators can be demonstrated by a simple example, coming from a study by Buchheit and Rabbani (2014). In this study, authors examined the relationship between the performance in the YoYo Intermittent Recovery Test Level 1 (YoYoIR1) and the 30-15 Intermittent Fitness Test (30-15IFT), and compared the sensitivity of both tests to the training. Although this study used two dependent groups (Pre-training and Post-training), the rationale can be applied to the topic of estimating effect sizes between the two independent groups. Table 2.3 contains Pre-training results and the effect sizes estimated with percent change4 and Cohen's d. Table 2.3: Training intervention effect sizes for YoYoIR1 and 30-15IFT. Modified from Buchheit and Rabbani (2014) Test Pre-training % Change Cohen’s d YoYoIR1 1031 ± 257 m 35 % 1.2 30-15IFT 17.4 ± 1.1 kmh-1 7 % 1.1 Since YoYoIR1 and 30-15IFT utilize different scales (total meters covered and velocity reached respectively), percent change estimator is not a good choice to compare the effect sizes between the two tests5. Since Cohen's d is standardized estimator, it should be used when comparing tests or measures that are at different scales. After estimating effect sizes, the question that naturally follows up is the question of magnitude. In other words - “how big is the effect?”. Since Cohen's d is standardized estimator, it allows for establishment of qualitative magnitude thresholds. Based on the original work by Cohen (Cohen 1988), Hopkins (Hopkins 2006; Hopkins et al. 2009) suggested the following magnitudes of effect (Table (2.4). According to the Table (2.4, the body height difference between males and females would be considered large, as well as changes in both YoYoIR1 and 30-15IFT. Table 2.4: Magnitudes of effect Magnitude of effect Trivial Small Moderate Large Very Large Nearly Perfect Cohen’s d 0 - 0.2 0.2 - 0.6 0.6 - 1.2 1.2 - 2.0 2.0 - 4.0 &gt; 4.0 Cohen's d, as well as associated magnitudes of effect, are commonly hard to interpret by non-statistically trained professionals (e.g. coaches). McGraw and Wong (1992) suggested common language effect size (CLES) estimator instead, which could be more intuitive to understand. CLES represents the probability that an observation sampled at random from one group will be greater than an observation sampled at random from other group. For example, if we take random male and random female from our two groups and repeat that 100 times6, how many times a male would be taller than a female (Figure 2.3)? Figure 2.3: Drawing random 100 pairs to estimate probability of males being taller than females. A. Scatterplot of 100 pairs drawn at random from two samples. Since we are comparing paired males and females, lines can be drawn between each of 100 draws. Blue line indicates taller male, while orange line indicates taller female. B. Distribution of the difference between males and females for each of 100 pairs drawn By using simple counting from 100 random paired samples, males are taller in 85 cases, or 85%. By using probability, that is equal to 0.85. In other words, if I blindfoldedly, randomly select a male and a female from the two groups and if I bet that the male is taller, I would be correct 85% of the time. CLES can be estimated using brute-force computational method, or algebraic method. Brute-force method involves generating all possible pair-wise combinations from two groups, and in our example that is equal to \\(50 \\times 50 = 2500\\) cases, and then simply counting in how many cases males are taller than females. This method can become very computationally intensive for groups with large sample number. Algebraic method, on the other hand, assumes normal distribution of the observations in the groups, and estimates standard deviation of the difference (SDdiff) (Equation (2.13)). Note that standard deviation of the all pairwise differences estimated with brute-force method would be very similar to algebraically derived SDdiff. \\[ \\begin{equation} SD_{difference} = \\sqrt{SD_{males}^{2} + SD_{females}^{2}} \\tag{2.13} \\end{equation} \\] Algebraically, CLES is then derived assuming normal distribution (where mean of the distribution is equal to mean diff between the groups, and standard deviation of the distribution is equal to SDdiff) by calculating probability of the difference scores higher than zero (see Figure 2.3B for a visual representation). Table 2.2 contains algebraically computed CLES estimate. CLES equivalent is utilized as a performance metric in class prediction tasks, termed area under curve (AUC), where 0.5 is a predictive performance equal to a random guess, and 1 is perfect predictive separation between the two classes (James et al. 2017; Kuhn and Johnson 2018). Overlap (OVL) estimator represents the overlap between the two sample distributions. Providing that samples are identical, the OVL is equal to 1. Providing there is complete separation between the two samples, then OVL is equal to 0 (Figure 2.4A). OVL can be estimated with brute-force computational methods (which doesn’t make assumptions regarding sample distribution) and with algebraic methods that make normality assumptions. Since Cohen's d, CLES and OVL are mathematically related, it is possible to convert one to another (assuming normal distribution of the samples and equal SD between the two groups for the OVL estimation). Figure 2.4B depicts relationship between the Cohen's d, CLES, and OVL. Figure 2.4C depicts relationship between the CLES and OVL. Figure 2.4: Relationship between the Cohen's d, CLES, and OVL. A. Visual display of the samples of varying degrees of separations, and calculated Cohen's d, CLES, and OVL. B. Relationship between the CLES and OVL to the Cohen's d. C. Relationship between the CLES and OVL Table 2.5 contains Cohen's d magnitudes of effect with accompanying estimated CLES and OVL thresholds. Table 2.5: Magnitudes of effect for CLES and OVL estimated using Cohen's d Magnitude of effect Trivial Small Moderate Large Very Large Nearly Perfect Cohen’s d 0.0 - 0.2 0.2 - 0.6 0.6 - 1.2 1.2 - 2.0 2.0 - 4.0 &gt; 4.0 CLES 0.50 - 0.56 0.56 - 0.66 0.66 - 0.80 0.80 - 0.92 0.92 - 1.00 1.00 OVL 1.00 - 0.92 0.92 - 0.76 0.76 - 0.55 0.55 - 0.32 0.32 - 0.05 0.00 2.1.3 The Smallest Effect Size Of Interest According to Cohen (1988), the qualitative magnitude thresholds from Table 2.5 are “arbitrary conventions, recommended for use only when no better basis for estimating the effect size is available” (p. 12). But what if practitioners a priori know what is the minimal important effect size and are interested in judging the practical or clinical significance (Sainani 2012) of the results (in this case difference between the groups)? In other words, the smallest effect size of interest (SESOI)7. There is no single way to approach definition and estimation of SESOI, but it usually tends to be based on either the known measurement error (ME) (e.g. the minimum detectable effect size), or the effect size that is large enough to be practically meaningful (e.g. the minimal important difference, or the smallest worthwhile change) (Anvari and Lakens 2019; Will G Hopkins 2004a; Hopkins 2015; King 2011; Lakens, Scheel, and Isager 2018; Turner et al. 2015; Swinton et al. 2018; Caldwell and Cheuvront 2019). In this book, statistical models and estimators that utilize SESOI are referred to as magnitude-based. To introduce magnitude-based estimators, consider ±2.5cm to be body height SESOI8, or the difference that would be practically significant. In other words, individuals with height difference within ±2.5cm would be considered practically equivalent (from the minimal important effect perspective), or it might be hard to detect this difference with a quick glance (from minimum detectable effect perspective). The simplest magnitude-based statistics would be mean diff divided by SESOI (Difference to SESOI) (Equation (2.14)). This estimator, similar to other standardized estimators (e.g. Cohen's d) allows comparison of variables at different scales, but it would also give more insight into differences from practical significance perspective. \\[ \\begin{equation} Difference\\;to\\;SESOI = \\frac{mean_{difference}}{SESOI_{upper} - SESOI_{lower}} \\tag{2.14} \\end{equation} \\] Second magnitude-based statistic is SDdiff divided by SESOI (SDdiff to SESOI) (Equation (2.15)). This estimator, similar to % CVdiff, would answer how variable are the differences compared to SESOI. \\[ \\begin{equation} SDdiff\\;to\\;SESOI = \\frac{SD_{difference}}{SESOI_{upper} - SESOI_{lower}} \\tag{2.15} \\end{equation} \\] Similarly, CLES estimator can become magnitude-based by utilizing SESOI. Rather than being interested in probability of a random male being taller than a random female (out of the two sample groups), we might be interested in estimating how probable are lower, equivalent, and higher (or usually defined as harmful, trivial, and beneficial) differences defined by SESOI. Practically equivalent (trivial) differences are differences ranging from \\(SESOI_{lower}\\) to \\(SESOI_{upper}\\), while everything over \\(SESOI_{upper}\\) is higher (or beneficial) difference and everything lower than \\(SESOI_{lower}\\) is lower (or harmful) difference. Using brute-force computational method and drawing all pair-wise combinations from the two groups (50x50 = 2500 cases), and using ±2.5cm SESOI as a practically equivalent difference9, we can estimate probabilities of lower (pLower), equivalent (pEquivalent) and higher difference (pHigher) by calculating proportion of cases within each magnitude band (Figure 2.5). Figure 2.5: Pairwise comparison of males and females to estimate probability of lower, equivalent, and higher magnitude of difference. A. Scatterplot of all pair-wise combinations (50x50 = 2500), drawn at random out of two samples. Since we are comparing paired males and females, lines can be drawn between each of 2500 draws. Blue line indicates males taller than females higher than SESOI, equivalent lines indicates pairs with a height difference less or equal to SESOI, while orange line indicates females taller than males higher than SESOI. B. Distribution of the differences between males and females for all 2500 pair-wise combinations. Grey band indicates SESOI. Surface of the distribution over SESOI (blue color) indicates probability of randomly selected male being taller than a randomly selected female (pHigher), with a height difference of at least SESOI magnitude. Surface of the distribution under SESOI (orange color) indicates probability of randomly selected female being taller than a randomly selected female (pLower), with a height difference of at least SESOI magnitude. Grey surface area indicates probability of randomly selecting male and female with a height difference within SESOI band (pEquivalent) Table 2.6 contains estimated probabilities of observing lower, equivalent, and higher differences in height between the randomly selected male and female using brute-force computational method and algebraic method. These estimates answer the following question “If I compare random male and random female from my sample, how probable are lower/equivalent/higher magnitudes of difference in height?”. Asking such a magnitude-based question regarding the random individual difference represents a form of prediction question and predictive task. In this book, such questions are answered with magnitude-based prediction approaches. Table 2.6: Estimated probabilities of observing lower, equivalent, and higher differences in height Method pLower pEquivalent pHigher brute-force 0.110 0.096 0.794 algebraic 0.111 0.095 0.794 It is common to represent means as systematic component or fixed effect (e.g. mean difference), and variability around the mean (i.e. SDdiff) as stochastic component or random effect. It is unfortunate that the common statistical modeling and analysis, particularly in sport science, takes the stance of approaching and treating between-individual variation as random error. The approach suggested in this book complements group-based or average-based statistics with magnitude-based predictions that aim to help in answering individual-based questions, common to sport practitioners. Table 2.7 contains discussed magnitude-based estimators that can complement common effect size statistics (Table 2.2) when comparing two independent groups. Table 2.7: Magnitude-based effect size statistics for estimating difference between two independent groups SESOI lower (cm) SESOI upper (cm) Difference to SESOI SDdiff to SESOI pLower pEquivalent pHigher -2.5 2.5 2.55 2.48 0.11 0.09 0.79 2.2 Comparing dependent groups As an example of dependent or paired groups descriptive analysis, let’s consider the simple Pre-test and Post-test design. We have given training intervention to a group of N=20 males involving bench-press training. Training intervention involved performing bench pressing two times a week for 16 weeks. One-repetition-maximum (1RM) in the bench press was performed before (Pre-test) and after (Post-test) training intervention. Table 2.8 contains individual Pre-test and Post-test scores, as well as the Change in the bench press 1RM. Table 2.8: Individual Pre and Post scores, as well as Change in the bench press 1RM Athlete Pre-test (kg) Post-test (kg) Change (kg) Athlete 01 111.80 121.42 9.62 Athlete 02 95.95 102.13 6.18 Athlete 03 105.87 125.56 19.69 Athlete 04 98.79 109.67 10.87 Athlete 05 95.81 108.11 12.30 Athlete 06 95.27 92.67 -2.60 Athlete 07 97.75 106.03 8.28 Athlete 08 106.50 109.51 3.01 Athlete 09 80.62 95.96 15.34 Athlete 10 100.40 94.30 -6.11 Athlete 11 82.71 78.91 -3.80 Athlete 12 102.89 93.98 -8.91 Athlete 13 91.34 105.21 13.87 Athlete 14 111.14 108.07 -3.07 Athlete 15 95.13 96.01 0.88 Athlete 16 109.12 112.12 3.00 Athlete 17 91.87 103.41 11.54 Athlete 18 92.16 103.93 11.77 Athlete 19 108.88 119.72 10.84 Athlete 20 97.94 95.91 -2.03 The results of this simple Pre-test and Post-test design can be described in multiple ways. Here, I will present the three most common approaches. 2.2.1 Describing groups as independent The simplest analysis involve descriptive statistics assuming groups as independent. Table 2.9 contains descriptive statistics applied to Pre-test, Post-test and Change scores as independent. Figure 2.6 visualizes the scores using three raincloud plots. Table 2.9: Descriptive analysis of the Pre-test, Post-test, and Change as independent samples Estimator Pre-test Post-test Change n 20.00 20.00 20.00 mean (kg) 98.60 104.13 5.53 SD (kg) 8.70 11.08 8.05 % CV 8.83 10.64 145.46 median (kg) 97.84 104.57 7.23 MAD (kg) 8.64 11.94 8.46 IQR (kg) 11.64 13.60 13.77 mode (kg) 96.49 105.76 10.78 min (kg) 80.62 78.91 -8.91 max (kg) 111.80 125.56 19.69 range (kg) 31.18 46.64 28.60 skew -0.26 -0.05 -0.16 kurtosis -0.73 -0.28 -1.28 Figure 2.6: Raincloud plots of the Pre-test, Post-test and Change scores in the bench press 1RM. A. Distribution of the Pre-test and Post-test scores. B. Distribution of the Change score 2.2.2 Effect Sizes Table 2.10 contains the most common effect size estimators utilized when describing change in the Pre-Post paired design. The terminology utilized in this book differentiates between the difference which is used in independent groups and the change which is used in paired or dependent groups Table 2.10: Effect size statistics for estimating change in two dependent groups Change (kg) SDchange (kg) % CVchange % Change Ratio Cohen’s d CLES OVL 5.53 8.05 145.46 5.75 1.06 0.64 0.65 0.75 Change, or mean change is calculated by taking average of the change score (Equation (2.16)). Change score is simple difference between Pre-test and Post-test. \\[ \\begin{equation} \\begin{split} mean_{change} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(post_{i}-pre_{i}) \\\\ mean_{change} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}change_{i} \\\\ change_{i} &amp;= post_{i}-pre_{i} \\end{split} \\tag{2.16} \\end{equation} \\] SDchange, or standard deviation of the change is a simple standard deviation of the change (Equation (2.17)). It represents a measure of dispersion of the change scores. \\[ \\begin{equation} SD_{change} = \\sqrt{\\frac{1}{n-1}\\Sigma_{i=1}^{n}(change_i -mean_{change})^2} \\tag{2.17} \\end{equation} \\] % CVchange, or percent coefficient of variation of the change is the SDchange divided by mean change (Equation (2.18)). \\[ \\begin{equation} \\%\\;CV_{change} = 100\\times\\frac{SD_{change}}{mean_{change}} \\tag{2.18} \\end{equation} \\] % Change, or Mean percent change is calculated by taking a mean of the ratio between the change and the Pre-test, multiplied by 100 (Equation (2.19)). \\[ \\begin{equation} mean_{\\% change} = 100\\times\\frac{1}{n}\\Sigma_{i}^{n}\\frac{change_{i}}{pre_{i}} \\tag{2.19} \\end{equation} \\] Mean ratio represents mean of the Post-test to Pre-test scores ratios (Equation (2.20)). \\[ \\begin{equation} mean_{ratio} = \\frac{1}{n}\\Sigma_{i}^{n}\\frac{post_{i}}{pre_{i}} \\tag{2.20} \\end{equation} \\] Cohen's d represents standardized effect size of the change. In the paired design, Cohen's d is calculated by dividing mean change with standard deviation of the Pre-test scores (SDpre) (Equation (2.21)). \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{change}}{SD_{pre}} \\tag{2.21} \\end{equation} \\] CLES for the paired groups represents probability of observing positive change. OVL, equally to the independent groups, represents overlap between the Pre-test and Post-test scores. Magnitude-based effect size estimators involve the use of SESOI and can be found on Table 2.11. Similarly to magnitude-based effect size estimators with the independent groups, magnitude-based effect size estimators with the paired group involve Change to SESOI, SDchange to SESOI as well as proportions of lower (pLower), equivalent (pEquivalent) and higher (pHigher) change scores. Table 2.11: Magnitude-based effect size statistics for estimating change between two dependent groups SESOI (kg) Change to SESOI SDchange to SESOI pLower pEquivalent pHigher ±5 0.55 0.81 0.1 0.37 0.53 Figure 2.7 depicts visually how proportions of lower, equivalent, and higher change scores are estimated. Same as with two independent groups, these proportions can be estimated using the brute-force method (i.e. simple counting of the change scores withing lower, trivial, and higher zones), or algebraic where SDchange is utilized and assumption of the normally distributed change scores is made. Figure 2.7: Visual analysis of the dependent groups scores using SESOI. A. Scatter plot of Pre-test and Post-test scores. Green line indicates change higher than SESOI upper, grey line indicates change within SESOI band, and red line indicates negative change lower than SESOI lower. B. Distribution of the change scores. Green area represents proportion of change scores higher than SESOI upper, red area represents proportion of negative change scores lower than SESOI lower, and grey area indicates equivalent change, which is within SESOI band It might be tempting to claim that this intervention is causing changes in the bench press 1RM, but we should be vary of doing that. It is important to keep in mind that the effect size estimators are used only descriptively without any causal connotation. To make causal claims, further criteria needs to be taken into account. This is discussed in more details in the Causal inference section of this book. 2.3 Describing relationship between two variables So far, we have dealt with single variable descriptive statistics. However, we are often interested in relationship or association between two variables. One of these variables takes the role of the dependent variable (outcome or target variable) and the other of the independent variable (or predictor variable). Let’s assume we tested N=30 female soccer athletes by using two tests: (1) YoYoIR1 test (expressed in meters), and (2) maximum aerobic speed (MAS) test (expressed in km/h)10. Variables in this example represent observations in each test (Table 2.12). Table 2.12: Results of YoYoIR1 and MAS tests for N=30 female soccer athletes Athlete YoYoIR1 (m) MAS (km/h) Athlete 01 1640 15.5 Athlete 02 1080 15.0 Athlete 03 1440 15.0 Athlete 04 1200 15.0 Athlete 05 960 14.5 Athlete 06 1120 15.0 Athlete 07 1000 14.5 Athlete 08 1440 15.0 Athlete 09 640 14.0 Athlete 10 1360 15.0 Athlete 11 760 14.5 Athlete 12 1240 15.0 Athlete 13 1000 15.0 Athlete 14 1600 15.5 Athlete 15 1160 15.0 Athlete 16 1520 15.0 Athlete 17 1000 14.5 Athlete 18 1000 14.5 Athlete 19 1480 15.5 Athlete 20 1280 15.0 Athlete 21 1200 14.5 Athlete 22 1200 14.5 Athlete 23 1200 15.0 Athlete 24 1120 14.5 Athlete 25 1560 15.5 Athlete 26 1120 14.5 Athlete 27 1640 15.5 Athlete 28 1280 15.0 Athlete 29 1040 14.5 Athlete 30 880 14.0 Descriptive statistics for YoYoIR1 and MAS test results can be found in the Table 2.13. Table 2.13: Descriptive statistics for YoYoIR1 and MAS test results Estimator YoYoIR1 MAS n 30.00 30.00 mean 1205.33 14.85 SD 255.96 0.42 % CV 21.24 2.82 median 1200.00 15.00 MAD 296.52 0.74 IQR 410.00 0.50 mode 1131.68 15.00 min 640.00 14.00 max 1640.00 15.50 range 1000.00 1.50 skew -0.02 -0.11 kurtosis -0.68 -0.72 Visual analysis in Figure 2.8 depicts the association between these two tests using scatter plot. Figure 2.8: Scatter plot between two variables. Dashed line represents linear regression line Table 2.14 contains common estimators of the association between two variables. All estimators except maximum information coefficient (MIC) (Albanese et al. 2012b; Reshef et al. 2011) assumes linear relationship between two variables. It is thus important to visually analyze the association (see Figure 2.8) before trusting numerical estimators. Table 2.14: Common estimators of the association between two variables Pearson r R-squared MIC 0.86 0.74 0.55 The Pearson product-moment correlation coefficient (Pearson's r) is a measure of the strength of the linear relationship between two variables (Equation (2.22)). \\[ \\begin{equation} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} \\tag{2.22} \\end{equation} \\] Pearson's r is standardized measure that can take values ranging from -1 to +1, where 0 indicates no relationship, and -1 and +1 indicates perfect relationship. Negative Pearson's r value represents negative association (i.e. as one variable increases the other decreases), while positive Pearson's r value represents positive association (i.e., as one variable increases so does the other). R-squared (\\(R^2\\)) represents variance explained, i.e. how much the model explains variance in the target variable. In this example the model is linear regression. R-squared is standardized measure of association that can take values ranging from zero (no association, or no variance explained) to 1 (perfect association, or all variance explained). R-squared, as its name suggests, represents Pearson’s r squared, but for more complex models it can be calculated using variances or mean squares (MS) (Equation (2.22)): \\[ \\begin{equation} \\begin{split} R^2 &amp;= \\frac{MS_{model}}{MS_{total}} \\\\ MS_{model} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(\\hat y_i - \\overline y)^2 \\\\ MS_{total} &amp;= \\frac{1}{n}\\Sigma_{i=1}^{n}(y_i - \\overline y)^2 \\end{split} \\tag{2.23} \\end{equation} \\] Maximal information coefficient (MIC) is a novel measure of the strength of the linear or non-linear association between two variables and belongs to the maximal information-based non-parametric exploration (MINE) class of statistics (Albanese et al. 2012b; Reshef et al. 2011). MIC is standardized measure of association that can take values ranging from zero (no association) to 1 (perfect association). As opposed to Pearson r, MIC can pick up non-linear association between two variables. Statistical model, or machinery underlying Pearson r and R-squared is linear regression. Similar to a sample mean (see section Sample mean as the simplest statistical model), linear regression can be seen as optimization algorithm that tries to find a line that passes through the data with the minimal error.11 A solution to this problem can be found computationally or analytically12. Either way, the coefficients (or parameters) that need to be estimated in this example with two variables are intercept (\\(\\hat{\\beta}_0\\)), slope coefficient (\\(\\hat{\\beta}_1\\)), and residual error (\\(\\hat{\\epsilon}\\)) (Equation (2.24)). \\[ \\begin{equation} \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{\\epsilon} \\tag{2.24} \\end{equation} \\] Table 2.15 contains estimates for intercept, slope, and residual error. Residual error (\\(\\epsilon\\)) is estimated by using residual standard error (RSE), which is similar to already discussed RMSE, but rather than dividing sum of square errors by \\(n\\) observations, it is divided by \\(n-p\\) (Equation (2.25)). The \\(p\\) is the number of model parameters, in this case 2 (intercept and one slope coefficient). \\[ \\begin{equation} RSE = \\sqrt{\\frac{1}{n-p}\\Sigma_{i=1}^{n}(y_i -\\hat{y_i})^2} \\tag{2.25} \\end{equation} \\] Table 2.15: Linear regression estimates for intercept, slope coefficient, and RSE when MAS is the target variable an YoYoIR1 is the predictor Intercept (km/h) Slope RSE (km/h) 13.16 0.0014 0.22 Estimated parameters in the Table 2.15 can be written using the linear equation format (Equation (2.26)). \\[ \\begin{equation} MAS = 13.16 + 0.0014 \\times YoYoIR1 \\pm 0.22 \\: km/h \\tag{2.26} \\end{equation} \\] Slope coefficient of 0.0014 can be interpreted the following way: if YoYoIR1 increases by 500m, then MAS would increase by 500 x 0.0014 or 0.7km/h. Although measures of association between two variables, such as Pearson's r and R-squared, are symmetrical (meaning it doesn’t matter which variable is predictor or target), one cannot reverse the linear regression equation to get YoYoIR1 from MAS as done in the Equation (2.25). \\[ \\begin{equation} \\begin{split} MAS &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times YoYoIR1 \\\\ YoYoIR1 &amp;= \\frac{-\\hat{\\beta}_0 + MAS}{\\hat{\\beta}_1} \\\\ YoYoIR1 &amp;= -\\frac{\\hat{\\beta}_0}{\\hat{\\beta}_1} + \\frac{1}{\\hat{\\beta}_1}\\times MAS \\\\ YoYoIR1 &amp;= -9385.59 + 713.19 \\times MAS \\end{split} \\tag{2.27} \\end{equation} \\] It can be seen that the reverse parameters from (2.25) differ from the parameters in the Table 2.16 which are estimated using YoYoIR1 as the target variable an MAS as the predictor variable. Table 2.16: Linear regression estimates for intercept, slope coefficient, and RSE when YoYoIR1 is the target variable an MAS is the predictor Intercept (m) Slope RSE (m) -6589.82 524.93 133.84 This difference between reversed parameters and correctly estimated can be visually seen as non-identical linear regression lines in the Figure 2.9. Figure 2.9: Regression line differs depending which variable is target or the outcome variable. Dashed grey line represents regression line when MAS is the target variable. Grey line represents regression line when YoYoIR1 is the target variable. Since they are not identical, one cannot reverse the equation to predict YoYoIR1 from MAS score, when such equation is estimated by predicting MAS from YoYoIR1 Unfortunately, this is common practice in sport science. Rather than reversing parameters, one needs to fit, in this case, linear regression model again with the properly defined target and predictor variables. In certain scenarios, such as Reliability analysis, we do not know which variable represents predictor and which represents target or outcome. For this reason, different approaches to regression, such as ordinary least products (OLP) are utilized (Ludbrook 2010, 2012, 1997, 2002; Mullineaux, Barnes, and Batterham 1999). These topics will be covered in the second part of this book. 2.3.1 Magnitude-based estimators Similarly to independent and dependent group analysis, with association we might be interested in the practical significance of the results. In order to judge results from practical significance perspective, we need to define SESOI of both variables (i.e. YoYoIR1 and MAS). Using minimal test increment, SESOI for the YoYoIR1 test is defined as ±40m, and SESOI for the MAS test is defined as ±0.5km/h. One question we might ask is whether the YoYoIR1 SESOI is associated with MAS SESOI. This can be answered with the sensitivity estimator (Equation (2.28)). \\[ \\begin{equation} \\begin{split} Sensitivity &amp;= \\frac{(SESOI_{YoYoIR1_{upper}} - SESOI_{YoYoIR1_{lower}})\\times\\hat{\\beta}_1}{SESOI_{MAS_{upper}}-SESOI_{MAS_{lower}}} \\\\ Sensitivity &amp;= \\frac{(40 - -40)\\times 0.0014}{0.5--0.5} \\\\ Sensitivity &amp;= \\frac{(80)\\times 0.0014}{1} \\\\ Sensitivity &amp;= \\frac{0.11}{1} \\\\ Sensitivity &amp;= 0.11 \\end{split} \\tag{2.28} \\end{equation} \\] This means that the change in the YoYoIR1 test equal to SESOI will yield only a small proportion of SESOI in the MAS test. In the case where SESOI of the MAS test is unknown, using known SESOI of the YoYoIR1 test can be used to estimate it. This is done by using estimated \\(\\hat{\\beta}_1\\) (slope coefficient), as demonstrated in the Equation (2.29). \\[ \\begin{equation} \\begin{split} SESOI_{MAS_{upper}} &amp;= \\hat{\\beta}_1\\times SESOI_{YoYoIR1_{upper}} \\\\ SESOI_{MAS_{upper}} &amp;= 0.0014\\times 40 \\\\ SESOI_{MAS_{upper}} &amp;= 0.06 \\: km/h \\\\ \\\\ SESOI_{MAS_{lower}} &amp;= \\hat{\\beta}_1\\times SESOI_{YoYoIR1_{lower}} \\\\ SESOI_{MAS_{lower}} &amp;= 0.0014\\times -40 \\\\ SESOI_{MAS_{lower}} &amp;= -0.06 \\: km/h \\end{split} \\tag{2.29} \\end{equation} \\] Next magnitude-based question might be related to the practically significant strength of the association between two variables. For example, we would like to know if the residuals are higher or lower than the SESOI in the target variable (i.e. MAS, which is equal to ±0.5km/h). Figure 2.10 depicts scatter plot between two variable (panel A) and residuals (panel B) utilizing SESOI in MAS as the grey area. Figure 2.10: Scatter plot between two variables using SESOI to indicate practically significant difference A. Scatterplot with SESOI depicted as grey band around linear regression line. B. Residual plot, where the difference between MAS and linear regression line (model estimate) is plotted against linear regression line (fitted or predicted MAS). SESOI is represented with the grey band. Residuals within SESOI band are of no practical difference. Dashed lines represent upper and lower levels of agreement using RSE and 95% confidence level (or in other words, 95% of the residuals distribution will be within these two dashed lines). Magnitude-based estimators of the practically significant strength of the two variable association involve ratio between the SESOI (\\(SESOI_{upper} - SESOI_{lower}\\)) and RSE (SESOI to RSE), and PPER. SESOI to RSE indicates how big are the residuals compared to the SESOI, and thus a metric of the practical strength of the association. Assuming that residuals are being normally distributed, SESOI to RSE over 4 (or \\(2\\times 1.96\\)) would indicate excellent practical strength of the association. If you look at the Table 15, estimated SESOI to RSE in this example is not great, indicating poor practical strength of association. Proportion of practically equivalent residuals (PPER) as a measure of the practical strength of the association revolves around estimating proportions of residuals in the equivalent range, defined as SESOI in the target variable (which is exactly the same as already introduced pEquivalent estimator). PPER can be estimated with the brute-force method by simply counting residuals in the equivalent zone, or using algebraic method and assuming normally distributed residuals (i.e. using RSE of the residuals13). Figure 2.11 graphically depicts how PPER is calculated. Practically significant association between two variables would have PPER equal to 1, which indicates that all residuals are within confines of the SESOI. If you look at the Table 2.17, estimated PPER in this example is almost perfect, indicating great practical strength of the association between YoYoIR1 and MAS tests. Figure 2.11: Residuals of the linear regression model predicting MAS from YoYoIR1 test. Proportion of residuals within SESOI band represent PPER Table 2.17: Magnitude-based estimators of the association between two variables. Association is estimated using linear regression model. MAS is the target variable, and YoYoIR1 is the predictor SESOI YoYoIR1 (m) SESOI MAS (km/h) Sensitivity RSE SESOI MAS to RSE PPER ±40 ±0.5 0.11 0.22 4.57 0.98 Visual inspection from the Figure 2.11 and magnitude-based estimates from the Table 2.17 indicate that using YoYoIR1 test scores, we are able to predict14 MAS test scores with the error within SESOI. But would that be the case if the we want to predict YoYoIR1 from MAS test scores? Predictive performance of such model is depicted on the Figure 2.12 and magnitude-based estimator are enlisted in the Table 2.18. Figure 2.12: Linear regression model estimating association between YoYoIR1 and MAS tests where YoYoIR1 is now the target variable. A. Scatterplot with SESOI depicted as grey band around linear regression line. B. Residual plot, where the difference between YoYoIR1 and linear regression line (model estimate) is plotted against MAS variable. SESOI is represented with the grey band. Residuals within SESOI band are of no practical difference. Proportion of residuals within SESOI band represent PPER Table 2.18: Magnitude-based estimators of the association between two variables. Association is estimated using linear regression model. YoYoIR1 is the target variable, and MAS is the predictor SESOI YoYoIR1 (m) SESOI MAS (km/h) Sensitivity RSE SESOI YoYoIR1 to RSE PPER ±40 ±0.5 6.56 133.84 0.6 0.23 As clearly indicated with this example, when estimating practical association between two variables, it is very important which variable is the target and which is predictor. When it comes to Pearson's r, R-Squared and MIC, this is not the case and results are same regardless of which variable is predictor and which is target. From the analysis performed, it seems that predicting MAS from YoYoIR1 is practically useful and the association is practically significant. Unfortunately, the same is not the case when we try to predict YoYoIR1 from MAS. This might be due different physical traits that determine the test scores. For example, results in the YoYoIR1 test might depend on the traits that include, but are not limited to, same traits important for the MAS test. The purpose of descriptive analysis is only to describe - further analysis involving answering the why questions is in the domain of explanatory modeling and causal inference (which are covered in the Causal inference section), as well as Advanced uses of descriptive modeling, such as latent variable modeling. What is important to remember is that to describe magnitude-based association, it is important to clearly state which variable is the target and which is the predictor. 2.4 Advanced uses Advanced techniques in the descriptive statistics involve dimension reduction, such as principal component analysis (PCA), latent variable modeling, such as factor analysis (FA), or cluster analysis (Beaujean 2014; Borsboom 2008; Borsboom, Mellenbergh, and van Heerden 2003; Everitt and Hothorn 2011; Finch and French 2015; Kabacoff 2015). These techniques are beyond the scope of this book and the interested readers are directed to references provided. References "],
["prediction.html", "Chapter 3 Prediction 3.1 Overfitting 3.2 Cross-Validation 3.3 Bias-Variance decomposition and trade-off 3.4 Interpretability 3.5 Magnitude-based prediction estimators 3.6 Practical example: MAS and YoYoIR1 prediction", " Chapter 3 Prediction In many disciplines there is a near-exclusive use of the statistical models for causal inference15 and the assumption that models with high explanatory power are inherently of high predictive power (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017; Hernán, Hsu, and Healy 2019). There is a constant tug-of-war between prediction versus explanation, and experts are leaning on one side or the other. Some experts warn against over-reliance on explanatory models with poor predictive power (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017), whereas some warn against over-reliance on predictive models that lack causal explanatory power that can guide intervention (Hernán, Hsu, and Healy 2019; Pearl and Mackenzie 2018; Pearl, Glymour, and Jewell 2016; Pearl 2019). It is thus important to differentiate between the two and take into account the research question that we are trying to answer. In this book, I define predictive modeling by using definition from Galit Shmueli “as the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations” (Shmueli 2010, 291). Usually this predictive statistical model is treated as a black box. Black box approach implies that we are not really interested in underlying mechanism and relationships between the predictor variables, only in predictive performance of the model (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017) Linear regression model from Describing relationship between two variables section already introduced predictive question (“If I know someone’s YoYoIR1 score, what would be his or her MAS score? Is the prediction within SESOI?”) to complement the association one (“How is YoYoIR1 associated with MAS?”). This section will continue this quest and introduce essential concepts and caveats of the predictive analysis needed to answer predictive questions. 3.1 Overfitting To explain a few caveats with predictive modeling, let’s take slightly more complex example (although we will come back to YoYoIR1 and MAS relationship later). Imagine we know the true relationship between back squat relative 1RM (BS)16 and vertical jump height during a bodyweight squat jump (SJ; measured in cm). This true relationship is usually referred to as data generating process (DGP) (Carsey and Harden 2013) and one of the aims of causal inference tasks is to uncover parameters and mechanism of DGP from the acquired sample17. With predictive tasks this aim is of no direct interest, but rather reliable prediction regarding new or unseen observations. DGP is usually unknown, but with simulations, such as this one, DGP is known and it is used to generate the sample data. Simulation is thus excellent teaching tool, since one can play with the problem and understand how the statistical analysis works, since the true DGP is known and can be compared with estimates (Carsey and Harden 2013; Hopkins 2007; Guillaume A Rousselet, Pernet, and Wilcox 2019a). DGP is assumed to consist of systematic component \\(f(x)\\) and stochastic component \\(\\epsilon\\) (Equation (3.1)). \\[ \\begin{equation} Y = f(X) + \\epsilon \\tag{3.1} \\end{equation} \\] Systematic component is assumed to be fixed in the population (constant from sample to sample) and captures the true relationship \\(f(X)\\) among variables in the population (e.g. this can also be termed signal), while stochastic component represents random noise or random error, that varies from sample to sample, although its distribution remains the same. Random error is assumed to be normally distributed with mean of 0 and standard deviation which represents estimated parameter (either with RMSE or RSE). Thus, RMSE or RSE are estimates of \\(\\epsilon\\). In our example, the relationship between SJ and BS is expressed with the following Equation (3.2). \\[ \\begin{equation} \\begin{split} SJ &amp;= 30 + 15\\times BS\\times\\sin(BS)+\\epsilon \\\\ \\epsilon &amp;\\sim \\mathcal{N}(0,\\,2) \\end{split} \\tag{3.2} \\end{equation} \\] Systematic component in the DGP is represented with \\(30 + 15\\times BS\\times\\sin(BS)\\), and stochastic component is represented with the known random error (\\(\\epsilon\\)) that is normally distributed with the mean equal to zero and standard deviation equal to 2cm (\\(\\mathcal{N}(0,\\,2)\\)). This random error can be termed irreducible error (James et al. 2017), since it is inherent to the true DGP. As will be demonstrated shortly, models that perform better than this irreducible error are said to overfit. In other words, models are jumping to the noise. The objective of causal inference or explanatory modeling is to estimate the \\(f(X)\\) (estimate is indicated with the hat symbol: \\(\\hat{f}(x)\\)) or to understand the underlying DGP. With the predictive analysis, the goal is to find the best estimate of \\(Y\\) or \\(\\hat{y}\\). The underlying DGP is treated as a black box. To demonstrate a concept of overfitting, we are going to generate two samples (N=35 observations) from the DGP with BS ranging from 0.8 to 2.5. These samples are training and testing sample (Figure 3.1). Training sample is used to train the prediction model, while testing sample will be used as a holdout sample for evaluating model performance on the unseen data. Figure 3.1: Two samples simulated from the known DGP. Black line represents systematic component of the DGP and it is equal for both training and testing samples. Observations vary in the two samples due stochastic component in the DGP Model used to predict SJ from BS will be polynomial linear regression. Equation (3.3) explains first, second, and third degree polynomial linear regression function and provides a form for n-degree polynomials. Please, note that first degree polynomial function represents simple linear regression. \\[ \\begin{equation} \\begin{split} \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\hat{\\beta}_2 x_i^2 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\hat{\\beta}_2 x_i^2 + \\hat{\\beta}_3 x_i^3 \\\\ \\hat{y_i} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^1 + \\dots + \\hat{\\beta}_n x_i^n \\end{split} \\tag{3.3} \\end{equation} \\] Increasing polynomial degrees increases the flexibility of the polynomial regression model, and thus can represent tuning parameter that we can select based on the model performance. In other words, we might be interested in finding polynomial degree that minimized model error (or maximize model fit). Figure 3.2 contains model performance on the training data for polynomial degrees ranging from 1 to 20. Figure 3.2: Model fit with varying polynomial degrees. More degrees equals better model fit As can be seen from the Figure 3.2, the more flexible the model (or the higher the polynomial degree) the better it fits the data. But how do these models perform on the unseen, testing data sample? In order to quantify model performance, RMSE metric is used. Figure 3.3 demonstrates performance of the polynomial regression model on the training and testing data sample across different polynomial degrees. Figure 3.3: Testing and training errors across varying polynomial degrees. Model error is estimated with the RMSE metric, while polynomial degree represents tuning or flexibility parameter of the model. As can be noted from the figure, better training performance doesn’t imply better testing performance. Vertical dashed line represents the polynomial degree at which testing error is lowest. Polynomial degrees on the right of the vertical dashed line are said to overfit the data, while polynomial degree on the left are said to underfit the data As can be seen from the Figure 3.3, models with higher polynomial degrees tend to overfit (indicated by performance better than the known irreducible error \\(\\epsilon\\) visualized with the horizontal line at 2cm). Performance on the training data sample improves as the polynomial degrees increase, which is not the case with the performance on the testing data sample. There is clearly the best polynomial degree that has the best predictive performance on the unseen data. Polynomial degrees on the left of the vertical dashed line are said to underfit, while polynomial degrees on the right are said to overfit. The take home message is that predictive performance on the training data can be too optimistic, and for evaluating predictive performance of the model, unseen data must be used, otherwise the model might overfit. 3.2 Cross-Validation In order to evaluate predictive performance of the model, researchers usually remove some percent of data to be used as a testing or holdout sample. Unfortunately, this is not always possible (although it is recommended, particularly to evaluate final model performance, especially when there are multiple models and model tuning). One solution to these problems is cross-validation technique (James et al. 2017; Kuhn and Johnson 2018; Yarkoni and Westfall 2017). There are numerous variations of the cross-validation, but the simplest one is n-fold cross validation (Figure 15). N-fold cross validation involve splitting the data into 5 to 10 equal folds and using one fold as a testing or hold-out sample while performing model training on the other folds. This is repeated over N-iteration (in this case 5 to 10) and the model performance is averaged to get cross-validated model performance. Figure 3.4: Cross-Validation With predictive analysis and machine learning, different model’s tuning parameters are evaluated (as well as multiple different models) to estimate the one that gives the best predictive performance. It is thus important to utilize techniques such as cross-validation to avoid overfitting and too optimistic model selection. Certain models, such as lasso, ridge regression, and elastic-net implement regularization parameters that penalizes the model complexity and are used as a tuning variable (James et al. 2017; Kuhn and Johnson 2018; Yarkoni and Westfall 2017). This is useful in situations when there are a lot of predictors, and it is easy to overfit the model. Selecting the best regularization parameter that has the best cross-validated performance helps in simplifying the model and avoiding the overfit. These topics are beyond the scope of this book, and interested readers are directed to references provided. 3.2.1 Sample mean as the simplest predictive model We have already discussed in Sample mean as the simplest statistical model section that sample mean can be considered simplest model that describes a particular sample with the lowest RMSE. But can it be used for prediction? Here is an example to demonstrate both sample mean as a predictive model, as well as to demonstrate cross-validation technique. Let’s assume that we have collected N=10 observations: 15, 19, 28, 28, 30, 57, 71, 88, 95, 97. Sample mean is equal to 52.8. If we assume that the sample mean represents our prediction for the observations, we can easily calculate prediction error for each observation, which is simple difference (column Error in the Table 3.1). Table 3.1: Sample mean as prediction with associated prediction errors Observed Predicted Error Absolute Error Squared Error 15 52.8 37.8 37.8 1428.84 19 52.8 33.8 33.8 1142.44 28 52.8 24.8 24.8 615.04 28 52.8 24.8 24.8 615.04 30 52.8 22.8 22.8 519.84 57 52.8 -4.2 4.2 17.64 71 52.8 -18.2 18.2 331.24 88 52.8 -35.2 35.2 1239.04 95 52.8 -42.2 42.2 1780.84 97 52.8 -44.2 44.2 1953.64 Besides simple difference, Table 3.1 provides errors (or losses) using two common loss functions (see Sample mean as the simplest statistical model section in Description chapter): absolute loss (column Absolute Error) and quadratic loss (column Squared Error). We need to aggregate these errors or losses into a single metric using the cost function. If we calculate the mean of the prediction errors (column Error in the Table 3.1), we are going to get 0. This is because the positive and negative errors cancel each other out for the sample mean estimator. This error estimator is often referred to as mean bias error (MBE) or simply bias and is often used in validity and reliability analysis (see Validity and Reliability sections). If we take the mean of the absolute prediction errors (column Absolute error in the Table 3.1) we are going to get mean absolute error (MAE) estimator, which is in this example equal to 28.8. If we take the mean of the squared prediction errors (column Squared error in the Table 3.1) we are going to get mean square error (MSE) estimator, often called variance in the case of describing sample dispersion. In this example MSE is equal to 964.36. To bring back MSE to the same scale with the observation scale, square root of the MSE is taken. This represents root mean square error (RMSE), which is equal to 31.05. As explained in Sample mean as the simplest statistical model section, sample mean represents statistical model of the central tendency with the lowest RMSE. Aforementioned error estimators, MBE, MAE, MSE, and RMSE can be considered different cost functions. Which one should be used18? As always, it depends (Chai and Draxler 2014). Assuming Gaussian normal distribution of the errors, MSE and RMSE have very useful mathematical properties that can be utilized in modeling stochastic or random components (i.e. random error propagation and prediction error decomposition in Bias-Variance decomposition and trade-off section). This property will be utilized thorough this book and particularly in the Example of randomized control trial section when estimating random or stochastic component of the treatment effect. I personally prefer to report multiple estimators, including error estimators, which is also a strategy suggested by Chai and Draxler (2014). The are, of course, other loss and cost functions that could be used. For example, one might only use the maximal error (MaxErr) and minimal error (MinErr), rather than average. Discussion and review of these different metrics is beyond the scope of this book (for more information please check the package Metrics (Hamner and Frasco 2018) and the following references (Botchkarev 2019; Chai and Draxler 2014; Willmott and Matsuura 2005; Barron 2019)). Figure 3.5 visualize the most common loss functions that are used in both model training and as performance metrics. It is important to keep in mind that in the case of OLS regression, MSE (or RMSE) is minimized. It is thus important to make a distinction between cost function used in the optimization and model training (i.e. in OLS, parameters of the model are found so that MSE is minimized; in some machine-learning models Huber loss or Rigde loss19 is minimized; see Figure 3.5) versus cost function used as a performance metric (e.g. reporting pEquivalent, MaxErr or MinErr for OLS models). Figure 3.5: Common loss functions. A. Trellis plot of each loss function. B. Plot of all loss functions on a common scale. Huber loss is a combination of absolute and quadratic loss. The take-away point is to understand that there are multiple performance metrics that can be utilized and it is best to report multiple of them. Estimates for MBE, MAE, MSE20, and RMSE represent training sample predictive performance since sample mean is estimated using this very sample. We are interested how sample mean, as predictive model, perform on the unseen data. Cross-validation is one method to estimate model performance on unseen data. Table 3.2 contains 3-fold cross-validation sample used to train the model (in this case estimate sample mean) and to evaluate the performance on unseen data. Table 3.2: Example cross-validation using sample mean as the prediction model Fold Training sample mean Testing Sample MBE MAE RMSE MinErr MaxErr 1 15 19 28 – 30 – 71 88 95 – 49.43 – – – 28 – 57 – – – 97 -11.24 25.52 30.44 -47.57 21.43 2 15 19 – 28 – 57 – 88 – 97 50.67 – – 28 – 30 – 71 – 95 – -5.33 27.00 28.81 -44.33 22.67 3 – – 28 28 30 57 71 – 95 97 58.00 15 19 – – – – – 88 – – 17.33 37.33 37.73 -30.00 43.00 Since this is a small sample, we can repeat cross-validation few times. This is called repeated cross-validation. Let’s repeat 3-folds cross-validation for 5 repeats (Table 3.3). Table 3.3: Example repeated cross-validation using sample mean as the prediction model Repeat Fold Training sample mean Testing Sample MBE MAE RMSE MinErr MaxErr 1 1 15 19 28 – 30 – 71 88 95 – 49.43 – – – 28 – 57 – – – 97 -11.24 25.52 30.44 -47.57 21.43 1 2 15 19 – 28 – 57 – 88 – 97 50.67 – – 28 – 30 – 71 – 95 – -5.33 27.00 28.81 -44.33 22.67 1 3 – – 28 28 30 57 71 – 95 97 58.00 15 19 – – – – – 88 – – 17.33 37.33 37.73 -30.00 43.00 2 1 – 19 28 – 30 57 71 88 – – 48.83 15 – – 28 – – – – 95 97 -9.92 37.25 38.83 -48.17 33.83 2 2 15 – 28 28 – 57 – – 95 97 53.33 – 19 – – 30 – 71 88 – – 1.33 27.50 28.45 -34.67 34.33 2 3 15 19 – 28 30 – 71 88 95 97 55.38 – – 28 – – 57 – – – – 12.87 14.50 19.39 -1.63 27.37 3 1 – 19 28 28 30 57 71 88 – – 45.86 15 – – – – – – – 95 97 -23.14 43.71 44.66 -51.14 30.86 3 2 15 19 – – 30 – – 88 95 97 57.33 – – 28 28 – 57 71 – – – 11.33 18.17 21.84 -13.67 29.33 3 3 15 – 28 28 – 57 71 – 95 97 55.86 – 19 – – 30 – – 88 – – 10.19 31.62 31.94 -32.14 36.86 4 1 – 19 28 – 30 57 71 88 – 97 55.71 15 – – 28 – – – – 95 – 9.71 35.90 36.37 -39.29 40.71 4 2 15 – – 28 30 – 71 – 95 97 56.00 – 19 28 – – 57 – 88 – – 8.00 24.50 28.19 -32.00 37.00 4 3 15 19 28 28 – 57 – 88 95 – 47.14 – – – – 30 – 71 – – 97 -18.86 30.29 33.41 -49.86 17.14 5 1 15 19 – 28 – 57 – 88 95 97 57.00 – – 28 – 30 – 71 – – – 14.00 23.33 24.26 -14.00 29.00 5 2 15 – 28 28 30 – 71 88 95 – 50.71 – 19 – – – 57 – – – 97 -6.95 28.10 32.60 -46.29 31.71 5 3 – 19 28 – 30 57 71 – – 97 50.33 15 – – 28 – – – 88 95 – -6.17 35.00 35.92 -44.67 35.33 To calculate cross-validated prediction performance metrics, average of testing MBE, MAE, RMSE, MinErr, and MaxErr is calculated and reported as cvMBE, cvMAE, cvRMSE, cvMinErr, and cvMaxErr (Table 3.4). Prediction performance metrics don’t need to be averaged across cross-validation samples and can be instead estimated by binding (or pooling) all cross-validated samples together (i.e. target variable and predicted target variable). More about this at the end of this chapter in Practical example: MAS and YoYoIR1 prediction section. Table 3.4: Cross-validated prediction performance metrics (estimators) cvMBE cvMAE cvRMSE cvMinErr cvMaxErr 0.21 29.32 31.52 -35.29 31.37 As can be seen from the Table 3.4, all performance metrics estimated using repeated cross-validation are larger than the when estimated using the training data (full sample). Utilizing cross-validated estimates of performance (or error) should be used over training estimates when discussing predictive performance of the models. Unfortunately, this is almost never the case in sport science literature, where prediction is never estimated on unseen data and the model performance estimates can suffer from over-fitting. Using sample mean as predictive model represents simplistic example, although this type of model is usually referred to as baseline model. Baseline models are used as benchmarks or anchor when discussing performance of more elaborate and complex predictive models. We will come back to these at the end of this section when we will perform predictive analysis of the linear regression model used in Magnitude-based estimators section for predicting MAS from YoYoIR1 (and vice versa) tests. 3.3 Bias-Variance decomposition and trade-off Prediction error can be decomposed into two components, reducible and irreducible error21. Reducible error is the error that can be reduced with a better model, while irreducible error is the unknown error inherent to the DGP itself (James et al. 2017). Reducible error can be further divided into \\(Bias^2\\) and \\(Variance\\) (Figure 3.6 and Equation (3.4)). \\[ \\begin{equation} \\begin{split} Prediction \\; error &amp;= Reducible \\; error + Irreducible \\; error \\\\ Prediction \\; error &amp;= (Bias^2 + Variance) + Irreducible \\; error \\end{split} \\tag{3.4} \\end{equation} \\] \\(Bias^2\\) represents constant or systematic error, which is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model (James et al. 2017). \\(Variance\\) represents variable or random error, and refers to the amount by which model parameters would change if we estimated it by using a different training data set (James et al. 2017). Figure 3.6: Bias and variance decomposition of the error. A. Visual representation of \\(Bias^2\\) and \\(Variance\\) using the shooting target. B. Error can be decomposed to \\(Bias^2\\), \\(Variance\\), and \\(Irreducible \\: error\\), where \\(Bias^2\\) represents constant or systematic error and \\(Variance\\) represents variable or random error To understand this concept, we need to run a simulation using known relationship between BS and SJ (see Figure 3.1, Figure 3.2, and Equation (3.2)). Prediction error decomposition to bias and variance is done for a single data point. To do that we need to differentiate between the following variables: \\(x\\) predictors. In our case we only have one \\(x\\) predictor - \\(BS\\) and we will use \\(BS = 1.75\\) for this simulation \\(y_{true}\\) value for a particular \\(x\\) values. In our case \\(SJ\\) variable represents target variable \\(y\\), which is equal to \\(SJ = 30 + 15\\times BS\\times\\sin(BS)\\) or \\(SJ=55.83\\)cm. Both \\(x\\) and \\(y_{true}\\) values are constant across simulations \\(y_{observed}\\) represents observed \\(y\\) which differs from \\(y_{true}\\) due stochastic component \\(\\epsilon\\). This implies that \\(y_{observed}\\) randomly varies across simulations. The equation for \\(y_{observed}\\) is: \\(SJ = 30 + 15\\times BS\\times\\sin(BS) + \\mathcal{N}(0,\\,2)\\). Error \\(\\epsilon\\) represents irreducible error, because it is inherent to DGP and it is equal to \\(\\mathcal{N}(0,\\,2)\\). \\(y_{predicted}\\) represents model prediction using the training sample of \\(x\\) and \\(y_{observed}\\) values. For every simulation (N=200 simulations in total), the whole sample of N=35 observations is generated from the known DGP. This includes \\(x\\), \\(y_{true}\\), and \\(y_{observed}\\) variables. Polynomial regression models (from 1 to 20 polynomial degrees) are being fitted (or trained) using \\(x\\) predictors (in our case \\(BS\\) variable) and \\(y_{observed}\\) as our target variable. True \\(y\\) (\\(y_{true}\\)) is thus unknown to the model, but only to us. After models are trained, we estimate \\(y_{predicted}\\) using \\(BS = 1.75\\), for which the \\(y_{true}\\) is equal to \\(SJ=55.83\\)cm. Table 3.5 contains results of the first 10 simulations for 2nd degree polynomial model. Table 3.5: Results of first 10 simulations for 2nd degree polynomial linear regression model sim model x y_true y_observed y_predicted 1 2 1.75 55.83 54.65 55.27 2 2 1.75 55.83 53.80 55.34 3 2 1.75 55.83 56.03 55.34 4 2 1.75 55.83 55.71 55.68 5 2 1.75 55.83 55.52 54.91 6 2 1.75 55.83 57.55 55.81 7 2 1.75 55.83 52.03 55.08 8 2 1.75 55.83 56.35 55.92 9 2 1.75 55.83 54.05 54.64 10 2 1.75 55.83 58.16 55.57 To estimate reducible error, MSE estimator is used (Equation (3.5). \\[ \\begin{equation} Reducible \\: error = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{predicted_{j,x=1.75}} - y_{true_{x=1.75}})^2 \\tag{3.5} \\end{equation} \\] Reducible error can be decomposed to \\(Bias^2\\) and \\(Variance\\), or systematic error and variable or random error. \\(Bias^2\\) is squared difference between \\(y_{true}\\) and mean of \\(y_{predicted}\\) across simulations (Equation (3.6)). \\[ \\begin{equation} Bias^2 = (\\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{predicted_{j, x=1.75}}) - y_{true_{x=1.75}})^2 \\tag{3.6} \\end{equation} \\] \\(Variance\\) represents, pretty much, SD of the of the \\(y_{predicted}\\) and it is an estimate of how much the predictions vary across simulations (Equation (3.7)). \\[ \\begin{equation} Variance = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(\\bar{y_{predicted_{x=1.75}}} - y_{predicted_{j, x=1.75}})^2 \\tag{3.7} \\end{equation} \\] Reducible error is thus equal to the sum of \\(Bias^2\\) and \\(Variance\\). If you remember from Description section, \\(Bias^2\\) and \\(Variance\\) are nothing more than measure of central tendency (i.e. systematic or constant error) and measure of spread (i.e. variable or random error). Figure 3.6 also illustrates this concept. Irreducible error is estimated using MSE as well (Equation MSE-irreducible-error). \\[ \\begin{equation} Irreducible \\: error = \\frac{1}{n_{sim}}\\Sigma_{j=1}^{n_{sim}}(y_{observed_{x=1.75}} - y_{true_{x=1.75}})^2 \\tag{3.8} \\end{equation} \\] Since we know that the stochastic error in the DGP is normally distributed with SD=2cm, expected irreducible error should be around 4cm (this is because MSE is mean squared error, and RMSE, which is equivalent to SD, is calculated by doing square root of MSE). This might not be exactly 4cm due sampling error which is the topic of Statistical inference section. As explained in Equation (3.4), \\(Prediction \\: error\\) is equal to sum of \\(Bias^2\\), \\(Variance\\) and \\(Irreducible \\: error\\). Table contains estimated aforementioned errors using all 200 simulations for 2nd degree polynomial linear regression. Table 3.6: Calculated errors for all 200 simulations for 2nd degree polynomial linear regression model x y_true Prediction error Bias^2 Variance Irreducible error 2 1.75 55.83 5.17 0.09 0.21 4.87 This decomposition of errors is one useful mathematical property when using squared erors that I alluded to in the Cross-Validation section when discussing prediction error metrics. If we perfrom this analysis for each degree of polynomial fit, we will estimate prediction error, as well as \\(Bias^2\\) and \\(Variance\\) across model complexity (i.e. polynomial degrees). This is visualized in the Figure 3.7. Figure 3.7: Bias and Variance error decomposition. \\(Prediction \\: error\\) is indicated with the black line, and is decomposed to \\(Bias^2\\), \\(Variance\\), and \\(Irreducible \\: error\\). These are represents with areas of different color Beside decomposition of \\(Prediction \\: error\\) to \\(Bias^2\\), \\(Variance\\) and \\(Irreducible \\: error\\), it is important to notice the trade-off between \\(Bias^2\\) and \\(Variance\\). Linear regression models with lower polynomial degree, particularly 1st degree which is simple linear regression, has higher \\(Bias^2\\) due imposed linearity of the model (we can say that linear regression model is more biased). As \\(Bias^2\\) decreases with more flexible models (i.e. higher polynomial degree), \\(Variance\\) increase due model being too jumpy across simulations. To achieve best prediction (or the lower \\(Prediction \\: error\\)) a balance between \\(Bias^2\\) and \\(Variance\\) needs to be found, both within a particular model and across models. The free lunch theorem (Kuhn and Johnson 2018; Yarkoni and Westfall 2017) states that there is no single model that is the best across all different sets of problems. One needs to evaluate multiple models22 to estimate which one is the best for a particular problem at hand. To estimate \\(Bias^2\\) and \\(Variance\\), true DGP must be known. Unfortunately, we do not know these for the real world problems, only for simulations. But even when we do not know \\(y_{true}\\) values (and thus \\(Irreducible \\: error\\)), concepts of \\(Bias^2\\) and \\(Variance\\) can be applied in cross-validation samples (particularly when using multiple repeats) and can estimated using \\(y_{observed}\\). I will provide one such analysis in Practical example: MAS and YoYoIR1 prediction section. 3.4 Interpretability As explained, predictive models put predictive performance over explanation of the underlying DGP mechanism (which is treated as a black box). However, sometimes we might be interested in which predictor is the most important, how do predictions change when particular predictor changes, or why23 model made a particular prediction for a case of interest (Kuhn and Johnson 2018; Molnar 2018; Ribeiro, Singh, and Guestrin 2016). Model interpretability can be defined as the degree to which a human can understand the cause of a decision (Miller 2017; Molnar 2018; Biecek and Burzykowski 2019). Some models are more inherently interpretable (e.g. linear regression) and some are indeed very complex and hard to interpret (e.g. random forest or neural networks). For this reason, there are model-agnostic techniques that can help increase model interpretability. Excellent book and R (R Core Team 2020) package by Christoph Molnar (Molnar, Bischl, and Casalicchio 2018; Molnar 2018) demonstrates a few model-agnostic interpretation techniques. One such technique is estimating which predictor is the most important (variable importance). One method for estimating variable importance involves perturbing one predictor and estimating the change in model performance. The predictor whose perturbing causes the biggest change in model performance can be considered the most important (Kuhn and Johnson 2018; Molnar 2018). There are others approaches for estimating variable importance (B. M. Greenwell 2017b). Those interested in further details can also check vip (Greenwell, Boehmke, and Gray 2020) R (R Core Team 2020) package. One might be interested in how the predicted outcome changes when particular predictor changes. Techniques such as partial dependence plot (PDP), individual conditional expectation (ICE) and accumulated local effects (ALE) can be helpful in interpreting the effect of particular predictor on predicted outcome (Molnar, Bischl, and Casalicchio 2018; Molnar 2018; Goldstein et al. 2013; Zhao and Hastie 2019; B. M. Greenwell 2017a). Similar techniques are utilized in Prediction as a complement to causal inference section. Interested readers are also directed toward visreg (Breheny and Burchett 2017) and effects (Fox and Weisberg 2018; Fox 2003; Fox and Hong 2009) R (R Core Team 2020) packages for more information about visualizing models. It is important to keep in mind that these model-agnostic explanations should not be automatically treated as causal explanations (Pearl and Mackenzie 2018; Pearl 2019), but as mere association and descriptive analysis that can still be useful in understanding and interpreting the underlying predictive black-box. They are not without problems, such as correlated variables, interactions and other issues (Altmann et al. 2019). According to Judea Pearl (Pearl and Mackenzie 2018; Pearl 2019), prediction models should belong to the first level of ladder of causation24, which represents simple “curve fitting”. Although in under special conditions these techniques can have causal interpretation (Zhao and Hastie 2019). The distinctions, similarities and issues between predictive modeling, machine learning and causal inference is currently hot topic in debates between machine learning specialists, statisticians and philosophers of science and it is beyond the scope of this book to delve into the debate. Interested readers are directed towards work by Miguel Hernan (Hernán 2017, 2016, 2018; Hernán and Robins, n.d.; Hernán, Hsu, and Healy 2019), Judea Pearl (Pearl and Mackenzie 2018; Pearl, Glymour, and Jewell 2016; Pearl 2019, 2009), Samantha Kleinberg (Kleinberg 2015, 2018) and others (Watts et al. 2018; Saddiki and Balzer 2018; Kleinberg, Liang, and Mullainathan 2017; Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017). The next Causal Inference section introduces the causal inference as a specific task of statistical modeling. 3.5 Magnitude-based prediction estimators Similar to the magnitude-based estimators from Describing relationship between two variables section, one can utilize target variable SESOI to get magnitude-based estimates of predictive performance of the model. Rather than utilizing RSE as an estimate of the model fit in the training data, one can utilize cross-validated RMSE (cvRMSE), SESOI to cvRMSE, as well as cross-validated proportion of practically equivalent residuals (cvPPER) estimators. Continuing with the squat jump height and relative squat 1RM example, one can assume that the SESOI in the squat jump is ±1cm. For the sake of example, we can feature engineer (Kuhn and Johnson 2018, 2019) relative squat 1RM variable to include all 20 degree polynomials. This way, we have created 20 predictor variables. To avoid overfitting, elastic-net model (Friedman, Hastie, and Tibshirani 2010) implemented in the caret R package (Kuhn and Johnson 2018; Kuhn et al. 2018) is utilized, as well as repeated cross-validation involving 3 splits repeated 10 times. Predictive model performance is evaluated by using cvRMSE, together with magnitude-based performance estimators (SESOI to cvRMSE and cvPPER). Elastic-net model represents regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, or alpha and lambda tuning parameters (Friedman, Hastie, and Tibshirani 2010; James et al. 2017; Kuhn and Johnson 2018). Total of nine combinations of tuning parameters is evaluated using aforementioned repeated cross-validation, and the model with minimal cvRMSE is selected as the best one. Performance metrics of the best model are further reported. Table 3.7 contains cross-validated best model performance metrics together with model performance on the training data set. Table 3.7: Common predictive metrics and magnitude-based predictive metrics. Metrics starting with cv indicate cross-validated performance metrics. Metrics without cv indicate performance metrics on the training data set, which is often more optimistic SESOI (cm) cvRMSE (cm) SESOI to cvRMSE cvPPER RMSE (cm) SESOI to RMSE PPER ±1 2.19 0.91 0.33 1.99 1.01 0.38 Utilizing apriori known SESOI gives us practical anchor to evaluate predictive model performance. Reported SESOI to cvRMSE (0.91) as well as cvPPER (0.33) indicate very poor predictive performance of the model. In practical terms, utilizing relative squat 1RM doesn’t produce practically meaningful predictions given SESOI of ±1cm and the model as well as the data sample utilized. Model performance can be visualized using the training data set (Figure 3.8). PPER estimator, for both cross-validate estimate and training data performance estimate, utilized SD of the residuals and provided SESOI. Grey band on panels A and B on Figure 3.8 represents SESOI, and as can be visually inspected, model residuals are much wider than the SESOI, indicating poor practical predictive performance. Figure 3.8: Model performance on the training data set. A. Model with the lowest cvRMSE is selected. SESOI is depicted as grey band around the model prediction (blue line). B. Residuals scatter plot. Residuals outside of the SESOI band (grey band) indicate prediction which error is practically significant. PPER represents proportion of residuals inside the SESOI band Predictive tasks are focusing on providing the best predictions on the novel or unseen data without much concern about the underlying DGP. Predictive model performance can be evaluated by using magnitude-based approach to give insights into practical significance of the predictions. These magnitude-based prediction estimators, can be used to complement explanatory or causal inference tasks, rather than relying solely on the group-based and average-based estimators. This topic is further discussed in the Prediction as a complement to causal inference section. 3.6 Practical example: MAS and YoYoIR1 prediction In Describing relationship between two variables we have used two physical performance tests, MAS and YoYoIR1, to showcase relationship or association between two variables. Besides mere association, we have stepped into the domain of prediction by utilizing magnitude-based estimators such as PPER and SESOI to RMSE. As you have learned so far in this section, these predictions were made on the training data set. Let’s implement concepts learned so far to estimate predictive performance on the unseen data. Why is this important? Although very simple model, we are interested in predicting MAS from YoYoIR1 test score for a new or unseen individual. For this reason, it is important to get estimates of model performance on the unseen athletes. 3.6.1 Predicting MAS from YoYoIR1 Let’s first estimate predictive performance when predicting MAS scores from the YoYoIR1 score using MAS SESOI ±0.5km/h and linear regression. Figure 3.9 consists of two panels. Panel A depicts scatter plot between YoYoIR1 and MAS scores (black line represents linear model fit). Panel B depicts \\(y_{predicted}\\) (predicted or fitted MAS using simple linear regression; i.e. the black line on the panel A) against the model residuals \\(y_residual = y_{predicted} - y_{observed}\\), or Predicted MAS - MAS. The data points represent model performance on the full training data set. Figure 3.9: Scatter plot for simple linear regression between MAS and YoYoIR1 using the full training data sample. A. Scatter plot between MAS and YoYoIR1 scores. Black line indicates model prediction. B. Scatter plot between \\(y_{predicted}\\) (fitted or predicted MAS) against model residual \\(y_{residual} = y_{predicted} - y_{observed}\\), or Predicted MAS - MAS. Dotted lines indicate Levels of Agreement (LOA; i.e. upper and lower threshold that contain 95% of residuals distribution) and grey band indicates SESOI. Blue line indicate linear regression fit of the residuals and is used to indicate issues with the model (residuals) Predictive performance for the full training data set is enlisted in the Table 3.8. Table 3.8: Predictive performance using the full training data set MBE MAE RMSE PPER SESOI.to.RMSE R.squared MinErr MaxErr MaxAbsErr 0 0.17 0.21 0.97 4.73 0.74 -0.44 0.39 0.44 But as already explained, these are not predictive performance estimators for the unseen data. To estimate how the model performs on the unseen data (i.e. unseen athletes in this case), cross-validation is performed using 3 folds and 5 repeats. Estimated predictive performance for every cross-validation sample is enlisted in the Table 3.9. Table 3.9: Predictive performance for every repeated cross-validated sample fold MBE MAE RMSE PPER SESOI to RMSE R-squared MinErr MaxErr MaxAbsErr 1 Fold1.Rep1 -0.13 0.27 0.29 0.87 3.42 0.62 -0.46 0.38 0.46 2 Fold1.Rep2 -0.04 0.13 0.14 0.99 7.15 0.87 -0.22 0.15 0.22 3 Fold1.Rep3 0.13 0.24 0.27 0.90 3.74 0.79 -0.27 0.49 0.49 4 Fold1.Rep4 -0.17 0.25 0.28 0.89 3.52 0.41 -0.51 0.26 0.51 5 Fold1.Rep5 0.08 0.20 0.24 0.92 4.20 0.77 -0.28 0.46 0.46 6 Fold2.Rep1 0.09 0.15 0.19 0.97 5.18 0.87 -0.27 0.37 0.37 7 Fold2.Rep2 0.05 0.20 0.24 0.93 4.16 0.71 -0.31 0.41 0.41 8 Fold2.Rep3 -0.14 0.18 0.22 0.96 4.65 0.81 -0.38 0.18 0.38 9 Fold2.Rep4 -0.02 0.13 0.17 0.98 5.92 0.75 -0.30 0.33 0.33 10 Fold2.Rep5 -0.07 0.21 0.27 0.89 3.76 0.27 -0.50 0.33 0.50 11 Fold3.Rep1 0.04 0.13 0.16 0.99 6.27 0.60 -0.20 0.31 0.31 12 Fold3.Rep2 -0.01 0.20 0.24 0.92 4.21 0.54 -0.45 0.34 0.45 13 Fold3.Rep3 0.02 0.15 0.21 0.95 4.74 0.51 -0.44 0.35 0.44 14 Fold3.Rep4 0.21 0.23 0.28 0.91 3.57 0.83 -0.06 0.53 0.53 15 Fold3.Rep5 -0.01 0.17 0.19 0.97 5.29 0.82 -0.32 0.34 0.34 As explained in Cross-Validation section, to calculate overall cross-validated performance, the mean is calculated for the performance metrics in the Table 3.9. Besides reporting the mean as the summary for predictive performances across cross-validated samples, SD, min, and max can be reported too. Another method of summarizing predictive performance over cross-validated samples would be to bind or pool all \\(y_{observed}\\) and \\(y_{predicted}\\) scores from the test samples together and then calculate overall predictive performance metrics. These pooled cross_validated \\(y_{observed}\\) and \\(y_{predicted}\\) can also be visualized using the residuals plot (Panel C in Figure 3.10). Figure 3.10: Residuals plot. A. Model residuals using the training data. This is exactly the same as panel B in Figure 3.9. B. Model residuals using the cross-validated training data. C. Model residuals using the cross-validated testing data. As can be seen from the panels B and C in Figure 3.10, since we have used 5 repeats of the 3-fold cross-validations, each \\(y_{observed}\\) will have 5 assigned \\(y_{predicted}\\) scores. These can be used to estimate \\(Bias^2\\) and \\(Variance\\) as explained in the Bias-Variance decomposition and trade-off section. Since we do not know the \\(y_{true}\\) scores, we can only estimate \\(Bias^2\\) and \\(Variance\\) of the model for each \\(y_{observed}\\) using cross-validated \\(y_{predicted}\\). This is, of course, only possible if multiple repeats of cross-validation are performed. It bears repeating that this \\(Bias^2\\) and \\(Variance\\) decomposition of the prediction error using cross-validated \\(y_{predicted}\\) and \\(y_{observed}\\) are NOT the same as when using simulation and known DGP as done in Bias-Variance decomposition and trade-off section. But these can be useful diagnostic tools for checking where the model fails (e.g. what particular observation might be problematic or outlier, as well how does \\(Bias^2\\) and \\(Variance\\) changes across \\(y_{observed}\\) continuum). These two concepts are depicted on Figure 3.11 and Figure 3.12. Figure 3.11: Prediction error (\\(MSE\\)), \\(Bias^2\\), and \\(Variance\\) across repeated cross-validated testing data. X-axis on the panels represents observation index, as in \\(y_{i, observed}\\) Figure 3.12: Prediction error (\\(MSE\\)), \\(Bias^2\\) and \\(Variance\\) across repeated cross-validated testing data. X-axis on the panels represent \\(y_{observed}\\). Since there might be multiple equal \\(y_{observed}\\), min and max are used and represent ribbon over mean (indicated by line) Since \\(Bias\\) and \\(Variance\\) represent a quantitative summary of the residuals across cross-validations, the residuals and predicted observations across cross-validation testing folds can be visualized in more details as depicted on Figures 3.13 and 3.14. Figure 3.13: Testing prediction residuals across cross-validation folds summarized with cross-bars for every observation. Cross-bars represent ranges of testing residuals for each observation, while horizontal bar represent mean residual. The length of the bar represents \\(Variance\\), while distance between horizontal dashed line and horizontal line in the cross-bar (i.e. mean residual) represents \\(Bias\\). Figure 3.14: Testing prediction residuals across cross-validation folds summarized with cross-bars for every observation value. Cross-bars represent ranges of testing residuals for each observation, while horizontal bar represent mean residual. The length of the bar represents \\(Variance\\), while distance between horizontal dashed line and horizontal line in the cross-bar (i.e. mean residual) represents \\(Bias\\). Cross-validated, pooled, and full training data set predictive performance metrics can be found in the Table 3.10. Please note that the pooled predictive performance metrics are in the column testing.pooled. Table 3.10: Predictive performance summary metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 0.00 0.00 0.10 -0.17 0.21 MAE 0.17 0.17 0.19 0.19 0.05 0.13 0.27 RMSE 0.21 0.21 0.23 0.23 0.05 0.14 0.29 PPER 0.97 0.98 0.97 0.94 0.04 0.87 0.99 SESOI to RMSE 4.73 4.83 4.33 4.65 1.12 3.42 7.15 R-squared 0.74 0.75 0.68 0.68 0.18 0.27 0.87 MinErr -0.44 -0.49 -0.51 -0.33 0.13 -0.51 -0.06 MaxErr 0.39 0.42 0.53 0.35 0.10 0.15 0.53 MaxAbsErr 0.44 0.49 0.53 0.41 0.09 0.22 0.53 Summary from the Table 3.10 as well as the individual cross-validated sample predictive performance from the Table 3.9 are visually represented in the Figure 3.15). Figure 3.15: Cross-validated model performance. Dot and line bar indicate mean, min and max of the cross-validated performance. Dotted line indicate model performance on the training data set. As can be seen from the Figure 3.15, cross-validated prediction performance metrics do not differ much25 from the metrics estimated using the full training sample (calculated in Describing relationship between two variables section and in the Table 3.8, and indicated by the dotted horizontal line in the Figure 3.15). For some more complex models, these differences can be much larger and are clear indication of the model over-fitting. Overall, predicting MAS from the YoYoIR1 score, given the data collected, SESOI of ±0.5km/h, and linear regression as a model, is practically excellent. Please note that prediction can be practically useful (given SESOI) (PPER; CV from 0.87 to 0.99) even when R-squared is relatively low (CV from 0.27 to 0.87). And the vice versa in some cases. That’s the reason why we need to utilize magnitude-based estimators as a complement of contemporary estimators such as R-squared, RSE, and RMSE. 3.6.2 Predicting YoYoIR1 from MAS As shown in Describing relationship between two variables, predicting YoYoIR1 from MAS scores was not practically useful (or precise enough). But for the sake of completeness, let’s perform cross-validated prediction (using 3 folds and 5 repeats). YoYoIR1 SESOI is taken to be ±40m. Figure 3.16 depicts modified Bland-Altman plot for predictions using the full training data set. Visual inspection demonstrates that many points are outside of SESOI band, indicating poor practically significant (or useful) predictions. Figure 3.16: Scatter plot for simple linear regression between YoYoIR1 and MAS using the full training data sample. A. Scatter plot between YoYoIR1 and MAS scores. Black line indicates model prediction. B. Scatter plot between \\(y_{predicted}\\) (fitted or predicted YoYoIR1) against model residual \\(y_{residual} = y_{predicted} - y_{observed}\\), or Predicted YoYoIR1 - YoYoIR1. Dotted lines indicate Levels of Agreement (LOA; i.e. upper and lower threshold that contain 95% of residuals distribution) and grey band indicates SESOI. Blue line indicate linear regression fit of the residuals and is used to indicate issues with the model (residuals) Predictive performance metrics can be found in the Table 3.10. As already expected, predicting YoYoIR1 from the MAS score, given the data collected, SESOI and linear regression model is not precise enough to be practically useful. Please note that the R-squared is very close to R-squared from the Table 3.10, but the PPER is much worse. Another reason to complement contemporary estimators with magnitude-based ones. Table 3.11: Predictive performance summary metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 3.13 1.95 50.33 -96.20 82.14 MAE 104.69 103.04 112.98 112.59 24.99 66.61 154.76 RMSE 129.30 127.55 138.73 135.92 26.46 82.62 175.52 PPER 0.24 0.25 0.23 0.22 0.05 0.17 0.34 SESOI to RMSE 0.62 0.63 0.58 0.61 0.14 0.46 0.97 R-squared 0.74 0.74 0.70 0.70 0.14 0.48 0.88 MinErr -235.93 -261.63 -253.04 -193.59 43.68 -253.04 -76.70 MaxErr 284.07 309.00 323.53 225.37 89.69 51.02 323.53 MaxAbsErr 284.07 309.00 323.53 260.89 44.88 158.47 323.53 In the second part of this book, we will get back to this example and estimate predictive performance using different models besides linear regression (like baseline prediction and regression trees) References "],
["causal-inference.html", "Chapter 4 Causal inference 4.1 Necessary versus sufficient causality 4.2 Observational data 4.3 Potential outcomes or counterfactuals 4.4 Ceteris paribus and the biases 4.5 Subject matter knowledge 4.6 Example of randomized control trial 4.7 Prediction as a complement to causal inference 4.8 Ergodicity", " Chapter 4 Causal inference Does playing basketball makes one taller? This is a an example of a causal question. Wrestling with the concept of causality, as a philosophical construct is outside the scope of this book (and the author too), but I will define it using the counterfactual theory or potential outcomes perspective (Hernán, Hsu, and Healy 2019; Kleinberg 2015; Pearl and Mackenzie 2018; Angrist and Pischke 2015; Gelman 2011) that define causes in terms of how things would have been different had the cause not occurred, as well as from causality-as-intervention perspective (Gelman 2011), which necessitates clearly defined interventions (Hernán 2018, 2016; Hernán and Taubman 2008). In other words, would someone be shorter if basketball was never trained? There are two broad classes of inferential questions that focus on what if and why: forward causal inference (“What might happen if we do X?”) and reverse causal inference (“What causes Y? Why?”) (Gelman 2011). Forward causation is more clearly defined problem, where the goal is to quantify the causal effect of treatment. Questions of forward causation are most directly studied using randomization (Gelman 2011) and are answered from the above mentioned causality-as-intervention and counterfactual perspectives. Reverse causation is more complex and it is more related to explaining the causal chains using the system-variable approach. Article by Gelman (2011) provides great overview of the most common causal perspectives, out of which I will mostly focus on forward causation. 4.1 Necessary versus sufficient causality Furthermore, we also need to distinguish between four kinds of causation (Pearl and Mackenzie 2018; Kleinberg 2015): necessary causation, sufficient causation and neither or both. For example, if someone says that A causes B, then: If A is necessary for B, it means that if A never happened (counterfactual reasoning), then B will never happen. Or, in other words, B can never happen without A. But sufficient causality also means that A can happen without B happening. If A is sufficient for B, it means that if you have A, you will always have B. In other words, B always follows A. However, sometimes B can happen without A If A is neither sufficient nor necessary for B, then sometimes when A happens B will happen. B can also happen without A. If A is both necessary and sufficient for B, then B will always happen after A, and B will never happen without A. Table 4.1 contains summary of the above necessary and sufficient causality. In all four types of causation, the concept of counterfactual reasoning is invoked. Table 4.1: Four kinds of causation Cause Necessary Sufficient Neither Both A happens B might happen B always happen B might happen B always happen A doesn’t happen B never happens B might happen B might happen B never happens Although the causal inference is a broad area of research, philosophical discussion and conflicts, there are a few key concepts that need to be introduced to get the big picture and understand the basics behind the aims of causal inference. Let’s start with an example involving the aforementioned question whether playing basketball makes one taller. 4.2 Observational data In order to answer this question, we have collected height data (expressed in cm) for the total of N=30 athletes, of which N=15 play basketball, and N=15 don’t play basketball (Table 4.2). Playing basketball can be considered intervention or treatment, in which causal effect we are interested in. Basketball players are considered intervention group or treatment group and those without the treatment are considered comparison group or control group Table 4.2: Height in the treatment and control groups Athlete Treatment Height (cm) Athlete 27 Basketball 214 Athlete 01 Basketball 214 Athlete 25 Basketball 211 Athlete 19 Basketball 210 Athlete 03 Basketball 207 Athlete 21 Basketball 200 Athlete 23 Basketball 199 Athlete 15 Basketball 198 Athlete 17 Basketball 193 Athlete 07 Basketball 192 Athlete 29 Basketball 192 Athlete 13 Basketball 191 Athlete 05 Basketball 191 Athlete 11 Basketball 184 Athlete 09 Basketball 180 Athlete 02 Control 189 Athlete 28 Control 183 Athlete 06 Control 181 Athlete 14 Control 180 Athlete 04 Control 179 Athlete 12 Control 176 Athlete 18 Control 176 Athlete 08 Control 173 Athlete 26 Control 173 Athlete 24 Control 170 Athlete 30 Control 168 Athlete 20 Control 168 Athlete 16 Control 165 Athlete 10 Control 165 Athlete 22 Control 163 Using descriptive estimators introduced in the Description section, one can quickly calculate the group mean and SD as well as their difference (Table 4.3). But does mean difference between basketball and control represent average causal effect (ACE)26? No, unfortunately not! Table 4.3: Descriptive analysis of the groups Mean (cm) SD (cm) Basketball 198.59 10.86 Control 174.04 7.54 Difference 24.55 13.22 4.3 Potential outcomes or counterfactuals To explain why this is the case, we need to imagine alternate counterfactual reality. What is needed are two potential outcomes: \\(Height_{0}\\), which represents height of the person if one doesn’t train basketball, and \\(Height_{1}\\) which represents height of the person if basketball is being played (Table 4.4). As can be guessed, the Basketball group has known \\(Height_{1}\\), but unknown \\(Height_{0}\\) and vice versa for the Control group. Table 4.4: Counterfactuals of potential outcomes that are unknown Athlete Treatment Height_0 (cm) Height_1 (cm) Height (cm) Causal Effect (cm) Athlete 27 Basketball ??? 214 214 ??? Athlete 01 Basketball ??? 214 214 ??? Athlete 25 Basketball ??? 211 211 ??? Athlete 19 Basketball ??? 210 210 ??? Athlete 03 Basketball ??? 207 207 ??? Athlete 21 Basketball ??? 200 200 ??? Athlete 23 Basketball ??? 199 199 ??? Athlete 15 Basketball ??? 198 198 ??? Athlete 17 Basketball ??? 193 193 ??? Athlete 07 Basketball ??? 192 192 ??? Athlete 29 Basketball ??? 192 192 ??? Athlete 13 Basketball ??? 191 191 ??? Athlete 05 Basketball ??? 191 191 ??? Athlete 11 Basketball ??? 184 184 ??? Athlete 09 Basketball ??? 180 180 ??? Athlete 02 Control 189 ??? 189 ??? Athlete 28 Control 183 ??? 183 ??? Athlete 06 Control 181 ??? 181 ??? Athlete 14 Control 180 ??? 180 ??? Athlete 04 Control 179 ??? 179 ??? Athlete 12 Control 176 ??? 176 ??? Athlete 18 Control 176 ??? 176 ??? Athlete 08 Control 173 ??? 173 ??? Athlete 26 Control 173 ??? 173 ??? Athlete 24 Control 170 ??? 170 ??? Athlete 30 Control 168 ??? 168 ??? Athlete 20 Control 168 ??? 168 ??? Athlete 16 Control 165 ??? 165 ??? Athlete 10 Control 165 ??? 165 ??? Athlete 22 Control 163 ??? 163 ??? Unfortunately, these potential outcomes are unknown, and thus individual causal effects are unknown as well. We just do not know what might have happened to individual outcomes in counterfactual world (i.e. alternate reality). A good control group serves as a proxy to reveal what might have happened on average to the treated group in the counterfactual world where they are not treated. Since the basketball data is simulated, the exact DGP is known (the true systematic or main causal effect of playing basketball on height is exactly zero), which again demonstrates the use of simulations as a great learning tool, in this case understanding the underlying causal mechanisms (Table 4.5). Individual causal effect in this case is the difference between two potential outcomes: \\(Height_{1}\\) and \\(Height_{0}\\). Table 4.5: Simulated causal effects and known counterfactuals Athlete Treatment Height_0 (cm) Height_1 (cm) Height (cm) Causal Effect (cm) Athlete 27 Basketball 214 214 214 0.12 Athlete 01 Basketball 214 214 214 0.00 Athlete 25 Basketball 212 211 211 -1.10 Athlete 19 Basketball 210 210 210 0.90 Athlete 03 Basketball 208 207 207 -0.07 Athlete 21 Basketball 200 200 200 0.20 Athlete 23 Basketball 198 199 199 0.57 Athlete 15 Basketball 198 198 198 0.18 Athlete 17 Basketball 193 193 193 0.44 Athlete 07 Basketball 193 192 192 -0.09 Athlete 29 Basketball 193 192 192 -0.40 Athlete 13 Basketball 192 191 191 -0.36 Athlete 05 Basketball 191 191 191 -0.15 Athlete 11 Basketball 183 184 184 0.46 Athlete 09 Basketball 179 180 180 0.72 Athlete 02 Control 189 189 189 0.06 Athlete 28 Control 183 184 183 0.41 Athlete 06 Control 181 181 181 0.42 Athlete 14 Control 180 180 180 -0.66 Athlete 04 Control 179 179 179 0.10 Athlete 12 Control 176 176 176 -0.39 Athlete 18 Control 176 175 176 -0.31 Athlete 08 Control 173 173 173 -0.55 Athlete 26 Control 173 174 173 0.77 Athlete 24 Control 170 170 170 0.02 Athlete 30 Control 168 168 168 0.27 Athlete 20 Control 168 168 168 -0.03 Athlete 16 Control 165 165 165 -0.01 Athlete 10 Control 165 164 165 -0.81 Athlete 22 Control 163 163 163 0.00 From Table 4.5, we can state that the mean difference between the groups consists of two components: average causal effect and the selection bias (Angrist and Pischke 2015) (Equation (4.1)). \\[ \\begin{equation} \\begin{split} mean_{difference} &amp;= Average \\; causal\\; effect + Selection\\; bias \\\\ Average \\; causal\\; effect &amp;= \\frac{1}{N_{Basketball}}\\Sigma_{i=1}^{n}(Height_{1i} - Height_{0i}) \\\\ Selection\\; bias &amp;= \\frac{1}{N_{Basketball}}\\Sigma_{i=1}^{n}Height_{0i} - \\frac{1}{N_{Control}}\\Sigma_{i=1}^{n}Height_{0i} \\end{split} \\tag{4.1} \\end{equation} \\] The mean group difference we have observed (24.55cm) is due to average causal effect (0.1cm) and selection bias (24.46cm). In other words, observed mean group difference can be explained solely by selection bias. Since we know the DGP behind the basketball data, we know that there is no systematic causal effect of playing basketball on height. On top of the selection bias involved in the example above, other confounders might be involved, such as age, sex, race, experience and others, some of which can be measured and some might be unknown. These are also referred to as the third variable which confounds the causal relationship between treatment and the outcome. In this example, all subjects from the Basketball group might be older males, whereas all the subjects from the Control group might be be younger females, and this can explain the group differences, rather than causal effect of playing basketball. 4.4 Ceteris paribus and the biases It is important to understand that, in order to have causal interpretation, comparisons need to be made under ceteris paribus conditions (Angrist and Pischke 2015), which is Latin for other things equal. In the basketball example above, we cannot make causal claim that playing basketball makes one taller, since comparison between the groups is not done in the ceteris paribus conditions due to the selection bias involved. We also know this since we know the DGP behind the observed data. Causal inference thus aims to achieve ceteris paribus conditions needed to make causal interpretations by careful considerations of the known and unknown biases involved (Angrist and Pischke 2015; Hernán 2017, 2016; Hernán and Robins, n.d.; Hernán, Hsu, and Healy 2019; Lederer et al. 2019; Rohrer 2018; Shrier and Platt 2008). According to Hernan et al. (Hernán 2017; Hernán and Robins, n.d.), there are three types of biases involved in causal inference: confounding, selection bias and measurement bias. Confounding is the bias that arises when treatment and outcome share causes. This is because treatment was not randomly assigned (Hernán 2017; Hernán and Robins, n.d.). For example, athletes that are naturally taller might be choosing to play basketball due to success and enjoyment over their shorter peers. On the other hand, it might be some hidden confounder that motivates to-be-tall athletes to choose basketball. Known and measured confounders from the observational studies can be taken into account to create ceteris paribus conditions when estimating causal effects (Angrist and Pischke 2015; Hernán 2017; Hernán and Robins, n.d.; Lederer et al. 2019; Rohrer 2018; Shrier and Platt 2008). 4.4.1 Randomization The first line of defence against confounding and selection bias is to randomly assign athletes to treatment, otherwise known as randomized trial or randomized experiment. Random assignment makes comparison between groups ceteris paribus providing the sample is large enough to ensure that differences in the individual characteristics such as age, sex, experience and other potential confounders are washed out (Angrist and Pischke 2015). In other words, random assignment works not by eliminating individual differences but rather by ensuring that the mix of the individuals being compared is the same, including the ways we cannot easily measure or observe (Angrist and Pischke 2015). In case the individuals from the basketball example were randomly assigned, given the known causal DGP, then the mean difference between the groups would be more indicative of the causal effect of playing basketball on height (Table 4.6). Table 4.6: Randomized participants Athlete Treatment Height (cm) Athlete 01 Basketball 214 Athlete 25 Basketball 211 Athlete 19 Basketball 210 Athlete 03 Basketball 207 Athlete 23 Basketball 199 Athlete 15 Basketball 198 Athlete 13 Basketball 191 Athlete 02 Basketball 189 Athlete 28 Basketball 184 Athlete 14 Basketball 180 Athlete 04 Basketball 179 Athlete 12 Basketball 176 Athlete 26 Basketball 174 Athlete 24 Basketball 170 Athlete 16 Basketball 165 Athlete 27 Control 214 Athlete 21 Control 200 Athlete 29 Control 193 Athlete 07 Control 193 Athlete 17 Control 193 Athlete 05 Control 191 Athlete 11 Control 183 Athlete 06 Control 181 Athlete 09 Control 179 Athlete 18 Control 176 Athlete 08 Control 173 Athlete 30 Control 168 Athlete 20 Control 168 Athlete 10 Control 165 Athlete 22 Control 163 If we calculate the mean differences in this randomly assigned basketball treatment (Table 4.7), we can quickly notice that random assignment washed out selection bias involved with the observational study, and that the mean difference is closer to the known systematic (or average or expected) causal effect. The difference between estimated systematic causal effect using mean group difference from the randomized trial and the true causal effect is due to the sampling error which will be explained in the Statistical inference section. Table 4.7: Descriptive summary of randomized participants Mean (cm) SD (cm) Basketball 189.91 16.15 Control 182.65 14.44 Difference 7.25 21.66 Apart from creating ceteris paribus conditions, randomization generates a good control group that serves as a proxy to reveal what might have happened to the treated group in the counterfactual world where they are not treated, since \\(Height_0\\) is not known for the basketball group. Creating those conditions with randomized trial demands careful considerations and balance checking since biases can crawl inside the causal interpretation. The logic of randomized trial is simple, yet the logistics can be quite complex. For example, a sample of sufficient size might not be practically feasible, and imbalances in the known confounders can be still found in the groups, thus demanding further control and adjustment in the analysis (e.g. using ANCOVA instead of ANOVA, adjusting for confounders in the linear regression by introducing them as interactions) in order to create ceteris paribus conditions needed to evaluate causal claims. Belief effect can sneak in, for example, if the treatment group knows they are being treated, or if researchers motivate treatment groups harder, since they expect and hope for better outcomes. For this reason, blinding both the subjects and researches can be considered, as well as providing placebo treatment to the Control group. In sport science research blinding and providing placebo can be problematic. For example, if our intervention is a novel training method or a technology, both researchers and subjects will expect better outcomes which can bias causal interpretations. 4.5 Subject matter knowledge One of the main problems with randomized trials is that it cannot be done in most real life settings, either due to the ethical or practical reasons. For example, if studying effects of smoking on baby mortality and birth defects, which parent would accept being in the treatment group. Or if studying effects of resistance training on injury risk in football players, which professional organization would allow random assignment to the treatment that is lesser than the known best practices and can predispose athletes to the injuries or sub-par preparation? For this reason, reliance on observation studies is the best we can do. However, in order to create ceteris paribus conditions necessary to minimize bias in the causal interpretations, expert subject-matter knowledge is needed, not only to describe the causal structure of the system under study, but also to specify the causal questions and identify relevant data sources (Hernán, Hsu, and Healy 2019). Imagine asking the following causal question: “Does training load lead to overuse injuries in professional sports”. It takes expert subject matter knowledge to specify the treatment construct (i.e. “training load”), to figure out how should be measured, as well as to quantify the measurement error which can induce measurement bias, to state over which time period the treatment is done, as well as to specify the outcome construct (i.e. “overuse-injuries”), and to define the variables and constructs that confound and define the causal network underlying such a question. This subject matter is fallible of course, and the constructs, variables and the causal network can be represented with pluralistic models that represents “Small World” maps of the complex “Large World”, in which we are hoping to deploy the findings (please refer to the Introduction for more information about this concept). Drawing assumptions that underly causal structure using direct acyclical graphs (DAGs) (Hernán 2017; Hernán and Robins, n.d.; Pearl and Mackenzie 2018; Rohrer 2018; Saddiki and Balzer 2018; Shrier and Platt 2008; Textor et al. 2017) represents a step forward in acknowledging the issues above, by providing transparency of the assumptions involved and bridging the subjective - objective dichotomy. 4.6 Example of randomized control trial Let’s consider the following example. We are interested in estimating causal effect of the plyometric training on the vertical jump height. To estimate causal effect, randomized control trial (RCT) is utilized. RCT utilizes two groups: Treatment (N=15) and Control (N=15), measured two times: Pre-test and Post-test. Treatment group received plyometric training over the course of three months, while Control group continued with normal training. The results of RCT study can be found in the Table 4.8. To estimate practical significance of the treatment effect, SESOI of ±2.5cm is selected to indicate minimal change of the practical value. It is important to have “well defined interventions” (Hernán 2018, 2016; Hernán and Taubman 2008), thus the question that should be answered is as follows: “Does plyometric training added to normal training improves vertical jump height over period of three months?” Table 4.8: Randomized control trial data Athlete Group Pre-test (cm) Post-test (cm) Change (cm) Athlete 01 Treatment 37.98 52.86 14.93 Athlete 27 Treatment 44.79 58.50 13.43 Athlete 19 Treatment 46.77 59.76 12.99 Athlete 25 Treatment 38.90 49.58 10.81 Athlete 03 Treatment 41.29 51.41 10.34 Athlete 23 Treatment 48.41 57.57 8.58 Athlete 17 Treatment 44.81 51.41 7.85 Athlete 21 Treatment 37.14 44.95 7.37 Athlete 15 Treatment 46.69 52.73 6.14 Athlete 29 Treatment 42.77 47.38 5.02 Athlete 13 Treatment 49.66 54.11 4.46 Athlete 05 Treatment 37.92 41.63 3.78 Athlete 07 Treatment 41.03 45.41 3.42 Athlete 11 Treatment 45.27 46.72 1.82 Athlete 12 Control 42.56 44.29 1.01 Athlete 28 Control 47.06 47.98 0.55 Athlete 04 Control 44.53 45.13 0.12 Athlete 02 Control 49.63 48.86 -0.01 Athlete 08 Control 41.11 42.13 -0.45 Athlete 26 Control 42.31 41.61 -0.51 Athlete 06 Control 45.96 45.70 -0.52 Athlete 14 Control 44.51 42.89 -0.63 Athlete 18 Control 42.57 42.15 -0.74 Athlete 16 Control 37.63 37.83 -0.75 Athlete 22 Control 36.52 34.83 -0.97 Athlete 24 Control 40.15 39.88 -1.03 Athlete 30 Control 39.34 38.34 -1.21 Athlete 09 Treatment 47.61 45.62 -1.57 Athlete 20 Control 38.94 36.97 -1.72 Athlete 10 Control 36.77 34.15 -2.26 Descriptive summary statistics for Treatment and Control groups are enlisted in the Table 4.9, and visually depicted in the Figure 4.1. Table 4.9: RCT summary using mean ± SD Group Pre-test (cm) Post-test (cm) Change (cm) Treatment 43.4 ± 4.15 50.64 ± 5.4 7.29 ± 4.65 Control 41.97 ± 3.87 41.52 ± 4.49 -0.61 ± 0.83 Figure 4.1: Visual analysis of RCT using Treatment and Control groups. A and B. Raincloud plot of the Pre-test and Post-test scores for Treatment and Control groups. Blue color indicates Control group and orange color indicates Treatment group. C and D. Raincloud plot of the change scores for the Treatment and Control groups. SESOI is indicated with a grey band Further analysis might involve separate dependent groups analysis for both Treatment and Control (Table 4.10), or in other words, the analysis of the change scores. To estimate Cohen's d, pooled SD of the Pre-test scores in both Treatment and Control is utilized. (see Equation (2.11)). Table 4.10: Descriptive analysis of the change scores for Treatment and Control groups independently Estimator Control Treatment Mean change (cm) -0.61 7.29 SDchange (cm) 0.83 4.65 SDpre-test pooled (cm) 4.01 4.01 Cohen’s d -0.15 1.82 SESOI lower (cm) -2.50 -2.50 SESOI upper (cm) 2.50 2.50 Change to SESOI -0.12 1.46 SDchange to SESOI 0.17 0.93 pLower 0.06 0.03 pEquivalent 0.93 0.14 pHigher 0.01 0.83 Figure 4.2 depicts same information as Figure 4.1 but organized differently and conveying different comparison. Figure 4.2: Visual analysis of RCT using Treatment and Control groups. A and B. Scatter plot of Pre-test and Post-test scores for Treatment and Control groups. Green line indicates change higher than SESOI upper, grey line indicates change within SESOI band, and red line indicates negative change lower than SESOI lower. C. Distribution of the change scores for Treatment (orange) and Control (blue) groups. Grey rectangle indicates SESOI band. But we are not that interested in independent analysis of Treatment and Control groups, but rather on their differences and understanding of the causal effect of the treatment (i.e. understanding and estimating parameters of the underlying DGP). As stated, treatment effect consists of two components: systematic component or main effect (i.e. expected or average causal effect), and stochastic component or random effect (i.e. that varies between individuals) (see Figure 4.3). As already explained, Control group serves as a proxy to what might have happened to the Treatment group in the counterfactual world, and thus allows for casual interpretation of the treatment effect. There are two effects at play with this RCT design: treatment effect and non-treatment effect. The latter captures all effects not directly controlled by a treatment, but assumes it affects both groups equally (Figure 4.3). For example, if we are treating kids for longer period of time, non-treatment effect might be related to the growth and associated effects. Another non-treatment effect is measurement error (discussed in more details in Measurement Error section). Figure 4.3: Treatment and Non-treatment effects of intervention. Both treatment and non-treatment effects consists of two components: systematic and random. Treatment group experiences both treatment and non-treatment effects, while Control group experiences only non-treatment effects. The following equation captures the essence of estimating Treatment effects from Pre-test and Post-test scores in the Treatment and Control groups (Equation (4.2)): \\[ \\begin{equation} \\begin{split} Treatment_{post} &amp;= Treatment_{pre} + Treatment \\; Effect + NonTreatment \\; Effect \\\\ Control_{post} &amp;= Control_{pre} + NonTreatment \\; Effect \\\\ \\\\ NonTreatment \\; Effect &amp;= Control_{post} - Control_{pre} \\\\ Treatment \\; Effect &amp;= Treatment_{post} - Treatment_{pre} - NonTreatment \\; Effect \\\\ \\\\ Treatment \\; Effect &amp;= (Treatment_{post} - Treatment_{pre}) - (Control_{post} - Control_{pre}) \\\\ Treatment \\; Effect &amp;= Treatment_{change} - Control_{change} \\end{split} \\tag{4.2} \\end{equation} \\] From the Equation (4.2), the differences between the changes in Treatment and Control groups can be interpreted as the estimate of the causal effect of the treatment. More precisely, average causal effect or expected causal effect represent systematic treatment effect. This is estimated using difference between mean Treatment change and mean Control change. Table 4.11 contains descriptive statistics of the change score differences. Panel C in the Figure 4.2 depicts distribution of the change scores and reflect the calculus in the Table 4.11 graphically. Table 4.11: Descriptive statistics of the change score differences Mean difference (cm) Cohen’s d Difference to SESOI pLower diff pEquivalent diff pHigher diff 7.9 9.56 1.58 -0.03 -0.79 0.82 Cohen's d in the Table 4.11 is calculated by using the Equation (4.3) and it estimates standardized difference between change scores in Treatment and the Control groups. \\[ \\begin{equation} Cohen&#39;s\\;d = \\frac{mean_{treatment\\; group \\; change} - mean_{control\\; group \\;change}}{SD_{control\\; group \\; change}} \\tag{4.3} \\end{equation} \\] Besides estimating systematic component of the treatment (i.e. the difference between the mean change in Treatment and Control groups), we might be interested in estimating random component and proportions of lower, equivalent and higher effects compared to SESOI (pLower, pEquivalent, and pHigher). Unfortunately, differences in pLower, pEquivalent, and pHigher from Table 4.11 don’t answer this question, but rather the expected difference in proportions compared to Control (e.g. the expected improvement of 0.82 in observing proportion of higher change outcomes compared to Control). Since the changes in Treatment group are due both to the treatment and non-treatment effects (equation 29), the average treatment effect (systematic component) represents the difference between the mean changes in Treatment and Control groups (Table 4.11). In the same manner, the variance of the change scores in the Treatment group are due to the random component of the treatment and non-treatment effects. Assuming normal (Gaussian) distribution of the random components, the SD of the treatment effects (\\(SD_{TE}\\))27 is estimated using the following Equation (4.4). \\[ \\begin{equation} \\begin{split} \\epsilon_{treatment \\;group \\;change} &amp;= \\epsilon_{treatment \\; effect} + \\epsilon_{nontreatment \\; effect} \\\\ \\epsilon_{control \\;group \\;change} &amp;= \\epsilon_{nontreatment \\; effect} \\\\ \\epsilon_{treatment \\; effect} &amp;= \\epsilon_{treatment \\;group \\;change} - \\epsilon_{control \\;group \\;change} \\\\ \\\\ \\epsilon_{treatment \\; effect} &amp;\\sim \\mathcal{N}(0,\\,SD_{TE}) \\\\ \\epsilon_{nontreatment \\; effect} &amp;\\sim \\mathcal{N}(0,\\,SD_{NTE}) \\\\ \\epsilon_{treatment \\;group \\;change} &amp;\\sim \\mathcal{N}(0,\\,SD_{treatment \\;group \\;change}) \\\\ \\epsilon_{control \\;group \\;change} &amp;\\sim \\mathcal{N}(0,\\,SD_{control \\;group \\;change}) \\\\ \\\\ SD_{TE} &amp;= \\sqrt{SD_{treatment \\;group \\;change}^2 - SD_{control \\;group \\;change}^2} \\end{split} \\tag{4.4} \\end{equation} \\] This neat mathematical solution is due to assumption of Gaussian error, assumption that random treatment and non-treatment effects are equal across subjects (see Ergodicity section for more details about this assumption), and the use of squared errors. This is one beneficial property of using squared errors that I alluded to in the section Cross-Validation section. Thus, the estimated parameters of the causal treatment effects in the underlying DGP are are summarized with the following Equation (4.5). This treatment effect is graphically depicted in the Figure 4.4. \\[ \\begin{equation} \\begin{split} Treatment \\; effect &amp;\\sim \\mathcal{N}(Mean_{TE},\\,SD_{TE}) \\\\ \\\\ Mean_{TE} &amp;= Mean_{treatment \\;group \\;change} - Mean_{control \\;group \\;change} \\\\ \\\\ SD_{TE} &amp;= \\sqrt{SD_{treatment \\;group \\;change}^2 - SD_{control \\; group \\; change}^2} \\end{split} \\tag{4.5} \\end{equation} \\] Figure 4.4: Graphical representation of the causal Treatment effect. Green area indicates proportion of higher than SESOI treatment effects, red indicates proportion of negative and lower than SESOI treatment effects, and grey indicates treatment effects that are within SESOI. Mean of treatment effect distribution represents average (or expected) causal effect or systematic treatment effect. SD of treatment effect distribution represents random systematic effect or \\(SD_{TE}\\) Using SESOI, one can also estimate the proportion of lower, equivalent and higher changes (responses) caused by treatment. The estimates of the causal treatment effects, with accompanying proportions of responses are enlisted in the Table 4.12. Table 4.12: Estimates of the causal treatment effects Average causal effect (cm) Random effect (cm) SESOI (cm) Average causal effect to SESOI SESOI to random effect pLower pEquivalent pHigher 7.9 4.57 ±2.5 1.58 1.09 0.01 0.11 0.88 Therefore, we can conclude that plyometric training over three months period, on top of the normal training, cause improvements in vertical jump height (in the sample collected; generalizations beyond sample are discussed in the Statistical inference section). The expected improvement (i.e. average causal effect or systematic effect) is equal to 7.9cm, with 1, 11, and 88% of athletes having lower, trivial and higher improvements. 4.7 Prediction as a complement to causal inference In the previous section, RCT is analyzed using analysis of changes. In this section, I will utilize linear regression model to analyze RCT data. There are multiple ways this could be done (J et al. 2018) and deeper analysis is beyond the scope of this book (see also Frank Harrell post on the use of change scores). The aim of this section is to provide an introduction to model-based and prediction-based RCT analysis, as well as to demonstrate potential uses of PDP+ICE plots as tools for counterfactual analysis. Analysis in the previous section can be represented using simple linear regression model (Equation (4.6)). \\[ \\begin{equation} \\widehat{Change} = \\hat{\\beta_0} + \\hat{\\beta_1}Group \\tag{4.6} \\end{equation} \\] According to Frank Harrell, the use of change scores in the RCT analysis is problematic. Although this model definition (Equation (4.6)) will give us exactly the same results as obtained in the previous section, the use of change scores should be avoided. Thus, look at this example as training vehicle. After this initial discussion, valid model representation will be used. Since Group column is a string, how is Group column represented in the model? Group column needs to be dummy-coded, using 0 for Control and 1 for Treatment (see Table 4.13). Table 4.13: Dummy coding of the Group column to be used in linear regression model Athlete groupTreatment Pre-test (cm) Post-test (cm) Change (cm) Athlete 12 0 42.56 44.29 1.01 Athlete 28 0 47.06 47.98 0.55 Athlete 04 0 44.53 45.13 0.12 Athlete 02 0 49.63 48.86 -0.01 Athlete 08 0 41.11 42.13 -0.45 Athlete 26 0 42.31 41.61 -0.51 Athlete 06 0 45.96 45.70 -0.52 Athlete 14 0 44.51 42.89 -0.63 Athlete 18 0 42.57 42.15 -0.74 Athlete 16 0 37.63 37.83 -0.75 Athlete 22 0 36.52 34.83 -0.97 Athlete 24 0 40.15 39.88 -1.03 Athlete 30 0 39.34 38.34 -1.21 Athlete 20 0 38.94 36.97 -1.72 Athlete 10 0 36.77 34.15 -2.26 Athlete 01 1 37.98 52.86 14.93 Athlete 27 1 44.79 58.50 13.43 Athlete 19 1 46.77 59.76 12.99 Athlete 25 1 38.90 49.58 10.81 Athlete 03 1 41.29 51.41 10.34 Athlete 23 1 48.41 57.57 8.58 Athlete 17 1 44.81 51.41 7.85 Athlete 21 1 37.14 44.95 7.37 Athlete 15 1 46.69 52.73 6.14 Athlete 29 1 42.77 47.38 5.02 Athlete 13 1 49.66 54.11 4.46 Athlete 05 1 37.92 41.63 3.78 Athlete 07 1 41.03 45.41 3.42 Athlete 11 1 45.27 46.72 1.82 Athlete 09 1 47.61 45.62 -1.57 Estimated parameters for this linear model are enlisted in the Table 4.14 Table 4.14: Estimated linear regression parameters for the simple RCT model Intercept groupTreatment -0.61 7.9 Intercept in the Table 4.14 represents the mean Change in the Control group, while \\(\\hat{\\beta_1}\\) (or slope, or GroupTreatment parameter) represents estimated average treatment effect (ATE) or average causal effect, since it represents the difference in the Change means between groups. For a reference please refer to Tables 4.10 and 4.12. Figure 4.5 depicts this model graphically. Figure 4.5: Graphical representation of the simple linear regression model for the vertical jump RCT data Model residuals are depicted on Figure 4.6. Please note the clusters of the data-points which indicate groups (they are color-coded). Figure 4.6: Model residuals using simple linear regression RCT model. Grey band represents SESOI of ±2.5cm. Residuals are color coded; blue are Control group and orange are Treatment group. SD of the residuals for the Control group is equal to 0.83cm and for Treatment group is equal to 4.65cm. Please compare these estimates and estimated parameters from the Table 4.14 with Table 4.10 and Table 4.12. These estimates are identical since the model utilized (Equation (4.6)) is mathematically equivalent to the analysis done in the Example of randomized control trial section. As alluded in the introduction of this section, RCT analysis using change scores should be avoided. Valid way to analyze the RCT in this case is to use Post-test as the outcome, and Pre-test and Group as predictors. This can be easily understood graphically (Figure 4.7). On Figure 4.7 each group (i.e. Control and Treatment) is modeled separately. Figure 4.7: Graphical representation of the valid way to analyze RCT data. Dashed line represent identity line, where Post-test is equal to Pre-test (i.e., the no effect line). The effect of treatment represents vertical distance between the Control and Treatment lines. This is easily grasped since the lines are almost perfectly parallel. If the lines are not parallel, that would imply there is interaction between Group and Pre-test (i.e. individuals with higher Pre-test scores shows higher or lower change). Figure 4.7 also represents ANCOVA (analysis of co-variance) design. Equation (4.7) represent model definition where effects of Group (i.e., Treatment) are estimated by controlling the effects of the Pre-test. \\[ \\begin{equation} \\widehat{Post} = \\hat{\\beta_0} + \\hat{\\beta_1}Group + \\hat{\\beta_2}Pre \\tag{4.7} \\end{equation} \\] Estimated parameters for this linear model are enlisted in the Table 4.15. Please note the similarity with the Table 4.14. Table 4.15: Estimated linear regression parameters for the ANCOVA RCT model (see Equation (4.7)) Intercept groupTreatment Pre-test 3.92 7.85 0.9 Model residuals are depicted on Figure 4.8. Please note the clusters of the data-points which indicate groups. Figure 4.8: Model residuals using ANCOVA RCT model. Grey band represents SESOI of ±2.5cm. Residuals are color coded; blue are Control group and orange are Treatment group. SD of the residuals for the Control group is equal to 1.43cm and for Treatment group is equal to 4.64cm. Please note the similarities with simple RCT model (i.e. using Change score as outcome and Group as predictor). As explained in the Prediction section, this model can be cross-validated. Predictive performance metrics using 10 repeats of 3 folds cross-validation are enlisted in the Table 4.16. Since our RCT data has two groups (i.e. Control and Treatment), cross-validation needs to be stratified. This makes sure that each group has their own cross-validation folds, and that testing data size for each group is proportional to the group size. This avoids scenarios where most training or testing data comes from a single group (which is more probable if one group is larger). Table 4.16: Cross-validated predictive performance metrics for the ANCOVA RCT model metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 -0.04 -0.04 1.55 -3.22 3.18 MAE 2.41 2.36 2.76 2.76 0.67 1.38 3.80 RMSE 3.31 3.22 3.78 3.66 0.97 1.72 5.15 PPER 0.54 0.56 0.49 0.49 0.13 0.33 0.80 SESOI to RMSE 1.51 1.55 1.32 1.49 0.52 0.97 2.91 R-squared 0.75 0.76 0.67 0.65 0.29 -0.53 0.95 MinErr -7.08 -8.66 -9.42 -6.08 2.47 -9.42 -1.84 MaxErr 8.80 8.96 10.99 5.68 3.23 1.34 10.99 MaxAbsErr 8.80 8.96 10.99 7.74 2.34 2.96 10.99 From the Table 4.16 we can conclude, that although we have explained the causal (or treatment) effects, predicting individual Post-test is not practically meaningful since the prediction error is too large (compared to SESOI). The take-home message is that high explanatory power of the model doesn’t doesn’t automatically yield high predictive power (Shmueli 2010). The selection of statistical analysis is thus related to the question asked. In my opinion, it would be insightful to complement causal estimates with prediction estimates. With the example above, we can predict the direction of the effect (using expected systematic change of 7.9cm and proportions of 1, 11, and 88% for lower, trivial and higher change magnitudes), but we are unable to predict individual Post-test (or changes scores) within acceptable practical precision (using SESOI as an anchor). In other words, we know that the effect will be 88% beneficial (i.e. higher than SESOI), but we are not able to predict individual responses. For the sake of completeness, Table 4.17 contains performance metrics for the simple RCT model. Table 4.17: Cross-validated predictive performance metrics for the simple RCT model metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 0.00 0.00 1.24 -2.81 1.72 MAE 2.16 2.14 2.33 2.33 0.61 1.10 3.53 RMSE 3.22 3.18 3.46 3.36 0.84 1.60 4.69 PPER 0.55 0.57 0.53 0.52 0.12 0.37 0.84 SESOI to RMSE 1.55 1.57 1.45 1.61 0.51 1.07 3.12 R-squared 0.60 0.61 0.54 0.59 0.15 0.25 0.88 MinErr -7.64 -8.63 -9.27 -5.67 2.52 -9.27 -0.40 MaxErr 8.86 9.74 9.89 5.79 2.84 1.88 9.89 MaxAbsErr 8.86 9.74 9.89 7.54 1.94 3.85 9.89 4.7.1 Analysis of the individual residuals: responders vs non-responders One particular use of the predictive analysis is in the identification of responders and non-responders to the treatment (Hecksteden et al. 2015, 2018; Hopkins 2015; Swinton et al. 2018). Common approach used in sport science (Will G Hopkins 2004a), that I will name observed outcome approach (further discussed in Measurement error chapter and second part of this book), uses known SESOI and measurement error to estimate probability of lower, equivalent, and higher changes. In RCT, random non-treatment effect can be assumed to be due to measurement error. Figure 4.9 depicts individual adjusted change (by deducting mean Control group change from observed change) with error bars representing smallest detectable change (SDC). SDC is calculated by multiplying Control group change SD by 1.96 (to get upper and lower change levels containing 95% of change distribution). This thus represent our uncertainty in true treatment effect (using Control group as source of information about random effects). Figure 4.9: Responses analysis for the Treatment group. Change scores are adjusted by deducting Control group mean change. Error bars represent smallest detectable change (SDC) that is calculated using SD of the Control group change scores multiplied by 1.96 (to get 95% levels containing 95% of the change distribution). Using this approach, we can classify athletes with high probability of higher change score as responders, those with high probability of equivalent change score as non-responders, and finally those with high probability of lower change score as negative-responders. This approach is useful in figuring out who responded positively or negatively to a particular treatment, but it doesn’t take into account information that might help explain the response (for example someone missing treatment session or having lower or higher treatment dose; see Direct and indirect effect, covariates and then some section). The topic is further discussed in Measurement error chapter and second part of this book. Another approach, that I have termed residuals approach or model-based approach can be used to help identifying outliers to intervention. To explain this approach, let’s plot athletes’ residuals (\\(\\hat{y_i} - y_i\\)) against observed Post-test (\\(y_i\\)) (Figure 4.10) using ANCOVA RCT model. Figure 4.10: Observed Post-test in vertical jump for each athlete in the study. Black dot indicate observed Post-test value; vertical line indicate model prediction; colored bar indicated the residual magnitude compared to defined SESOI (±2.5cm in this example): grey for equivalent magnitude, green for lower magnitude, and red for higher magnitude; horizontal error bar represents cross-validated RMSE (see Table 4.16, RMSE metric, column testing.pooled) and is used to indicate model predictive performance and uncertainty around model prediction graphically If we visualize simple model of RCT, using Change score as outcome and Group as predictor (see Figure 4.5), the predictions for athletes in each group are identical (i.e. the average change). This is depicted in Figure (Figure 4.11). Figure 4.11: Observed Change in vertical jump for each athlete in the study. Black dot indicate observed Change value; vertical line indicate model prediction; colored bar indicated the residual magnitude compared to defined SESOI (±2.5cm in this example): grey for equivalent magnitude, green for lower magnitude, and red for higher magnitude; horizontal error bar represents cross-validated RMSE (see Table 4.17, RMSE metric, column testing.pooled) and is used to indicate model predictive performance and uncertainty around model prediction graphically More complex models (e.g. ANCOVA RCT model in Figure 4.10), like the one utilized in Direct and indirect effect, covariates and then some section will have different predictions for each athlete. Residuals approach uses observed scores and model predictions to indicate individuals who differ more or less than predicted by the model. If this difference between observed and predicted scores (or residual) is bigger than SESOI, this individual is flagged. But before jumping to conclusions, I need to remind you that the predictive performance of this simple model is pretty bad (see Table 4.17). Thus, this type of analysis and visualization should be interpreted given the model performance (which is indicated by horizontal line on the Figures 4.10 and 4.11 which indicates cross-validated pooled testing RMSE; see Table 4.16). I will utilize this method with a better model in the Direct and indirect effect, covariates and then some section that has much lower cross-validated RMSE. What is important to remember with this analysis is that athletes who showed lower or higher observation compared to what was predicted by the model are flagged with red or green color. As opposed to the observed outcome approach, a model-based prediction approach uses ceteris paribus in estimating responders vs. non-responders, or at least providing residuals for such a decision. For example, everything else being equal, based on predictor variables, the expected observation is higher or lower than the model prediction. This indicates that there might be something not-identified by predictive algorithm and thus needs to be flagged for a further analysis. But more about this in the Direct and indirect effect, covariates and then some. Besides analyzing residuals in the training data-set, we can also check how model predicts for each individual within cross-validation using Bias-Variance decomposition (see Bias-Variance decomposition and trade-off section). Figure 4.12 depicts prediction error (decomposed to Bias and Variance) for each athlete using ANCOVA RCT model. Figure 4.12: Bias-variance across 10 times repeated 3-fold cross-validation for each athlete. This analysis can also be utilize to flag certain observations (i.e. athlete in this case) that are troublesome for the predictive model These graphs will make much more sense when more predictive model is applied in the Direct and indirect effect, covariates and then some section. 4.7.2 Counterfactual analysis and Individual Treatment Effects As explained, causal inference and explanatory modeling aim to understand, or at least to quantify the causal (or treatment) effects. This is done by elaborate and transparent control of the confounders and study designs. Predictive modelling on the other hand is interested in providing valid predictions on new or unseen data without assuming underlying DGP by treating it as a black-box. In certain scenarios, when confounders are controlled, predictive modelling can be interpreted causally (Zhao and Hastie 2019). With observational studies, there is always a risk of not controlling all important confounders, but transparency of the causal model is there to be falsified and discussed openly (Gelman and Hennig 2017; Hernan 2002; Hernán 2018, 2016; Hernán and Taubman 2008; Hernán, Hsu, and Healy 2019). Even with the RCTs, there might be uncertainties in applying the findings from the experiments to realistic settings (Gelman 2011; Heckman 2005). PDP and ICE plots are model-agnostic tools for interpreting black-box models that can be used in causal interpretations only after the effort to define the causal structure with domain-specific expertise is taken into account. This approach can be used to estimate, based on the predictive model, counterfactual change and Post-test scores when athletes do not receive treatment (for the Treatment group), or when athletes receive treatment (for the Control group). This way potential outcomes are predicted. To explain how this is done, let’s again consider the Table 4.13. Changing the Group column (in this case groupTreatment) for every observation (athletes in this case) while keeping all other variables (i.e. columns) the same, we can get a glimpse into causal effects of the treatment (assuming the model and the data are valid for such an inference). The prediction model utilized is ANCOVA RCT model. In the Table 4.18, columns Post-test_0 (cm) and Post-test_1 (cm) indicate these counterfactual changes for which we are interested how the model predicts. These predictions are in the columns Post-test_0 (cm) and Post-test_1 (cm). Table 4.18: Counterfactual table used to check how the model predicts when Group changes Athlete groupTreatment Pre-test (cm) Post-test (cm) Change (cm) groupTreatment_0 Post-test_0 (cm) groupTreatment_1 Post-test_1 (cm) Athlete 12 0 42.56 44.29 1.01 0 42.04 1 49.89 Athlete 28 0 47.06 47.98 0.55 0 46.07 1 53.92 Athlete 04 0 44.53 45.13 0.12 0 43.80 1 51.65 Athlete 02 0 49.63 48.86 -0.01 0 48.37 1 56.22 Athlete 08 0 41.11 42.13 -0.45 0 40.74 1 48.59 Athlete 26 0 42.31 41.61 -0.51 0 41.82 1 49.67 Athlete 06 0 45.96 45.70 -0.52 0 45.09 1 52.93 Athlete 14 0 44.51 42.89 -0.63 0 43.79 1 51.64 Athlete 18 0 42.57 42.15 -0.74 0 42.06 1 49.90 Athlete 16 0 37.63 37.83 -0.75 0 37.63 1 45.47 Athlete 22 0 36.52 34.83 -0.97 0 36.63 1 44.48 Athlete 24 0 40.15 39.88 -1.03 0 39.88 1 47.73 Athlete 30 0 39.34 38.34 -1.21 0 39.16 1 47.01 Athlete 20 0 38.94 36.97 -1.72 0 38.80 1 46.65 Athlete 10 0 36.77 34.15 -2.26 0 36.85 1 44.70 Athlete 01 1 37.98 52.86 14.93 0 37.94 1 45.79 Athlete 27 1 44.79 58.50 13.43 0 44.04 1 51.88 Athlete 19 1 46.77 59.76 12.99 0 45.81 1 53.66 Athlete 25 1 38.90 49.58 10.81 0 38.76 1 46.61 Athlete 03 1 41.29 51.41 10.34 0 40.90 1 48.75 Athlete 23 1 48.41 57.57 8.58 0 47.28 1 55.13 Athlete 17 1 44.81 51.41 7.85 0 44.06 1 51.91 Athlete 21 1 37.14 44.95 7.37 0 37.19 1 45.04 Athlete 15 1 46.69 52.73 6.14 0 45.75 1 53.59 Athlete 29 1 42.77 47.38 5.02 0 42.23 1 50.08 Athlete 13 1 49.66 54.11 4.46 0 48.40 1 56.25 Athlete 05 1 37.92 41.63 3.78 0 37.89 1 45.74 Athlete 07 1 41.03 45.41 3.42 0 40.67 1 48.52 Athlete 11 1 45.27 46.72 1.82 0 44.47 1 52.31 Athlete 09 1 47.61 45.62 -1.57 0 46.57 1 54.42 If we depict these changes in the Group for every athlete, we will get the ICE graph. Average of these predictions gives us the PDP graph. Figure 4.13 depicts PDP and ICE for the Group variable. We will get back to this graph in the Direct and indirect effect, covariates and then some section. Figure 4.13: (ref:simple-rct-pdp-ice-caption) Besides PDP and ICE plot, we can also create a counterfactual plot for each athlete. For example, for the athletes in the Treatment group, we are interested how would the predicted Post-test change (given model used) if they are in the Control group and vice versa for the athletes from the Control group. This is done by flipping “Treatment” and “Control” in the Group column and predicting Post-test using the trained model. Figure 4.14 depicts this visually for each athlete. Arrows represents predicted Change when flipping the Group variable. Figure 4.14: Individual counterfactual prediction when the Group changes. This way we can estimate model counterfactual predictions when the treatment changes (i.e. Controls receive treatment, and Treatment doesn’t receive treatment). Arrows thus represent model predicted Individual Treatment Effects (pITE). Arrows are color coded based on the magnitude of the effect using defined SESOI (±2.5cm in this example): grey for equivalent magnitude, green for lower magnitude, and red for higher magnitude. Vertical line indicates observed Post-test scores. This analysis allows us to estimate counterfactual individual causal (treatment) effects (ITE) predicted by the model. These are indicated with the arrows on the Figure 4.14. Mathematically, arrow width is calculates using the Equation (4.8). \\[ \\widehat{ITE_i} = \\widehat{y_{i}^{Group=Treatment}} - \\widehat{y_{i}^{Group=Control}} \\tag{4.8} \\] Since ANCOVA RCT model is used, which predicts average Group effect for every participant, estimated counterfactual ITEs are all the same and are equal to 7.85cm. Table 4.19 contains all individual model predictions using ANCOVA RCT model. Table 4.19: Individual model predictions using ANCOVA RCT model subject group observed predicted residual magnitude counterfactual pITE pITE_magnitude Athlete 12 Control 44.29 42.04 -2.25 Equivalent 49.89 7.85 Higher Athlete 28 Control 47.98 46.07 -1.91 Equivalent 53.92 7.85 Higher Athlete 04 Control 45.13 43.80 -1.33 Equivalent 51.65 7.85 Higher Athlete 02 Control 48.86 48.37 -0.48 Equivalent 56.22 7.85 Higher Athlete 08 Control 42.13 40.74 -1.39 Equivalent 48.59 7.85 Higher Athlete 26 Control 41.61 41.82 0.21 Equivalent 49.67 7.85 Higher Athlete 06 Control 45.70 45.09 -0.61 Equivalent 52.93 7.85 Higher Athlete 14 Control 42.89 43.79 0.90 Equivalent 51.64 7.85 Higher Athlete 18 Control 42.15 42.06 -0.10 Equivalent 49.90 7.85 Higher Athlete 16 Control 37.83 37.63 -0.21 Equivalent 45.47 7.85 Higher Athlete 22 Control 34.83 36.63 1.80 Equivalent 44.48 7.85 Higher Athlete 24 Control 39.88 39.88 0.00 Equivalent 47.73 7.85 Higher Athlete 30 Control 38.34 39.16 0.82 Equivalent 47.01 7.85 Higher Athlete 20 Control 36.97 38.80 1.84 Equivalent 46.65 7.85 Higher Athlete 10 Control 34.15 36.85 2.70 Higher 44.70 7.85 Higher Athlete 01 Treatment 52.86 45.79 -7.08 Lower 37.94 -7.85 Lower Athlete 27 Treatment 58.50 51.88 -6.62 Lower 44.04 -7.85 Lower Athlete 19 Treatment 59.76 53.66 -6.10 Lower 45.81 -7.85 Lower Athlete 25 Treatment 49.58 46.61 -2.98 Lower 38.76 -7.85 Lower Athlete 03 Treatment 51.41 48.75 -2.66 Lower 40.90 -7.85 Lower Athlete 23 Treatment 57.57 55.13 -2.44 Equivalent 47.28 -7.85 Lower Athlete 17 Treatment 51.41 51.91 0.49 Equivalent 44.06 -7.85 Lower Athlete 21 Treatment 44.95 45.04 0.09 Equivalent 37.19 -7.85 Lower Athlete 15 Treatment 52.73 53.59 0.86 Equivalent 45.75 -7.85 Lower Athlete 29 Treatment 47.38 50.08 2.69 Higher 42.23 -7.85 Lower Athlete 13 Treatment 54.11 56.25 2.14 Equivalent 48.40 -7.85 Lower Athlete 05 Treatment 41.63 45.74 4.11 Higher 37.89 -7.85 Lower Athlete 07 Treatment 45.41 48.52 3.11 Higher 40.67 -7.85 Lower Athlete 11 Treatment 46.72 52.31 5.59 Higher 44.47 -7.85 Lower Athlete 09 Treatment 45.62 54.42 8.80 Higher 46.57 -7.85 Lower PDP and ICE plots, as well as individual treatment effects plots (and estimates) can be very valuable tool in visualizing the causal effects, which are appropriate in this case since we are analyzing RCT data. But we need to be very wary when using them with the observational data and giving them causal interpretation. 4.7.3 Direct and indirect effect, covariates and then some In the previous RCT example, we have assumed binary treatment (either plyometric training is done or not), whereas in real life there can be nuances in the treatment, particularly in volume of jumps performed, making the treatment continuous rather than binary variable. This way, we are interested in the effects of number of jumps on the changes in vertical jump height. There could also be hidden variables involved that moderate and mediate the effects of the treatment28. For example, the higher someone jumps in the Pre-test, the lower the change in the Post-test (i.e. it is harder to improve vertical jump height). Or, the stronger someone is in the Pre-test (measured using relative back squat 1RM) the more potentiated the effects of the plyometrics are. All these are needed expert subject-matter knowledge, required to understand the underlying DGP (and thus to avoid introducing bias in causal analyses; see Lübke et al. (2020)). With such causal structure, we do not have direct treatment effect (plyometric –&gt; change in vertical jump) only anymore, but moderated and mediated, or indirect effects estimated using the interactions in the regression models. To explain these concepts, let’s assume that that besides Pre-test and Post-test scores in our RCT study, we have also measured Back squat relative 1RMs since we believed that strength of the individual will moderate the effects of the plyometric treatment. This data is enlisted in the Table 4.20. Squat 1RM in this case represent characteristic of the subject, or a covariate. Additional covariates (not considered here) might include gender, experience, height, weight and so forth. Table 4.20: (ref:simple-rct-counterfactual-group-caption) Athlete Squat 1RM Group Pre-test (cm) Post-test (cm) Change (cm) Athlete 12 1.53 Control 42.56 44.29 1.01 Athlete 28 1.59 Control 47.06 47.98 0.55 Athlete 04 1.49 Control 44.53 45.13 0.12 Athlete 02 1.31 Control 49.63 48.86 -0.01 Athlete 08 1.77 Control 41.11 42.13 -0.45 Athlete 26 1.34 Control 42.31 41.61 -0.51 Athlete 06 1.33 Control 45.96 45.70 -0.52 Athlete 14 2.03 Control 44.51 42.89 -0.63 Athlete 18 1.21 Control 42.57 42.15 -0.74 Athlete 16 1.91 Control 37.63 37.83 -0.75 Athlete 22 1.49 Control 36.52 34.83 -0.97 Athlete 24 1.37 Control 40.15 39.88 -1.03 Athlete 30 1.04 Control 39.34 38.34 -1.21 Athlete 20 1.58 Control 38.94 36.97 -1.72 Athlete 10 1.67 Control 36.77 34.15 -2.26 Athlete 01 2.05 Treatment 37.98 52.86 14.93 Athlete 27 2.05 Treatment 44.79 58.50 13.43 Athlete 19 1.87 Treatment 46.77 59.76 12.99 Athlete 25 1.97 Treatment 38.90 49.58 10.81 Athlete 03 1.79 Treatment 41.29 51.41 10.34 Athlete 23 1.44 Treatment 48.41 57.57 8.58 Athlete 17 1.21 Treatment 44.81 51.41 7.85 Athlete 21 1.49 Treatment 37.14 44.95 7.37 Athlete 15 1.43 Treatment 46.69 52.73 6.14 Athlete 29 1.22 Treatment 42.77 47.38 5.02 Athlete 13 1.18 Treatment 49.66 54.11 4.46 Athlete 05 1.15 Treatment 37.92 41.63 3.78 Athlete 07 1.21 Treatment 41.03 45.41 3.42 Athlete 11 0.86 Treatment 45.27 46.72 1.82 Athlete 09 0.69 Treatment 47.61 45.62 -1.57 Since the individual are randomized into Treatment and Control groups, we expect that there is no difference between Squat 1RM betweem them. Figure 4.15 demonstrates that there is no difference between groups. Figure 4.15: Squat 1RM scores for Control and Treatment groups. Since athletes are randomized, we expect them to be similar (i.e. no selection bias involved) Before modeling this RCT data, let’s check visually the relationship between Pre-test and Change (panel A), Squat 1RM and Change (panel B), and Pre-test and Squat 1RM (panel C) (Figure 4.16). Figure 4.16: Relationship between predictors. A. Relationship between Pre-test and Change scores. Lines indicate linear regression model and are used to indicate relationship and interaction. Colored area represent 95% confidence intervals (see Statistical inference section for more information). If there is no interaction between Pre-test and Change score, the lines would be parallel. As can be seen with the Treatment group, it seems that individuals with higher Pre-test demonstrates lower Change, and vice versa for the Control group. But if we check the confidence interval area, these are due to random chance. B. Relationship between Squat 1RM and Change scores. Visual analysis indicates that individuals in the Treatment group with the higher Squat 1RM demonstrates higher changes after the training intervention. C. Relationship between Squat 1RM and Pre-test predictors. It seems that, in both groups, athletes with higher Squat 1RM scores have lower Pre-test values. Checking confidence interval areas, this relationship is due to random chance. As can be seen from the Figure 4.16, there seems to be some interaction between Squat 1RM and Change. Other panels indicate that there might be some relationship (i.e. interaction), but 95% confidence intervals around the regression lines indicate that these are due to random chance (see Statistical inference chapter for more info; for now you do not need to understand this concept). What does interaction mean? If we check the panel C in the Figure 4.16, interaction refers to change in regression line slopes. If the Group regression lines are parallel, then the distance between them is due to effect of treatment (i.e. direct or main effect of treatment). Since they are not parallel, it means that Squat 1RM and treatment interact: the higher the Squat 1RM for the Treatment group, the higher the Change, while that is not the case for the Control group. Mathematically, interaction is a simple multiplication between two predictors, as cab be seen in the Table 4.21. Table 4.21: RCT data with interaction between Group and Squat 1RM Athlete groupTreatment Squat 1RM groupTreatment:Squat 1RM Pre-test (cm) Change (cm) Athlete 12 0 1.53 0.00 42.56 1.01 Athlete 28 0 1.59 0.00 47.06 0.55 Athlete 04 0 1.49 0.00 44.53 0.12 Athlete 02 0 1.31 0.00 49.63 -0.01 Athlete 08 0 1.77 0.00 41.11 -0.45 Athlete 26 0 1.34 0.00 42.31 -0.51 Athlete 06 0 1.33 0.00 45.96 -0.52 Athlete 14 0 2.03 0.00 44.51 -0.63 Athlete 18 0 1.21 0.00 42.57 -0.74 Athlete 16 0 1.91 0.00 37.63 -0.75 Athlete 22 0 1.49 0.00 36.52 -0.97 Athlete 24 0 1.37 0.00 40.15 -1.03 Athlete 30 0 1.04 0.00 39.34 -1.21 Athlete 20 0 1.58 0.00 38.94 -1.72 Athlete 10 0 1.67 0.00 36.77 -2.26 Athlete 01 1 2.05 2.05 37.98 14.93 Athlete 27 1 2.05 2.05 44.79 13.43 Athlete 19 1 1.87 1.87 46.77 12.99 Athlete 25 1 1.97 1.97 38.90 10.81 Athlete 03 1 1.79 1.79 41.29 10.34 Athlete 23 1 1.44 1.44 48.41 8.58 Athlete 17 1 1.21 1.21 44.81 7.85 Athlete 21 1 1.49 1.49 37.14 7.37 Athlete 15 1 1.43 1.43 46.69 6.14 Athlete 29 1 1.22 1.22 42.77 5.02 Athlete 13 1 1.18 1.18 49.66 4.46 Athlete 05 1 1.15 1.15 37.92 3.78 Athlete 07 1 1.21 1.21 41.03 3.42 Athlete 11 1 0.86 0.86 45.27 1.82 Athlete 09 1 0.69 0.69 47.61 -1.57 Bu as already alluded, the use of Change scores should be avoided. Let’s see this exact graphs, but using Post-test (Figure 4.17) as our variable of interest (i.e. outcome variable). Figure 4.17: Relationship between predictors. A. Relationship between Pre-test and Post-test scores. Lines indicate linear regression model and are used to indicate relationship and interaction. Colored area represent 95% confidence intervals (see Statistical inference section for more information). If there is no interaction between Pre-test and Post-test score, the lines would be parallel. As can be seen on the figure, there doesn’t seem to be interaction involved. B. Relationship between Squat 1RM and Post-test scores. Visual analysis indicates that individuals in the Treatment group with the higher Squat 1RM demonstrates higher Post-test scores after the training intervention. C. Relationship between Squat 1RM and Pre-test predictors. It seems that, in both groups, athletes with higher Squat 1RM scores have lower Pre-test values. Checking confidence interval areas, this relationship is due to random chance. Let’s apply linear regression model to these predictors. The parameters we are going to estimate are enlisted in the linear Equation (4.9). \\[ \\begin{equation} \\widehat{Post} = \\hat{\\beta_0} + \\hat{\\beta_1}Group + \\hat{\\beta_2}Pre + \\hat{\\beta_3}Squat\\:1RM + \\hat{\\beta_4}Group:Squat\\:1RM \\tag{4.9} \\end{equation} \\] Estimated parameters for this linear model with interaction are enlisted in the Table 4.22. Table 4.22: Estimated linear regression parameters for the RCT model with interaction between Group and Squat 1RM Intercept groupTreatment Pre-test Squat 1RM Group:Squat 1RM -5.21 -8.03 1.1 0.26 10.82 If we check the estimated coefficient for the group Treatment in the Table 4.22, which is equal to -8.03, can we conclude that this is the whole effect of treatment (i.e. plyometric treatment)? Luckily no! This coefficient represents direct treatment effect, but there are indirect treatment effects due to Squat 1RM and interaction between Squat 1RM and Group. This also assumes that we have not introduced bias in treatment effect estimation by adjusting (or by not adjusting) for covariates that we should not adjust for (or vice versa; for great applied paper please refer to Lübke et al. (2020)). Figure 4.18 depicts residuals of the RCT model with interactions. Figure 4.18: Residuals of the linear regression RCT model with interaction. Grey band on both panels represents SESOI of ±2.5cm As opposed to the Figures 4.6 and 4.8, we can quickly see that this model have almost all residuals in the SESOI band. Table 4.23 contains cross-validated model performance (using the same 10 repeats of 3 folds cross-validation splits as used in the simple and ANCOVA RCT models). Table 4.23: Cross-validated predictive performance metrics for the RCT model with interaction metric training training.pooled testing.pooled mean SD min max MBE 0.00 0.00 0.03 0.03 0.51 -1.12 0.92 MAE 0.90 0.82 1.17 1.17 0.26 0.50 1.58 RMSE 1.08 1.01 1.42 1.40 0.27 0.67 1.87 PPER 0.97 0.99 0.92 0.88 0.06 0.76 0.99 SESOI to RMSE 4.64 4.95 3.51 3.74 0.94 2.68 7.44 R-squared 0.97 0.98 0.95 0.95 0.03 0.85 0.99 MinErr -2.12 -2.57 -3.00 -1.91 0.64 -3.00 -0.72 MaxErr 1.97 2.20 3.38 2.23 0.64 0.95 3.38 MaxAbsErr 2.12 2.57 3.38 2.48 0.41 1.57 3.38 As expected, predictive performance metrics are now much better. Let’s inspect the athlete’s residuals (Figure 4.19). Figure 4.19: Observed Post-test in vertical jump for each athlete in the study. Black dot indicate observed Post-test; vertical line indicate model prediction; colored bar indicated the residual magnitude compared to defined SESOI (±2.5cm in this example): grey for equivalent magnitude, green for lower magnitude, and red for higher magnitude; horizontal error bar represents cross-validated RMSE (see Table 4.23, RMSE metric, column testing.pooled) and is used to indicate visually model predictive performance and uncertainty around model prediction If we compare residuals from the simple model (Figure 4.11) and ANCOVA RCT model (Figure 4.10), in RCT model with interactions the residuals are much smaller, indicating better prediction. You can also see that model predictions differ, as opposed to simple model that predicted same Change values for all athletes in the Control and Treatment groups. Athletes who are flagged (have residual bigger than SESOI) might need further inspection, since given training data and model, these demonstrate Post-test that is larger/smaller than expected taking their covariates into account (in this case Strength 1RM). Figure 4.20 depicts prediction errors during cross-validation for each athlete. This analysis, together with the Figure 4.19, can be used to detect athletes that are troublesome for the predictive model, and thus bear some further inspection. Figure 4.20: Bias-variance across 10 times repeated 3-fold cross-validation for each athlete. This analysis can also be utilize to flag certain observations (i.e. athlete in this case) that are troublesome for the predictive model Interpreting and understanding direct and indirect effects can be quite difficult, especially when causal structure becomes complex. Visualization techniques such as already mentioned PDP and ICE graphs can be utilized to understand the causal mechanism (Zhao and Hastie 2019; Goldstein et al. 2013). These techniques can also be implemented in observational studies, but with considerable domain knowledge and assumptions needed (Zhao and Hastie 2019). Although predictive analysis, particularly those using black box machine learning models, has been criticized to lack causal interpretation (Hernán, Hsu, and Healy 2019; Pearl and Mackenzie 2018; Pearl 2019), they can complement causal (or explanatory) analysis (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017). Figure 4.21 depicts PDP and ICE plots for Group and Strength 1RM predictors. Figure 4.21: PDP and ICE plots for Group and Strength 1RM predictors using RCT model with interaction Since this is RCT, we can give causal interpretation to PDP and ICE plot, particularly panel B in the Figure 4.21. According to this analysis (given the data and the model), if one increase Squat 1RM, the effect of treatment (i.e. plyometric training) will be higher. This can be further analyzed using each athlete. The question we would like to answer (given the data collected and the model) is “How would particular athlete respond to the treatment if his strength was higher or lower?”. Figure 4.22 shows ICE plots in separate facets. This gives us the ability to analyze (given the data and the model) how would each athlete respond if his or her Squat 1RM changed. Figure 4.22: ICE plots for each athlete Figure 4.23 depicts predicted ITE (i.e. what would happen if Groups flipped and everything else being equal). If we compare Figure 4.23 with Figure 4.14, we can quickly see that the ITEs differ and are not equal for every individual. If we calculate the mean of the ITEs (i.e. arrow lengths), we will get an estimate of ATE. SD of ITEs will give us estimate how variable the treatment effects are, or estimate of VTE. Since these are predicted with the model, I’ve used the terms pATE and pVTE indicating that they are estimated with the predictive model. Figure 4.23: Individual counterfactual prediction when the Group changes. This way we can estimate model counterfactual predictions when the treatment changes (i.e. Controls receive treatment, and Treatment doesn’t receive treatment). Arrows thus represent model predicted Individual Treatment Effects (ITE). Vertical line indicates observed Change Table 4.24 contains comparison between the ANCOVA RCT model (\\(\\widehat{Post} = \\hat{\\beta_0} + \\hat{\\beta_1}Group + \\hat{\\beta_2}Pre\\)) and model with Squat 1RM and interaction term (\\(\\widehat{Post} = \\hat{\\beta_0} + \\hat{\\beta_1}Group + \\hat{\\beta_2}Pre + \\hat{\\beta_3}Squat\\:1RM + \\hat{\\beta_4}Group:Squat\\:1RM\\)) for few estimators that are insightful for the comparison. Table 4.24: Comparison between two models using few estimators and performance metrics estimator ANCOVA.model Interaction.model Training RMSE 3.31 1.08 Training PPER 0.54 0.97 Training SESOI to RMSE 1.51 4.64 Training R-squared 0.75 0.97 CV RMSE 3.78 1.42 CV PPER 0.49 0.92 CV SESOI to RMSE 1.32 3.51 CV R-squared 0.67 0.95 SD Residual (Treatment) 1.43 1.12 SD Residual (Control) 4.64 1.11 pATE (Treatment) 7.85 7.57 pATE (Control) 7.85 8.33 pVTE (Treatment) 0.00 4.62 pVTE (Control) 0.00 2.84 As can be seen in the Table 4.24, linear model with interactions predicts Post-test in the vertical jump height better than the ANCOVA model. This means that the variance in the treatment effect (SD Residual in the Table 4.24) is now explained with additional variables and their interactions29. We are not only interested in prediction, but rather in the underlying causal structure and explaining this model performance when we intervene on variables (i.e. what happens to the target variable when I change X from a to b, while keeping other variables fixed - which is ceteris paribus condition). For this reason, PDP and ICE plots can be used to give causal interpretation of the model (see Figure 4.21). The main critique of Judea Pearl regarding the use of predictive models and techniques is the lack of counterfactual causal interpretation (Pearl and Mackenzie 2018; Pearl 2019), particularly with observational studies. I agree that “the role of expert knowledge is the key difference between prediction and causal inference tasks” (Hernán, Hsu, and Healy 2019, 44) and that “both prediction and causal inference require expert knowledge to formulate the scientific question, but only causal inference requires causal expert knowledge to answer the question” (Hernán, Hsu, and Healy 2019, 45), but this doesn’t negate the need for providing predictive performance, as well as helping in interpreting the model when such data is available (Zhao and Hastie 2019). According to Zhao and Hastie (Zhao and Hastie 2019, 1): “There are three requirements to make causal interpretations: a model with good predictive performance, some domain knowledge in the form of a causal diagram and suitable visualization tools.”. The common denominator among multiple experts is that for causal inference and causal interpretation there is a need for domain knowledge, particularly when RCTs are not available. This domain knowledge can be made more transparent by using DAGs and other structural diagrams, and thus help in falsifying assumptions (Gelman and Hennig 2017; Hernán 2017; Hernan 2002; Hernán, Hsu, and Healy 2019). Adding prediction to explanatory models can be seen as complement, particularly since statistical analysis has been neglecting predictive analysis over explanatory analysis (Breiman 2001; Shmueli 2010; Yarkoni and Westfall 2017). 4.7.4 Model selection Besides applying model with a single interaction term, we can also apply models with more interactions, or quadratic or polynomial terms, or what have you. As we have seen in the Prediction chapter, these models can overfit quite easily. That is why we utilized cross-validation to check for model performance on the unseen data. But what if two or more models of different complexity perform similarly on the unseen data (i.e. when cross-validated)? This is the problem of model selection and comparison. Generally, we want to select the simplest model that gives us reliable predictions. Discussions regarding the model comparison and model selection are beyond the scope of this book, although I will provide few examples in the second part of the book. The model definition should rely on pre-existing beliefs (i.e. subject-matter expert knowledge) around causal structure underlying intervention. If the statistical analysis is done to confirm the structural causal model, then this represents confirmatory analysis. Usually, these studies need to be pre-registered with the exact analysis workflow and assumption defined before data is collected. This is required because in the exploratory analysis one can play with the data, different models can be tried and the model that fits the data best can be selected. Exploratory analysis is useful for generating models and hypothesis for future studies, but also introduces hindsight bias since the model is selected after seeing the data or the results of multiple analyses. Very related is so called p-harking (Hypothesizing After Results are Known) which involves modifying the hypothesis or in this case causal model, after seeing the results, most often with the aim to reach statistical significance (discussed later in the Statistical inference section). In predictive analysis this hindsight bias is reduced by using hold-out sample, cross-validation, and evaluating the final model performance on the data that has not been seen by the model. 4.8 Ergodicity Ergodic process is underlying DGP that is equivalent at between-individual (or inter-individual; or group-based analysis) and within-individual (or intra-individual or simply individual-based analysis) levels. Thus the causal effects identified using between-individual analysis can be applied to understand within-individual causation as well. Non-Ergodic process on the other hand differs between these two levels and effects identified at between-individual level cannot be inferred to within individual level. Few authors have already brought into question the generalizability of group-based research to individual-based prediction and causal inferences (Fisher, Medaglia, and Jeronimus 2018; Glazier and Mehdizadeh 2018; Molenaar 2004; Molenaar and Campbell 2009). Here is an interesting quote from Molenaar (Molenaar and Campbell 2009, 112): “Most research methodology in the behavioral sciences employs inter-individual analyses, which provide information about the state of affairs of the population. However, as shown by classical mathematical-statistical theorems (the ergodic theorems), such analyses do not provide information for, and cannot be applied at, the level of the individual, except on rare occasions when the processes of interest meet certain stringent conditions. When psychological processes violate these conditions, the inter-individual analyses that are now standardly applied have to be replaced by analysis of intra-individual variation in order to obtain valid results.” The individual counterfactual predictions (ITEs) and ICE plots thus rely on ergodicity, which represents big assumption. This means that we should be cautious when generalizing model predictions and causal explanations from group-based level to individual-level. Data analysis at the individual level (i.e. collecting multiple data point for individuals) and identifying individual causal effects and predictions might be the step forward, but even with such an approach we are retrodicting under ceteris paribus conditions and we might not be able to predict future responses. For example, if we have collected responses to various interventions for a particular individual, we might not be able to estimate with certainty how he or she is going to respond to a familiar treatment in the future, since such a prediction relies on stationarity of the parameters or the underlying DGP (Molenaar and Campbell 2009). But this doesn’t imply that all our efforts are useless. We just need to accept the uncertainty and the assumptions involved. For example, for completely novel subject, the best response estimate or prediction estimate is the expected response calculated by using the group-based analysis (i.e. average treatment effect). This represents the most likely response or prediction. But on top of providing these distribution-based or group-based estimates, one can provide expected uncertainties showing individual-based or response proportions as anchor-based (magnitude-based) estimates (Estrada, Ferrer, and Pardo 2019; Norman et al. 2001). It is important to note that these usually estimate the same information (Estrada, Ferrer, and Pardo 2019; Norman et al. 2001); e.g. the higher the Cohen's d the higher the proportion of higher responses. Thus reporting magnitude-based proportions as uncertainties together with expected responses using average-based approach at least help in communicating uncertainties much better than reporting solely average-based estimates. When warranted with the research question, researchers should also report predictive performances on unseen data, as well as underlying assumption using graphical models such as DAGs. It might be the best to conclude the section on causal inference with the quote from Andrew Gelman’s paper (Gelman 2011, 965): “Casual inference will always be a challenge, partly because our psychological intuitions do not always match how the world works. We like to think of simple causal stories, but in the social world, causes and effects are complex. We—in the scientific community—still have not come to any agreement on how best to form consensus on causal questions: there is a general acceptance that experimentation is a gold standard, but it is not at clear how much experimentation should be done, to what extent we can trust inference from observational data, and to what extent experimentation should be more fully incorporated into daily practice (as suggested by some in the “evidence-based medicine” movement).\" References "],
["statistical-inference.html", "Chapter 5 Statistical inference 5.1 Two kinds of uncertainty, two kinds of probability, two kinds of statistical inference", " Chapter 5 Statistical inference Why did estimates diverge from the true parameters in the previous examples? Why did estimated treatment effect differed from the true treatment effect we used to generate the data? This brings us to the important concept in statistical analysis: a difference between the true parameter in the population and the sample estimate. Population refers to all members of specified group, whereas sample contains only a part, or a subset of a population from which it is taken. To generate the data in simulations, we have assumed DGP in the population, out of which a single or multiples samples were drawn. The mathematical notation to differentiate true population parameters and sample estimates, or statistics involve using Greek letters to represent true population parameters, and Latin letters to represent sample estimates. For example, \\(\\mu\\) stands for true population mean, while \\(\\bar{x}\\) stand for sample mean, where bar symbol (\" \\(\\bar{}\\) “) indicates mean. When it comes to standard deviation, \\(\\sigma\\) stands for true population parameter, while SD stands for sample estimate. Sometimes the hat symbol (” \\(\\hat{}\\) \") is used to denote estimator. For example, \\(\\hat{y_i}\\) would be a model estimate or prediction of the observed \\(y_i\\). The difference between the true population parameter and the sample estimate (assuming correctly identified and represented DGP among other issues such as the use of non-biased estimators for example) is due to the sampling error, that we will covered shortly. In the simulations, we are certain about the true population parameters, but in real life we are uncertain about the true parameters and we need to somehow quantify this uncertainty. This is the objective of statistical inference. In other words, with statistical inference we want to generalize from sample to a population, while taking into account uncertainties of the estimates. 5.1 Two kinds of uncertainty, two kinds of probability, two kinds of statistical inference In the previous RCT example, by using the Pre-Test score and Group variables, we have estimated predictive performance of the model using Post-test as the target variable. Error (or uncertainty) around individual Post-test score was much bigger than SESOI, which made individual prediction practically useless. This prediction uncertainty decreased as new variables were introduced to the prediction model. With model involving Squat 1RM variable, we have achieved much better predictive performance. This type of uncertainty can be called epistemic uncertainty (O’Hagan 2004), which is a result of lack of knowledge or incomplete information. In the Prediction section of this book, I have utilized irreducible error in the DGP to represent stochastic component or the random error. Due to this random error, scores will differ from sample to sample. This type of uncertainty can be called aleatory uncertainty (O’Hagan 2004), which results due intrinsic randomness. Another flagship example of the aleatory uncertainty is tossing a dice, drawing a card from a shuffled pack, or random sampling that produces the sampling error. You can argue of course that aleatory uncertainty is ultimately epistemic uncertainty. For example, if I knew infinite details about the dice tossing, I would be able to predict exactly the number that will be landed. Philosophers have been arguing about these issues for ages, and it is not in the domain of this book to delve deeper into the matters of uncertainty. The theory of statistical inference and statistics in general rests on describing uncertainties by using probability. Since there are two kinds of uncertainty, there are two kinds of probabilities and their meaning. Aleatory uncertainties, like tossing a dice or random sampling, are described using long-frequency definition of probability. For example, it can happen that I toss six for 4 times in a row, but in the long-run, which means infinite number of times, probability of tossing a six is equal to 1/6, or probability of 0.166, or 16.6% (assuming a fair dice of course). Probability viewed from this perspective represents long-frequency number of occurrences of the event of interest, divided by number of total events. For example, if I toss a dice for 1000 times, and if I get number six for 170 times, the probability of tossing a six is equal to 170 / 1000, or 17%. With epistemic uncertainty, probability of a proposition simply represents a degree-of-belief in the truth of that proposition (O’Hagan 2004). The degree-of-belief interpretation of probability is referred to as subjective probability or personal probability, while long-frequency interpretation of probability is referred to as objective probability. There are two major schools of statistical inference leaning either on long-frequency interpretation of probability, called frequentist, or leaning on degree-of-belief interpretation of probability, called Bayesian. There are of course many nuances and other schools of statistical inference (Dienes 2008; Efron and Hastie 2016; Gelman and Hennig 2017) which are beyond the scope of this book. The additional approach to inference that will be considered in this book as a preferred approach is the bootstrap (Efron and Hastie 2016; Hesterberg 2015; Guillaume A Rousselet, Pernet, and Wilcox 2019b, 2019a). But more about it later. To better explain the differences between the frequentist and Bayesian approach to statistical inference I am going to use known male mean height and SD in the population. The population parameter of interest is the population mean (Greek \\(\\mu\\)) estimated using the sample mean (\\(\\bar{x}\\)). References "],
["frequentist-perspective.html", "Chapter 6 Frequentist perspective 6.1 Null-Hypothesis Significance Testing 6.2 Statistical Power 6.3 New Statistics: Confidence Intervals and Estimation 6.4 Minimum Effect Tests 6.5 Magnitude Based Inference", " Chapter 6 Frequentist perspective As already stated, using simulations is outstanding teaching tool (Carsey and Harden 2013; Hopkins 2007), and also very useful for understanding the frequentist inference as well. Figure 6.1 (Panels A and B) depicts hypothetical male population where mean height \\(\\mu\\) is 177.8cm and SD (\\(\\sigma\\)) is 10.16cm. From this population we are randomly drawing N=5 (left side panels on Figure 6.1) and N=20 (right side panels on Figure 6.1) individuals for which we estimate the mean height. Individuals are represented as blue dots (Panels C and D on Figure 6.1), whereas estimated mean height is depicted as orange dot. Now imagine we repeat this sampling 50 times, calculate the mean for every sample, and then draw the distribution of the sampled means (Panels E an F on Figure 6.1). This distribution is called sampling distribution of the sample mean and the SD of this distribution is referred to as standard error or sampling error. Since our estimate of interest is the mean, standard deviation of the sampling distribution of the mean is called standard error of the mean (SEM). On Panels E and F in the Figure 6.1, mean of the sampling means is indicated by a black dot, and error bars represent SEM. Figure 6.1: Sampling distribution of the mean. A and B. Distribution of the height in the population. From this population we draw samples. C and D. 50 sample are taken with N=5 (panel C) and N=20 (panel D) observations. Each observation is indicated by a blue dot. Calculated mean, as a parameter of interest, is indicated by an orange dot. E and F. Distribution of collected sample means from panels C and D. This distribution of the sample means is narrower, indicating higher precision when higher N is used. Black dot indicates the mean of the sample means, with error bars indicating SD of sample means. Orange line represents hypothetical distribution of the sample means when number of samples is infinitely large As can be seen from the Figure 6.1, the sampling distribution of the mean looks like normal distribution. If the number of samples reach very large number or infinity, the sampling distribution of the mean will eventually be distributed with the SEM equal to (Equation (6.1)): \\[ \\begin{equation} SEM = \\frac{\\sigma}{\\sqrt{N}} \\tag{6.1} \\end{equation} \\] This theoretical distribution is overlaid on the acquired sampling distribution from 50 samples in the Figure 6.1 (Panels E and F). Since the true \\(\\sigma\\) is not known, sample SD is utilized instead, in order to estimate the true SEM (Equation (6.2)): \\[ \\begin{equation} \\hat{SEM} = \\frac{SD}{\\sqrt{N}} \\tag{6.2} \\end{equation} \\] The take-home point is that the larger the sample, the smaller the standard error, which is visually seen as narrower sampling distribution (compare N=5 and N=20 sampling distributions on Figure 6.1). Another conclusion regarding frequentist inference, is that calculated probabilities revolve around sampling distribution of the sample mean and other long-frequency sampling distributions. Everything else are details. But as the saying goes, the devil is in the details. Sampling distributions and equations for standard errors are derived algebraically for most estimators (e.g. mean, SD, Cohen's d), but for some estimators it might be hard to derive them, so other solutions do exist (like bootstrapping which will be covered in Bootstrap section). For example, sampling distribution of the change score proportions can be very difficult to be derived algebraically (Swinton et al. 2018). For some estimators, mean of the long-frequency samples is different than the true population value, thus these estimators are termed biased estimators. One example of the biased estimator would be SD of the sample where we divide with \\(N\\), instead of \\(N-1\\). Estimators that have the mean of the long-frequency sample estimate equal to the true population parameter are called unbiased estimators. Although the sampling distribution of the mean looks like a normal distribution, it actually belongs to the Student’s t distribution, which has fatter tails for smaller samples (Figure 6.2). Besides mean and SD, Student’s t distribution also has degrees of freedom (DF) parameters, which is equal to N-1 for the sample mean. Normal distribution is equal to Student’s t distribution when DF is infinitely large. Figure 6.2: Student’s t-distribution Since Student’s t distribution is fatter on the tails, critical values that cover 90, 95, and 99% of distribution mass are different than the commonly used ones for the normal distribution. Table 6.1 contains critical values for different DF. For example, 90% of the sampling distribution will be inside the \\(\\bar{x} \\pm 1.64 \\times SEM\\) interval for the normal distribution, but for Student t with DF=5, 90% of the sampling distribution will be inside the \\(\\bar{x} \\pm 2.02 \\times SEM\\) interval. Table 6.1: Critical values for Student’s t distribution with different degrees of freedom 50% 90% 95% 99% 99.9% DF=5 0.73 2.02 2.57 4.03 6.87 DF=10 0.70 1.81 2.23 3.17 4.59 DF=20 0.69 1.72 2.09 2.85 3.85 DF=30 0.68 1.70 2.04 2.75 3.65 DF=50 0.68 1.68 2.01 2.68 3.50 (Normal) 0.67 1.64 1.96 2.58 3.29 6.1 Null-Hypothesis Significance Testing There are two approaches to statistical inference, be it frequentist or Bayesian: hypothesis testing and estimation (Cumming 2014; Kruschke and Liddell 2018b). I will focus on the former, although latter will be covered as well. For the frequentist inference, mathematics behind both of these are the same and both involve standard errors. Null-hypothesis significance testing (NHST) is still one of the most dominant approaches to statistical inference, although heavily criticized (for example see (Cumming 2014; Kruschke and Liddell 2018b)). In Figure 6.1, we have sampled from the known population, but in practice we don’t know the true parameter values in the population, nor we are able to collect data from the whole population (unless it is a small one, but there is no need for statistical inference then, since the whole population is represented in our sample). Thus, we need to use sampled data to make inferences about the population. With NHST we want to test sample parameter or estimator (i.e. mean in this case) against the null-hypothesis (\\(H_{0}\\)). Null-hypothesis usually takes the no effect value, but it can take any value of interest for the researcher. Although this sounds mouthful, a simple example will make it clearer. Imagine that we do know the true population mean height, but in one particular region the mean height of the sample differs from the known population mean. If we assume that this region belongs to the same population, then we want to test to see how likely are we to sample mean we have acquired or more extreme. Figure 6.3 contains known population mean height as the null-hypothesis and estimated probabilities of observing sample mean of 180, 182.5, and 185cm (or +2.2, +4.7, +7.2cm difference) or larger for sample sizes N=5, N=10 and N=20. Panel A on Figure 6.3 depicts one-sided approach for estimating probability of observing these sample mean heights. One-sided approach is used when we are certain about the direction of the effect. Two-sided approach, on the other hand, calculates probability for the effect of the unknown direction. In this example that would be sample mean height difference of ±2.2, ±4.7, ±7.2cm or larger. Two-sided approach is depicted on the Panel B (Figure 6.3). Figure 6.3: Null-hypothesis significance testing. Assuming null-hypothesis (true parameter value, or parameter value in the population, in this case mean or \\(\\mu\\)) is true, probability of observing sample parameter of a given magnitude or larger, is estimated by calculating proportion of sampling distribution that is over sample parameter value. The larger the sample size, the smaller the width of the sampling distribution. A. One-sided approach is used when we are certain about the direction of the effect. B. Two-sided approach is used when expected direction of the effect is unknown The calculated probability of observing sample mean or larger, given null-hypothesis, is called p-value. In other words, p-value is the probability of observing data (in this case sample mean) given the null hypothesis (Equation (6.3)) \\[ \\begin{equation} p \\; value = p(Data | H{0}) \\tag{6.3} \\end{equation} \\] It is easy to interpret p-values as “probability of the null hypothesis (given data)” (\\(p(H{0}|Data)\\)), but that is erroneous. This is Bayesian interpretation (also called inverse probability) which is quite common, even for the experienced researchers. Unfortunately, p-values cannot be interpreted in such way, but rather as a “probability of data given null hypothesis”. As you can see from the Figure 6.3), for the same difference in sample mean height, different sample sizes will produce different p-values. This is because sampling distribution of the mean will be narrower (i.e. smaller SEM) as the sample size increase. In other words, for the same effect (in this case sample mean), p-value will be smaller as the sample size gets bigger. It is thus important to realize that p-values don’t tell us anything about the magnitude of the effect (in this case the difference between the sample mean and the known population mean). The procedures of acquiring p-values are called statistical tests. With the example above, we are using one variation of the Student t test, where we are calculating the test value t using the following Equation (6.4). \\[ \\begin{equation} \\begin{split} t &amp;= \\frac{\\bar{x} -\\mu}{SEM} \\\\ \\\\ t &amp;= \\frac{\\bar{x} -\\mu}{\\frac{SD}{\\sqrt{N}}} \\end{split} \\tag{6.4} \\end{equation} \\] P-value is then estimated by using the calculated t value and appropriate Student’s t distribution (see Figure 6.2) to calculate the surface area over a particular value of t. This is usually done in the statistical program, or by using tables similar to Table 6.1 . Once the p-value is estimated, we need to decide whether to reject the null-hypothesis or not. In order to do that, we need to define the error we are willing to accept. This error is called alpha (Greek \\(\\alpha\\)) or Type I error and refers to making an error of rejecting the null-hypothesis when null-hypothesis is true. Out of sheer convenience, alpha is set to 0.1 (10% error), 0.05 (5% error) or 0.01 (1% error). If p-value is smaller than alpha, we will reject the null-hypothesis and state that the effect has statistical significance. The statistical significance has bad wording since it does not imply magnitude of the effect, only that the sample data come from a different population assumed by null-hypothesis. Take the following example. Let’s assume we have sample size of N=20 where sample mean is equal to 185cm. Using the known population mean (177.8cm) and SD (10.16cm), we get that \\(t=3.17\\). Using two-sided test and alpha=0.05, can we reject the null-hypothesis? In order to do this we can refer to Table 6.1 and check that for DF=20 (which is not exact, but it will serve the purpose), 95% of sampling distribution (which leaves 2.5% on each tail which is equal to 5% alpha) will be within ±2.08. Since calculated \\(t=3.17\\) is over ±2.08, we can reject the null-hypothesis with alpha=0.05. Figure 6.3 (Panel B) depicts the exact p-value for this example, which is equal to p=0.005. Statistical software can calculate exact p-values, but before these were available, tables and procedure just describes were used instead. 6.2 Statistical Power There is other type of error that we can commit: Type II error or beta (Greek \\(\\beta\\)). In order to understand Type II error, we need to assume alternate hypothesis or \\(H{a}\\). Type II error refers to the error we make when we reject the alternate-hypothesis when alternate-hypothesis is true. Type I and Type II error are inversely related - the more we are willing to make Type I errors, the less likely we are going to make Type II errors, and vice versa (Table 6.2). Table 6.2: Type I and Type II errors True \\(H_{0}\\) True \\(H_{a}\\) Rejected \\(H_{0}\\) Type I Rejected \\(H_{a}\\) Type II It is important to keep in mind that with NHST, we never accept any hypothesis, we either reject it or not. For example, we never say “null-hypothesis is accepted (p=0.23)”, but rather “null-hypothesis is not rejected (p=0.23)”. Assuming that alternate-hypothesis is true, probability of rejecting the null-hypothesis is equal to \\(1-\\beta\\). This is called statistical power and depends on the magnitude of the effect we are aiming to detect (or not-to-reject to correct myself). Figure 6.4 depicts multiple examples of one-sided and two-sided statistical power calculations given the known alpha of 0.05 and null-hypothesis for difference in sample mean height of ±2.5, ±5, and ±7.5cm (+2.5, +5, and +7.5cm for one sided test) for N=5, N=10 and N=20. Figure 6.4: Statistical power. Statistical power is probability of detecting an effect of particular magnitude or larger. Visually, statistical power is dark blue surface and represents probability of rejecting the null-hypothesis given that the alternative hypothesis is true. A. One-sided approach. B. Two-sided approach As can be seen from the Figure 6.4, the higher the magnitude of the effect (in this case difference in height means), the more likely we are to detect it (by rejecting null-hypothesis). Statistical power is mostly used when planning the studies to estimate sample size needed to detect effects of magnitude of interest (usually using known or observed effect from previous studies, or even SESOI). For example, question such as “How big of a sample do I need to detect 2.5cm difference with 80% power, alpha 0.05 and expected sample SD of 10cm?” is answered by using statistical power analysis. Statistical power, or sample size for needed statistical power can be easily calculated for simple analysis, but for some more elaborate analyses simulations are needed. The frequentist approach to statistical inference is all about maintaining accepted error rates, particularly Type I, for both tests and estimates. This can be particularly difficult when family-wise error rates need to be controlled, and these can emerge when multiple NHST are done. Some techniques, called p-harking, can also introduce bias in the error rates by fishing for p-values (e.g. collecting samples until significant results are found). These topics are beyond the scope of this paper, but one of the reasons why some researchers prefer Bayesian perspective. 6.3 New Statistics: Confidence Intervals and Estimation Rather than performing NHST, uncertainty of the estimated parameter can be represented with the confidence interval (CI). CIs are usually pretty hard to explain and non-intuitive since they do not carry any distributional information.30 It is thus better to refer to CIs as compatibility intervals (Gelman and Greenland 2019), since, let’s say 95% confidence interval contains all the hypotheses parameter values that would not be rejected by p&lt;0.05 NHST (Kruschke and Liddell 2018b). This implies that, in the long-run (when sampling is repeated infinite number of times), 95% confidence interval will capture true parameter value 95% of the time. Assuming N=20 samples come from the population where the true mean height is equal to 177.8cm and SD is equal to 10.16cm, calculated 95% CIs around sample parameter estimate (in this case sample mean), in the long run, will capture true population parameter 95% of the time. Figure 6.5 depicts first 100 samples out of total of 1,000 taken from the population with calculated 95% CIs. CIs that missed the true population parameter value are depicted in red. Table 6.3 contain the summary for this simulation. If this simulation is repeated for many more times, CIs will capture true population parameter 95% of the time, or in other words, have Type I error of 5%. Figure 6.5: \\(95\\%\\) confidence intervals for the sample mean, estimated for 100 samples (N=20 observations) drawn from population of known parameters (population mean is indicated by vertical line). In the long-run, \\(95\\%\\) confidence intervals will span the true population value \\(95\\%\\) of the time. Confidence intervals that didn’t span the true population parameter value are colored in red Table 6.3: Type I errors in 1000 samples Sample Correct % Type I Errors % 1000 95.9 4.1 Similarly to different alphas, CIs can use different levels of confidence, usually 90%, 95%, 99%. As already mentioned, mathematics behind the confidence intervals is equal to mathematics behind NHST. In order to calculate two-sided CIs for the sample mean, the Equation (6.5) is used: \\[ \\begin{equation} CI = \\bar{x} \\pm t_{crit} \\times \\widehat{SEM} \\tag{6.5} \\end{equation} \\] \\(T_{crit}\\) can be found in the Table 6.1, where for 95% two-sided confidence and DF=20, is equal to 2.086. Using the example of observed sample mean of 185cm, known sample SD (10.16cm) and N=20 (which is equal to DF=19, but for the sake of example DF=20 will be used), calculated 95% confidence interval is equal to 180.26 to 189.74cm. From the compatibility interpretation standpoint, this CI means that the hypotheses with values ranging from 180.26 to 189.74cm, will not be rejected with alpha=0.05. Confidence intervals are great solution for visualizing uncertainties around estimates. Figure 6.6 depicts already used example in Figure 6.3 (two-sided and one-sided p-values), but this time 95% CIs around the sample means are depicted. Please note that in scenarios where 95% CIs cross the null-hypothesis, NHST will yield p&gt;0.05. This means that null-hypothesis is not rejected and results are not statistically significant. CIs can be thus used to visually inspect and conclude whether or not the null-hypothesis would be rejected or not if NHST is performed. Figure 6.6: \\(95\\%\\) Confidence intervals for sample mean. Null-hypothesis of the population parameter value is indicated by vertical dashed line. If the \\(95\\%\\) confidence interval doesn’t touch or cross the null-hypothesis parameter value, p-value is less than 0.05 and effect is statistically significant (given alpha of 0.05). A. One-sided confidence intervals. B. Two-sided confidence intervals 6.4 Minimum Effect Tests NHST doesn’t tell us anything about the magnitudes of the effect. Just because the test is statistically significant (p&lt;0.05), it’s doesn’t imply practically meaningful effect. Rather than using null-hypothesis of no effect, we can perform numerous one-sided NHSTs by using SESOI thresholds to infer practical significance. These are called minimum effect tests (METs) and can distinguish between 6 different conclusions: lower, not-higher, equivalent, not-lower, higher, and equivocal effect. Figure 6.7 depicts how SESOI and CIs can be used to distinguish between these 6 magnitude-based conclusions (Barker and R. Schofield 2008; Sainani et al. 2019). Figure 6.7: Inference about magnitudes of effects. Error bars represent confidence intervals around estimate of interest. Adapted and modified from Barker and R. Schofield (2008); Sainani et al. (2019) 6.4.1 Individual vs. Parameter SESOI So far we have used SESOI to infer practically significant differences or changes at the individual level. For example, answering what is the practically meaningful difference in height, SESOI was used to calculate proportions and chances of observing individuals with lower, equivalent and higher magnitudes of effects. In prediction tasks, SESOI was used to infer practically meaningful prediction error. This helped answering the question regarding whether the individual predictions are within practically equivalent region. However, apart from using SESOI to infer individual change, difference, and prediction magnitudes, SESOI can also be used to evaluate statistics or parameters against practically significant anchor. For example, in Equation (2.14), we have divided mean group difference with SESOI to create magnitude-based estimator. But here, we assumed that the same magnitude used at the individual level is of equal practical importance at the group level (i.e. aggregate level using the mean estimator). For example, individual change of ±5kg might be practically important at the level of the individual, but not at the level of the group (i.e. parameter), and vice versa. Usually, they are assumed to be the same (see also Ergodicity section). Since sample mean difference is the estimator of the parameter in the population we are interested in estimating, we tend to use SESOI to give practical anchors for parameters as well. It could be argued that different terms should be used for the parameter SESOI (particularly for standardized estimators such as Cohen's d) versus individual SESOI. For example, we can use ROPE term for parameters (Kruschke and Liddell 2018a, 2018b), and SESOI for individual-level magnitude inferences. For practical purposes they are considered equal, although I believe further discussion about this distinction is warranted, but outside the scope of this book. 6.4.2 Two one-sided tests of equivalence Besides testing again null-hypothesis of no-effect, we can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of the smallest effect size of interest (SESOI) (Lakens, Scheel, and Isager 2018; Lakens 2017). TOST involves using two one-sided NHSTs assuming parameter values at SESOI thresholds (Figure 6.8). Since the TOST produces two p-values, the larger of the two is reported. A conclusion of statistical equivalence is warranted when the larger of the two p-values is smaller than alpha (Lakens 2017). From estimation perspective, statistical equivalence at the level of alpha=0.05 can be inferred if the 90% (90% not 95%; it is not a typo) CI falls completely within SESOI band. Figure 6.8: Equivalence test using two one-sided tests (TOST). Equivalence test involves two NHSTs at SESOI thresholds and calculates two one-sided p-values, out of which a larger one is reported as result. Error bars represent 90% confidence intervals. 6.4.3 Superiority and Non-Inferiority Two same NHSTs at SESOI thresholds are utilized to test superiority and non-inferiority of the effects. In other words, we want to conclude whether the effect is higher and/or not-lower than SESOI. To achieve this, two one-sided NHSTs are performed to estimate the probability of observing effect in the positive direction (Figure 6.9). Figure 6.9: Superiority and Non-Inferiority tests. Similar to equivalence test using TOST procedure, superiority and non-inferiority tests involve two one-sided NHSTs at SESOI thresholds in the positive direction. Error bars represent 90% confidence intervals. 6.4.4 Inferiority and Non-Superiority To test the inferiority and non-superiority of the effects, two one-sided NHSTs are performed to estimate the probability of observing effect in the negative direction (Figure 6.10). Figure 6.10: Inferiority and Non-Superiority tests. Similar to equivalence test using TOST procedure, inferiority and non-superiority tests involve two one-sided NHSTs at SESOI thresholds in the negative direction. Error bars represent 90% confidence intervals. 6.4.5 Inference from METs The aforementioned METs provide five p-values: for lower (inferiority), not-higher (non-superiority), equivalent (equivalence), not-lower (non-inferiority), and higher (superiority) effect magnitude. These p-values can be used to make magnitude-based inferences about the effects. Figure 6.11 depicts already used examples to calculate p-values from METs and the final inference on the magnitude of the effect (see Figure 6.7). Figure 6.11: Minimum Effect Test results. Error bars represent 90% confidence intervals. 6.5 Magnitude Based Inference Batterham and Hopkins (Batterham and Hopkins 2006; Hopkins et al. 2009) proposed novel approach in making meaningful inference about magnitudes, called magnitude based inference (MBI). MBI has been recently criticized (Barker and R. Schofield 2008; Borg et al. 2018; Curran-Everett 2018; Hopkins and Batterham 2018; Nevill et al. 2018; Sainani et al. 2019; Sainani 2018; Welsh and Knight 2015) for interpreting CIs as Bayesian credible intervals and for not controlling Type I and Type II errors. As explained, CIs doesn’t contain any probability distribution information about the true parameter. Although CIs, Bayesian credible intervals (with flat or non-informative prior), and bootstrap CIs tend to converge to the approximately same values for very simple tests (such as t-test for the sample mean), interpreting CIs established using frequentist approach as Bayesian credible intervals is not valid approach to statistical inference (Sainani et al. 2019). Figure 6.12 depicts Bayesian interpretation of the confidence intervals used in MBI. Using MBI as a simple descriptive approach to interpret CIs can be rationalized, but making inferences from estimated probabilities is not recommended (Caldwell and Cheuvront 2019). If frequentist approaches are used for magnitude-based statistical inference, METs should be used instead. Figure 6.12: Magnitude-based inference use inappropriate Bayesian interpretation of the confidence intervals to calculate lower, equivalent, and higher probabilities of the effect. Vertical dashed lines represent SESOI. thresholds. Error bars represent 90% confidence intervals. There are numerous problems with frequentist inference (Kruschke and Liddell 2018b, 2018a). The results are not intuitive and are usually erroneously interpreted from the Bayesian perspective. Error rates need to be controlled for and adjusted when multiple comparisons are made, or when different stopping techniques are used, sampling distributions are unknown for some estimators and cannot be derived algebraically. Various assumptions such as assumptions of normality, non-colinearity and others, need to be made and tested for, and for more complex models, such as hierarchical models, p-values and CIs are only approximated (Kruschke and Liddell 2018a, 2018b). It is beyond this short chapter to delve into more details, regarding the frequentist approach to statistical inference, and readers are directed to references provided in this section. References "],
["bayesian-perspective.html", "Chapter 7 Bayesian perspective 7.1 Grid approximation 7.2 Priors 7.3 Likelihood function 7.4 Posterior probability 7.5 Adding more possibilities 7.6 Different prior 7.7 More data 7.8 Summarizing prior and posterior distributions with MAP and HDI 7.9 Comparison to NHST Type I errors", " Chapter 7 Bayesian perspective Bayesian inference is reallocation of plausibility (credibility) across possibilities (Kruschke and Liddell 2018a, 2018b; McElreath 2015). Kruschke and Liddell (Kruschke and Liddell 2018a, 156) wrote in their paper as follows: “The main idea of Bayesian analysis is simple and intuitive. There are some data to be explained, and we have a set of candidate explanations. Before knowing the new data, the candidate explanations have some prior credibility of being the best explanation. Then, when given the new data, we shift credibility toward the candidate explanations that better account for the data, and we shift credibility away from the candidate explanations that do not account well for the data. A mathematically compelling way to reallocate credibility is called Bayes’ rule. The rest is just details.” The aim of this section is to provide the gross overview of the Bayesian inference using grid approximation method (McElreath 2015), which is excellent teaching tool, but very limited for Bayesian analysis beyond simple mean and simple linear regression inference. More elaborate discussion on the Bayesian methods, such as Bayes factor, priors selection, model comparison, and Markov Chain Monte Carlo sampling is beyond the scope of this book. Interested readers are directed to the references provided and suggested readings at the end of this book. 7.1 Grid approximation To showcase the rationale behind Bayesian inference let’s consider the same example used in Frequentist perspective chapter - the male height. The question we are asking is, given our data, what is the true average male height (mean; mu or Greek letter \\(\\mu\\)) and SD (sigma or Greek letter \\(\\sigma\\)). You can immediately notice the difference in the question asked. In the frequentist approach we are asking “What is the probability of observing the data31 (estimator, like mean or Cohen's d), given the null hypothesis?” True average male height and true SD represents parameters, and with Bayesian inference we want to relocate credibility across possibilities of those parameters (given the data collected). For the sake of this simplistic example, let’s consider the following possibilities for the mean height: 170, 175, 180cm, and for SD: 9, 10, 11cm. This gives us the following grid (Table 7.1), which combines all possibilities in the parameters, hence the name grid approximation. Since we have three possibilities for each parameter, the grid consists of 9 total possibilities. Table 7.1: Parameter possibilities mu sigma 170 9 175 9 180 9 170 10 175 10 180 10 170 11 175 11 180 11 7.2 Priors Before analyzing the collected data sample, with Bayesian inference we want to state the prior beliefs in parameter possibilities. For example, I might state that from previous data collected, I believe that the mean height is around 170 and 180cm, with a peak at 175cm (e.g. approximating normal curve). We will come back to topic of prior later on, but for now lets use uniform or vague prior, which assigns equal plausibility to all mean height and SD possibilities. Since each parameter has three possibilities, and since probabilities needs to sum up to 1, each possibility has probability of 1/3 or 0.333. This is assigned to our grid in the Table 7.2. Table 7.2: Parameter possibilities with priors mu sigma mu prior sigma prior 170 9 0.33 0.33 175 9 0.33 0.33 180 9 0.33 0.33 170 10 0.33 0.33 175 10 0.33 0.33 180 10 0.33 0.33 170 11 0.33 0.33 175 11 0.33 0.33 180 11 0.33 0.33 7.3 Likelihood function The sample height data we have collected for N=5 individuals is 167, 192, 183, 175, 177cm. From this sample we are interested in making inference to the true parameter values (i.e. mean and SD, or \\(\\mu\\) and \\(\\sigma\\)). Without going into the Bayes theorem for inverse probability, the next major step is the likelihood function. Likelihood function gives us a likelihood of observing data, given parameters. Since we have 9 parameter possibilities, we are interested in calculating the likelihood of observing the data for each possibility. This is represented with a following Equation (7.1): \\[ \\begin{equation} L(x|\\mu, \\sigma) = \\prod_{i=1}^{n}f(x_{i}, \\mu, \\sigma) \\tag{7.1} \\end{equation} \\] The likelihood of observing the data is calculated by taking the product (indicated by \\(\\prod_{i=1}^{n}\\) sign in the Equation (7.1) of likelihood of observing individual scores. The likelihood function is normal probability density function (PDF)32:, whose parameters are \\(\\mu\\) and \\(\\sigma\\) (see Figure 7.1). This function has the following Equation (7.2): \\[ \\begin{equation} f(x_{i}, \\mu, \\sigma) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}} \\tag{7.2} \\end{equation} \\] Let’s take a particular possibility of \\(\\mu\\) and \\(\\sigma\\), e.g. 175cm and 9cm, and calculate likelihoods for each observed score (Table 7.3). Table 7.3: Likelihoods of observing scores given \\(\\mu\\) and \\(\\sigma\\) equal to 175cm and 9cm mu sigma x likelihood 175 9 167 0.03 175 9 192 0.01 175 9 183 0.03 175 9 175 0.04 175 9 177 0.04 Now, to estimate likelihood of the sample, we need to take the product of each individual score likelihoods. However, now we have a problem, since the result will be very, very small number (1.272648^{-8}). To solve this issue, we take the log of the likelihood function. This is called log likelihood (LL) and it is easier to compute without the fear of losing digits. Table 7.4 contains calculated log from the score likelihood. Table 7.4: Likelihoods and log likelihoods of observing scores given \\(\\mu\\) and \\(\\sigma\\) equal to 175cm and 9cm mu sigma x likelihood LL 175 9 167 0.03 -3.51 175 9 192 0.01 -4.90 175 9 183 0.03 -3.51 175 9 175 0.04 -3.12 175 9 177 0.04 -3.14 Rather than taking a product of the LL to calculate the overall likelihood of the sample, we take the sum. This is due the properties of the logarithmic algebra, where \\(\\log{x_1\\times x_2} = log{x_1} + log{x_2}\\), which means that if we take the exponent of the sum of the log likelihoods, we will get the same result as taking the exponent of the product of likelihoods. Thus the overall log likelihood of observing the sample is equal to -18.18. If we take the exponent of this, we will get the same results as the product of individual likelihoods, which is equal to 1.272648^{-8}. This mathematical trick is needed to prevent very small numbers and thus loosing precision. If we repeat the same procedure for every parameter possibility in our grid, we get the following log likelihoods (Table 7.5). This procedure is also visually represented in the Figure 7.1 for easier comprehension. Table 7.5: Sum of data log likelihoods for parameter possibilities mu sigma mu prior sigma prior LL 170 9 0.33 0.33 -20.12 175 9 0.33 0.33 -18.18 180 9 0.33 0.33 -17.78 170 10 0.33 0.33 -19.79 175 10 0.33 0.33 -18.21 180 10 0.33 0.33 -17.89 170 11 0.33 0.33 -19.63 175 11 0.33 0.33 -18.32 180 11 0.33 0.33 -18.06 Figure 7.1: Likelihood of data given parameters. \\(\\mu\\) and \\(\\sigma\\) represent parameters for which we want to estimate likelihood of observing data collected 7.4 Posterior probability To get the posterior probabilities of parameter possibilities, likelihoods need to be multiplied with priors (\\(posterior = prior \\times likelihood\\)). This is called Bayesian updating. Since we have log likelihoods, we need to sum the log likelihoods with log of priors instead (\\(\\log{posterior} = \\log{prior} + \\log{likelihood}\\)). To get the posterior probability, after converting log posterior to posterior using exponent (\\(posterior = e^{\\log{posterior}}\\))33, we need to make sure that probabilities of parameter possibility sum to one. This is done by simply dividing probabilities for each parameter possibility by the sum of probabilities. Table 7.6 contains the results of Bayesian inference. The posterior probabilities are called joint probabilities since they represent probability of a combination of particular \\(\\mu\\) and \\(\\sigma\\) possibility. Table 7.6: Estimated posterior probabilities for parameter possibilities given the data mu sigma mu prior sigma prior LL posterior 170 9 0.33 0.33 -20.12 0.02 175 9 0.33 0.33 -18.18 0.14 180 9 0.33 0.33 -17.78 0.20 170 10 0.33 0.33 -19.79 0.03 175 10 0.33 0.33 -18.21 0.13 180 10 0.33 0.33 -17.89 0.18 170 11 0.33 0.33 -19.63 0.03 175 11 0.33 0.33 -18.32 0.12 180 11 0.33 0.33 -18.06 0.15 Table 7.6 can be be converted into 3x3 matrix, with possibilities of \\(\\mu\\) in the columns, and possibilities of the \\(\\sigma\\) in the rows and posterior joint probabilities in the cells (Table 7.7). The sums of the joint probabilities in the Table 7.7 margins represent marginal probabilities for parameters. Table 7.7: Joint distribution of the parameter possibilities. Sums at the table margins represent marginal probabilities 170 175 180 Sum 9 0.02 0.14 0.20 0.36 10 0.03 0.13 0.18 0.34 11 0.03 0.12 0.15 0.30 Sum 0.08 0.38 0.54 1.00 Since we have only two parameters, the joint probabilities can be represented with the heat map. Figure 7.2 is a visual representation of the Table 7.7. Figure 7.2: Heat map of \\(\\mu\\) and \\(\\sigma\\) joint probabilities When we have more than 2 parameters, visualization of joint probabilities get’s tricky and we rely on visualizing marginal posterior probabilities of each parameter instead. As explained, marginal probabilities are calculated by summing all joint probabilities for a particular parameter possibility (see Table 7.7). Figure @ref(fig:(marginal-prior-posterior) depicts marginal probabilities (including prior probabilities) for \\(\\mu\\) and \\(\\sigma\\). Figure 7.3: Prior and posterior distributions resulting from simplified grid-approximation example As can be seen from the Figures 7.2 and 7.3, the most likely parameter possibilities, given the data, are \\(\\mu\\) of 180cm and \\(\\sigma\\) of 9cm. 7.5 Adding more possibilities So far, we have made this very granular in order to be understood. However, since we are dealing with continuous parameters, performing grid approximation for more than 9 total parameter possibilities seems warranted. The calculus is exactly the same, as well as the sample collected, but now we will use the larger range for both \\(\\mu\\) (160-200cm) and \\(\\sigma\\) (1-30cm), each with 100 possibilities. We are estimating credibility for total of \\(100 \\times 100 = 10,000\\) parameter possibilities. Figure 7.4 depicts heat map for the joint probabilities, and Figure 7.5 depicts prior and posterior marginal distributions for each parameter. Figure 7.4: Heat map of \\(\\mu\\) and \\(\\sigma\\) joint probabilities when \\(100\\times 100\\) grid-approximation is used Figure 7.5: Prior and posterior distributions resulting from \\(100\\times 100\\) grid-approximation example Grid approximation utilized here is great for educational purposes and very simple models, but as number of parameters increases, the number of total parameter possibility grow so large, that it might take millions of years for a single computer to compute the posterior distributions. For example, if we have linear regression model with two predictors, we will have 4 parameters to estimate (intercept \\(\\beta_{0}\\), predictor one \\(\\beta_{1}\\), predictor two \\(\\beta_{2}\\), and residual standard error \\(\\sigma\\)), and if we use 100 possibilities for each parameter, we will get 10^8 total number of possibilities. This was the reason why Bayesian inference was not very practical. Until algorithms such as Markov Chain Monte Carlo (MCMC) emerged making Bayesian inference a walk in the park. Statistical Rethinking book by Richard McElreath is outstanding introduction into these topics. 7.6 Different prior In this example we have used vague priors for both \\(\\mu\\) and \\(\\sigma\\). But let’s see what happens when I strongly believe (before seeing the data), that \\(\\mu\\) is around 190cm (using normal distribution with mean 190 and SD of 2 to represent this prior), but I do not have a clue about \\(\\sigma\\) prior distribution and I choose to continue using uniform prior for this parameter. This prior belief is, of course, wrong, but maybe I am biased since I originate, let’s say from Montenegro, country with one of the tallest men. Figure 7.6 contains plotted prior and posterior distributions. As can be seen, using very strong prior for \\(\\mu\\) shifted the posterior distribution to the higher heights. In other words, the data collected were not enough to overcome my prior belief about average height. Figure 7.6: Effects of very strong prior on posterior 7.7 More data The sample height data we have collected for N=5 individuals (167, 192, 183, 175, 177cm) was not strong to overcome prior belief. However, what if we sampled N=100 males from known population of known mean height of 177.8 and SD of 10.16? Figure 7.7 depicts prior and posterior distributions in this example. As can be seen, besides having narrower posterior distributions for \\(\\mu\\) and \\(\\sigma\\), more data was able to overcome my strong prior bias towards mean height of 190cm. Figure 7.7: When larger sample is taken (N=100) as opposed to smaller sample (N=20), strong prior was not able to influence the posterior distribution 7.8 Summarizing prior and posterior distributions with MAP and HDI In Bayesian statistics, the prior and posterior distributions are usually summarized using highest maximum a posteriori (MAP) and 90% or 95% highest density interval (HDI) (Kruschke and Liddell 2018a, 2018b; McElreath 2015). MAP is simply a mode, or the most probable point estimate. In other words, a point in the distribution with the highest probability. With normal distribution, MAP, mean and median are identical. The problems arise with distributions that are not symmetrical. HDI is similar to frequentist CI, but represents an interval which contains all points within the interval that have higher probability density than points outside the interval (Makowski, Ben-Shachar, and Lüdecke 2019a, 2019b). HDI is more computationally expensive to estimate, but compared to equal-tailed interval (ETI) or percentile interval, that typically excludes 2.5% or 5% from each tail of the distribution (for 95% or 90% confidence respectively), HDI is not equal-tailed and therefore always includes the mode(s) of posterior distributions (Makowski, Ben-Shachar, and Lüdecke 2019a, 2019b). Figure 7.8 depicts comparison between MAP and 90% HDI, median and 90% percentile interval or ETI, and mean and \\(±1.64 \\times SD\\) for 90% confidence interval. As can be seen from the Figure 7.8, the distribution summaries differ since the distribution is asymmetrical and not-normal. Thus, in order to summarize prior or posterior distribution, MAP and HDI are most often used, apart from visual representation. Figure 7.8: Summarizing prior and posterior distribution. A. MAP and \\(90\\%\\) HDI. B. Median and \\(90\\%\\) ETI. C. Mean and \\(\\pm1.64\\times SD\\) Using SESOI as a trivial range, or as a ROPE (Kruschke and Liddell 2018a, 2018b), Bayesian equivalence test can be performed by quantifying proportion of posterior distribution inside the SESOI band (Kruschke and Liddell 2018a, 2018b; Makowski, Ben-Shachar, and Lüdecke 2019a; Makowski et al., n.d.). Magnitude Based Inference discussed in the the previous chapter, would also be valid way of describing the posterior distribution. Besides estimating using MAP and HDI, Bayesian analysis also allows hypothesis testing using Bayes factor or MAP based p-value (Kruschke and Liddell 2018a, 2018b; Makowski, Ben-Shachar, and Lüdecke 2019a; Makowski et al., n.d.). Discussing these concepts is out of the range of this book and interested readers can refer to references provided for more information. 7.9 Comparison to NHST Type I errors How do the Bayesian HDIs compare to frequentist CIs? What are the Type I error rates when the data is sampled from the null-hypothesis? To explore this question, I will repeat the simulation from New Statistics: Confidence Intervals and Estimation section, where 1,000 samples of N=20 observations are sampled from a population where the true mean height is equal to 177.8cm and SD is equal to 10.16cm. Type I error is committed when the the 95% CIs or 95% HDI intervals of the sample mean don’t cross the true value in the population. Table 7.8 contains Type I errors for frequentist and Bayesian estimation. Table 7.8: Frequentist vs. Bayesian Type I errors method Sample Correct % Type I Errors % Bayesian 1000 96.1 3.9 Frequentist 1000 95.5 4.5 As can be seen from the Table 7.8, frequentist CI and Bayesian HDI Type I error rate are not identical (which could be due to the grid approximation method as well as due to only 1000 samples used). This is often a concern, since Bayesian methods do not control error rates (Kruschke and Liddell 2018a). Although frequentist methods revolve around limiting the probability of Type I errors, error rates are extremely difficult to pin down, particularly for complex models, and because they are based on sampling and testing intentions (Kruschke and Liddell 2018a). For more detailed discussion and comparison of Bayesian and frequentist methods regarding the error control see Kruschke (2013); Wagenmakers (2007); Morey et al. (2016). Papers by Kristin Sainani (Sainani et al. 2019; Sainani 2018) are also worth pondering about which will help in understanding estimation and comparison of Type I and Type II error rates between different inferential methods, particularly when magnitude-based inference using SESOI is considered. References "],
["bootstrap.html", "Chapter 8 Bootstrap 8.1 Summarizing bootstrap distribution 8.2 Bootstrap Type I errors", " Chapter 8 Bootstrap As already stated, some estimators have unknown sampling distribution, particularly those that might have more practical use and answer predictive questions by the practitioners (e.g. “what is the proportion of athletes I can expect to demonstrate beneficial response due to this treatment?”). Hence, the frequentist approach is very hard to use. With the Bayesian approach, some estimators might be really hard to be modeled and represented, researchers might be confused with the priors and likelihood function selections, there is knowledge needed to understand and diagnose sampling chains from MCMC algorithms and so forth. Bootstrap comes for the rescue (Canty and Ripley 2017; A. C. Davison and Hinkley 1997b; Efron and Hastie 2016; Hesterberg 2015; Guillaume A Rousselet, Pernet, and Wilcox 2019b, 2019a). Bootstrap is very simple and intuitive technique that is very easy to carry out. Let’s take an example to demonstrate simplicity of the bootstrap. Continuing with the height example, let’s assume that the following sample is collected for N=10 individuals: 167, 175, 175, 176, 177, 181, 188, 190, 197, 211cm. We are interested in estimating the true mean in the population, SD in the population, and proportion of individuals taller than 185cm (prop185; using algebraic method and estimated SD). The first step, as explained in the Description section of this book, is to estimate those parameters using sample. But how do we estimate the uncertainty around the sample estimates? Bootstrap involves resampling from the sample itself and then recalculating estimates of interest. If we have N=10 observations in the collected sample, for each bootstrap resample we are going to draw 10x1 observations. Some observations might be drawn multiple times, while some might not be drawn at all. This is then repeated numerous times, e.g., 2,000-10,000 times and for each bootstrap resample the estimators of interest are estimated. Table 8.1 contains 10 bootstrap resamples with calculated estimators of interest. Bootstrap resample of number 0 represents the original sample. Table 8.1: Bootstrap resamples Boot resample Observations mean SD prop185 0 167 175 175 176 177 181 188 190 197 211 183.7 13.00 0.46 1 167 167 167 177 188 188 188 188 197 197 182.4 11.98 0.41 2 175 175 175 181 181 181 181 190 197 197 183.3 8.49 0.42 3 175 175 177 177 181 181 188 188 188 190 182.0 5.98 0.31 4 167 181 181 181 181 188 188 197 197 197 185.8 9.61 0.53 5 167 176 176 177 181 188 190 190 211 211 186.7 14.71 0.55 6 167 167 167 175 177 177 188 188 190 211 180.7 13.88 0.38 7 175 175 175 176 177 177 181 181 188 190 179.5 5.50 0.16 8 175 175 175 176 177 181 188 190 190 190 181.7 6.96 0.32 9 176 176 177 181 190 190 190 190 197 211 187.8 10.97 0.60 10 167 167 175 175 175 176 176 177 177 188 175.3 5.83 0.05 11 175 177 181 181 181 188 190 197 197 211 187.8 11.21 0.60 12 175 175 175 176 176 177 181 188 190 211 182.4 11.47 0.41 13 175 175 177 181 181 188 190 190 211 211 187.9 13.43 0.59 14 167 175 175 176 177 177 188 190 211 211 184.7 15.34 0.49 15 175 177 177 181 181 188 188 190 211 211 187.9 13.21 0.59 16 167 175 175 175 177 177 181 188 190 197 180.2 8.92 0.30 17 175 175 175 176 177 177 181 190 190 197 181.3 8.04 0.32 18 167 175 175 175 176 177 177 188 188 188 178.6 7.07 0.18 19 167 167 175 177 181 188 190 197 197 211 185.0 14.24 0.50 20 167 167 175 175 176 176 177 181 197 211 180.2 13.66 0.36 If we repeat this procedure 10,000 times, we can visualize bootstrap distribution of the estimators (Figure 8.1). Figure 8.1: Bootstrap distribution of the estimators using 10,000 resamples How should this bootstrap distribution be interpreted? In “Elements of Statistical Learning”, the following quote regarding bootstrap distribution can be found (Hastie, Tibshirani, and Friedman 2009, 272): “In this sense, the bootstrap distribution represents an (approximate) nonparametric, noninformative posterior distribution for our parameter. But this bootstrap distribution is obtained painlessly — without having to formally specify a prior and without having to sample from the posterior distribution. Hence we might think of the bootstrap distribution as a “poor man’s” Bayes posterior. By perturbing the data, the bootstrap approximates the Bayesian effect of perturbing the parameters, and is typically much simpler to carry out\" Although the bootstrap was originally developed as a purely frequentist device (Efron 2005), as per the quote above, it can be treated as “poor man’s” Bayes posterior. 8.1 Summarizing bootstrap distribution Bootstrap allows for both estimation and hypothesis testing. When it comes to estimations, point estimate of the bootstrap distribution is the sample parameter estimate. Confidence intervals around sample estimate are usually calculated using percentile approach (or ETI), or other approaches such as adjusted bootstrap percentile (BCa) (Canty and Ripley 2017; A. C. Davison and Hinkley 1997b; Efron and Hastie 2016; Hesterberg 2015; Guillaume A Rousselet, Pernet, and Wilcox 2019b, 2019a), or even HDI as used with Bayesian posterior distributions. In this book I will utilize BCa intervals unless otherwise stated. Hypothesis testing using the bootstrap distribution is possible through calculated p-value (Guillaume A Rousselet, Pernet, and Wilcox 2019b, 2019a). This not only allows for bootstrap NHST, but also all other MET, as well as MBI estimates (which assumes Bayesian interpretation of the bootstrap distributions). This is simply done by counting bootstrap sample estimates that are below or above particular threshold (i.e. null-hypothesis or SESOI). The R code (R Core Team 2018; RStudio Team 2016) below demonstrates how two-way NHST can be performed as well as probability of lower, equivalent, and higher effect given the SESOI thresholds. null_hypothesis &lt;- 0 # Value for the null SESOI_lower &lt;- -1 # threshold for the &#39;lower&#39; effect magnitude SESOI_upper &lt;- 1 # threshold for the &#39;upper&#39; effect magnitude # Calculation of the p-value # where boot.estimator is the boostrap resample values for the estimator # of interest p_value &lt;- mean(boot.estimator &gt; null_hypothesis) p_value &lt;- p_value + 0.5 * mean(boot.estimator == null_hypothesis) p_value &lt;- 2 * min(c(p_value, 1 - p_value)) # Two-way p-value # Calculate probability of lower, equivalent and higher effect magnitude lower &lt;- mean(boot.estimator &lt; SESOI_lower) higher &lt;- mean(boot.estimator &gt; SESOI_upper) equivalent &lt;- 1 - (lower + higher) 8.2 Bootstrap Type I errors As we already did with the frequentist and Bayesian inference, let’s get estimates of Type I errors for bootstrap method (10,000 bootstrap resamples) by drawing 1,000 samples of N=20 observations from the population where the true mean height is equal to 177.8cm and SD is equal to 10.16cm. Besides estimating Type I error for the sample mean, we can also estimate Type I errors for sample SD and prop185, since the true population values are known. In the case of prop185, the true population value is equal to 0.24. Type I error is committed when the the 95% bootstrap CIs of the sample estimate don’t cross the true value in the population. Figure 8.2 depicts the first 100 samples out of the total of 1,000, taken from the population with calculated 95% bootstrap CIs. CIs that missed the true population parameter value are depicted in red. Table 8.2 contains the summary for this simulation. Figure 8.2: Bootstrap \\(95\\%\\)confidence intervals. Intervals not capturing the true population parameter are colored in red Table 8.2: Bootstrap Type I errors parameter Sample Correct % Type I Errors % mean 1000 92.7 7.3 sd 1000 89.9 10.1 prop185 1000 93.1 6.9 As can be seen from the Table 8.2, Type I error for the \\(\\sigma\\) parameter is larger than expected. This could be due to the non-symmetrical bootstrap distribution that might not be perfectly represented with the BCa approach of calculating CIs. I am not hiding my preference for the bootstrap methods due to their intuitive nature and simple usage for generating inferences for any estimator. However, bootstrap is not panacea and there are caveats especially for the small samples sizes (Guillaume A Rousselet, Pernet, and Wilcox 2019b, 2019a; Wilcox, Peterson, and McNitt-Gray 2018; Wilcox and Rousselet 2017). References "],
["statistical-inference-conclusion.html", "Chapter 9 Statistical inference conclusion", " Chapter 9 Statistical inference conclusion Which one is better, frequentist, Bayesian or bootstrap? Unfortunately, there is no objective, automatic way to statistical inference. All approaches demand careful planning and consideration of the research question of interest. All approaches rely on assumptions, some more clearly stated as with Bayesian approach, and some more inherent as with frequentist approach. All approaches to statistical inference represent Small Worlds that are hoped to help explain the Large World. Rather than approaching inferential statistics as something that represents and estimates true state of the World, inferential statistics might be approached as descriptive statistic (Amrhein, Trafimow, and Greenland 2019). This section on statistical inference is best summarized with the following quotes: “Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.\" (Amrhein, Trafimow, and Greenland 2019, Abstract, pp. 262) “Exercising awareness of multiple perspectives, we emphasize that we do not believe that one of these philosophies is the correct or best one, nor do we claim that reducing the different approaches to a single one would be desirable. What is lacking here is not unification, but rather, often, transparency about which interpretation of probabilistic outcomes is intended when applying statistical modeling to specific problems. Particularly, we think that, depending on the situation, both “aleatory” or “epistemic” approaches to modeling uncertainty are legitimate and worthwhile, referring to data generating processes in observer-independent reality on one hand and rational degrees of belief on the other.\" (Gelman and Hennig 2017, 20) “Broadly speaking, 19th century statistics was Bayesian while the 20th century was frequentist, at least from the point of view of most scientific practitioners. Here in the 21st century scientists are bringing statisticians much bigger problems to solve, often comprising millions of data points and thousands of parameters. Which statistical philosophy will dominate practice? My guess, backed up with some recent examples, is that a combination of Bayesian and frequentist ideas will be needed to deal with our increasingly intense scientific environment. This will be a challenging period for statisticians, both applied and theoretical, but it also opens the opportunity for a new golden age, rivaling that of Fisher, Neyman, and the other giants of the early 1900’s.” (Efron 2005) This being said, my personal opinion is that more and more sport scientists will consider Bayesian analysis, particularly with the new tools that provide ease of Bayesian model definition and training, as well as visualization of the results. For the benefits of Bayesian over frequentist analysis please see the papers by Kruschke et al. (Kruschke and Liddell 2018a, 2018b) as well as suggested reading on Bayesian analysis at the end of this book. References "],
["measurement-error.html", "Chapter 10 Measurement Error 10.1 Estimating TE using ordinary least products regression 10.2 Smallest Detectable Change 10.3 Interpreting individual changes using SESOI and SDC 10.4 What to do when we know the error? 10.5 Extending the Classical Test Theory", " Chapter 10 Measurement Error Measurement error is involved in all measurements and causes an observed score to be different from the true score (Allen and Yen 2001; Novick 1966; Swinton et al. 2018; Borsboom 2009). This results in measurement bias affecting descriptive analysis, causal inferences (Hernán 2017; Hernán and Robins, n.d.; Hernan and Cole 2009), and predictive performances (Kuhn and Johnson 2018). In mathematical notation, observed score (OS) comprises of the hypothetical true score (TS) and measurement error (ME) (Equation (10.1)) (Allen and Yen 2001; Swinton et al. 2018). This conceptual model is called Classical Test Theory (for more information, philosophy, and issues behind it please see Borsboom (2009)) \\[ \\begin{equation} OS = TS + ME \\tag{10.1} \\end{equation} \\] In the sports science domain, since the measured objects are usually humans, measurement error comprises of instrumentation and biological noise (Swinton et al. 2018). In this book I assume instrumentation noise to be error caused solely by the measurement apparatus (Swinton et al. 2018). Biological noise, on the other hand, is defined as an error in the observed scores caused by biological processes, including, but not limited to, phenomena such as circadian rhythm, nutritional intake, sleep and motivation (Swinton et al. 2018). Thus, I prefer to use the terms instrumentation error and biological variation (these will be further discussed in the Extending the Classical Test Theory section) Both instrumentation and biological noises consist of two types of errors: systematic error and random error (Figure 10.1)34. Figure 10.1: Measurement error components Systematic error represents constant and stable error that is fixed across measurements. Systematic error is commonly refereed to as bias. With measurement instruments that have a linear response, systematic error can be further divided into proportional bias and fixed bias (Will G Hopkins 2004b; Hopkins 2010, 2007). Random error (\\(\\epsilon\\)) represents unknown and unpredictable error, which varies between measurements. Random errors are often represented and modeled using a Gaussian normal distribution (with mean zero and SD which represent a parameter of the random error). The Equation (10.2) represent theoretical linear relationship between TS and OS, with normally distributed random error. \\[ \\begin{equation} OS = Fixed\\; Bias + (Proportional \\; Bias\\times TS) + \\epsilon \\tag{10.2} \\end{equation} \\] This can be easily explained with a simple example. Imagine N=20 athletes being measured on a novel bodyweight scale, using total of 5 trials separated by 1 minute. The assumption in this example is that there is no change in the TS across trials (e.g. athletes are not allowed to use bathroom, consume water or food, nor change the wardrobe) and that there is no biological noise involved (i.e. there are no fluctuations in bodyweight due motivation, fatigue or what have you). Since this is simulated example, we know the TS of the athletes, but also the instrumentation noise characteristic of the novel bodyweight scale (i.e. this represents the underlying DGP). This scale tends to have proportional bias equal to factor 1.01 (i.e. athlete weighting 100kg which is his TS, will have OS equal to 101kg due proportional bias, while the athlete weighting 50kg will have her OS equal to 50.5kg), fixed bias equal to 1kg (everyone will get OS higher for 1kg than TS), and random error normally distributed with SD equal to 0.5kg. Equation (10.3) captures this relationship between OS and TS. \\[ \\begin{equation} OS = 1 + (1.01 \\times TS) + \\mathcal{N}(0,\\, 0.5) \\tag{10.3} \\end{equation} \\] Table 10.1 contains the simulated sample for N=20 athletes and 5 trials. Table 10.1: Simulated 5 trials from known true score and measurement error Athlete TS (kg) OS 1 (kg) OS 2 (kg) OS 3 (kg) OS 4 (kg) OS 5 (kg) Athlete 01 77.93 79.47 79.37 79.25 80.38 79.46 Athlete 02 76.11 77.83 77.71 77.36 78.32 76.78 Athlete 03 77.04 78.65 78.30 78.48 78.30 78.55 Athlete 04 54.96 56.58 56.36 56.05 56.20 56.45 Athlete 05 84.03 85.53 85.44 86.10 85.96 85.11 Athlete 06 61.32 61.84 63.05 63.52 63.73 62.43 Athlete 07 68.62 70.26 69.82 70.34 70.45 70.84 Athlete 08 61.06 62.03 62.91 62.93 62.15 62.40 Athlete 09 80.46 81.34 83.04 82.57 82.60 82.33 Athlete 10 91.14 94.02 93.47 93.28 93.40 93.51 Athlete 11 79.98 81.89 82.13 81.13 81.57 81.94 Athlete 12 67.07 69.54 68.32 68.29 69.23 67.72 Athlete 13 79.41 80.66 80.81 81.17 80.56 80.92 Athlete 14 69.54 71.14 72.34 70.16 72.01 71.11 Athlete 15 76.01 77.42 77.41 78.32 78.22 77.61 Athlete 16 68.31 70.11 70.05 70.17 69.02 70.06 Athlete 17 58.53 60.04 60.56 59.69 59.72 60.69 Athlete 18 81.64 82.61 83.30 83.66 82.87 82.60 Athlete 19 55.03 56.70 56.35 56.90 56.48 56.37 Athlete 20 65.03 66.33 66.44 67.29 67.28 66.82 The objective of the analysis is to estimate DGP parameters of the measurement error (the proportional bias, fixed bias, and the SD of the random error). Unfortunately, since TS is unknown, we are unable to estimate proportional bias and fixed bias. To overcome this problem, we usually compare OS to some gold standard measure which can serve as proxy to TS. These issues are covered in much more detail in the second part of this book in the Validity and Reliability chapter. What is left to be estimated is the SD of the random error, which is often referred to as typical error (TE) of the test, or standard error of the measurement (SEM)35. TE is estimated using individual SD of the OS in the five trials (Table 10.2). Table 10.2: Individual mean and SD from five trials Athlete Mean SD Athlete 01 79.59 0.45 Athlete 02 77.60 0.57 Athlete 03 78.46 0.16 Athlete 04 56.33 0.21 Athlete 05 85.63 0.40 Athlete 06 62.91 0.78 Athlete 07 70.34 0.37 Athlete 08 62.48 0.42 Athlete 09 82.38 0.63 Athlete 10 93.54 0.28 Athlete 11 81.73 0.39 Athlete 12 68.62 0.75 Athlete 13 80.82 0.24 Athlete 14 71.35 0.86 Athlete 15 77.80 0.44 Athlete 16 69.88 0.49 Athlete 17 60.14 0.47 Athlete 18 83.01 0.46 Athlete 19 56.56 0.24 Athlete 20 66.83 0.45 The mean of athletes’ typical errors (SD in Table 10.2) is equal to 0.45kg, which is quite close to DGP random error parameter of 0.5kg. The reason for the difference between estimated and true value of the random error SD is due to the sampling error, which is a topic covered in the Statistical inference section of this book. Unfortunately, this method of estimating TE is not always practically feasible. TE is usually estimated with two trials (OS1 and OS2; see Table 10.3), by using SD of the difference scores (\\(SD_{diff}\\)) across athletes (Hopkins 2000; Swinton et al. 2018). Table 10.3: Estimating Typical Error using SD of the difference scores Athlete OS 1 (kg) OS 2 (kg) Difference OS 2-1 (kg) Athlete 01 79.47 79.37 -0.11 Athlete 02 77.83 77.71 -0.11 Athlete 03 78.65 78.30 -0.35 Athlete 04 56.58 56.36 -0.23 Athlete 05 85.53 85.44 -0.09 Athlete 06 61.84 63.05 1.22 Athlete 07 70.26 69.82 -0.44 Athlete 08 62.03 62.91 0.88 Athlete 09 81.34 83.04 1.70 Athlete 10 94.02 93.47 -0.55 Athlete 11 81.89 82.13 0.24 Athlete 12 69.54 68.32 -1.22 Athlete 13 80.66 80.81 0.15 Athlete 14 71.14 72.34 1.21 Athlete 15 77.42 77.41 -0.01 Athlete 16 70.11 70.05 -0.06 Athlete 17 60.04 60.56 0.52 Athlete 18 82.61 83.30 0.70 Athlete 19 56.70 56.35 -0.35 Athlete 20 66.33 66.44 0.11 If we calculate SD of the difference scores from the Table 10.3, we get 0.7kg. However, this is not quite right, since we know that the true SD of the random error is 0.5kg. This happens because random error is affecting both Trial 1 (OS1) and Trial 2 (OS2), and is propagated to the difference between the two (Figure measurement-error-propagation). This is exactly the same concept as described in the Example of randomized control trial. The benefit of using squared errors and assuming Gaussian error distribution, as alluded multiple times in this book, is that this propagation can be mathematically and neatly expressed and estimated. Figure 10.2: Propagation of the random component of measurement error to two trials Error propagation, assuming normal distribution, is equal to Equation (10.4): \\[ \\begin{equation} \\begin{split} SD_{difference}^2 &amp;= TE_{trial\\;1}^2 + TE_{trial\\;2}^2 \\\\ TE_{test} &amp;= TE_{trial\\;1} = TE_{trial\\;2} \\\\ SD_{difference}^2 &amp;= TE_{test}^2 + TE_{test}^2 \\\\ SD_{difference}^2 &amp;= 2 \\times TE_{test}^2 \\\\ SD_{difference} &amp;= \\sqrt{2 \\times TE_{test}^2} \\\\ SD_{difference} &amp;= \\sqrt{2} \\times TE_{test} \\\\ TE_{test} &amp;= \\frac{SD_{difference}}{\\sqrt{2}} \\end{split} \\tag{10.4} \\end{equation} \\] According to the Equation (10.4), the SD of the difference score needs to be divided by \\(\\sqrt{2}\\) to estimate TE of the measurement. The assumption is that TE is equal in both trials (and for all athletes), which is defined in the second line in the Equation (10.4). Estimated TE is now equal to 0.49kg, which is much closer to known SD of the random error of 0.5kg. 10.1 Estimating TE using ordinary least products regression The method of estimating TE using SD of the difference score can be termed method of the differences and it is quite commonly used in Validity and Reliability analyses, particularly when using Bland-Altman plots. Another method involves the use of the linear regression, or the special kind of linear regression called ordinary least products (OLP) (Ludbrook 2010, 2012, 1997, 2002; Mullineaux, Barnes, and Batterham 1999). The ordinary least squares (OLS; the one we have used thorough this book) regression find the line that minimizes squared residuals between \\(y_i\\) and \\(\\hat{y_i}\\), while OLP minimizes the product of the residuals using both \\(x\\) and \\(y\\): \\((x_i - \\hat{x_i}) \\times (y_i - \\hat{y_i})\\). The benefit of using OLP over OLS is that estimated model is the same regardless of which variable is considered outcome or predictor. This is not the case with OLS, as demonstrated in the Describing relationship between two variables section. For this reason, OLP is a preferred choice when performing Reliability analyses, since neither variable is considered outcome. Figure 10.3 demonstrate OLP regression between Trial 1 (OS1) and Trial 2 (OS2). Estimated residual standard error (RSE) is equal to 0.72cm. To estimate TE, this RSE also needs to be divided by \\(\\sqrt{2}\\), which results in 0.51cm. This estimate of TE is very close to TE estimated by the method of differences. Figure 10.3: Ordinary least squares regression (OLP) between Trial 1 (OS1) and Trial 2 (OS2). A. Scatter-plot between OS1 and OS2. Dashed line represent identity line, and black line represent estimated OLP regression. B. Residuals plot. Dashed lines represent upper and lower levels of agreement using RSE and 95% confidence level (or in other words, 95% of the residuals distribution will be within these two dashed lines). Blue line represent additional linear regression model (using OLS) between fitted and residual, used to indicate issue with the model. If we consider ±1kg to be SESOI in the observed score, we can estimate practical reliability of this scale. Magnitude-based estimators, such as PPER or SESOI to RSE can be used to quantify scale reliability from a practical significance perspective. This can be represented with the SESOI band as done in the Figure 10.4. Figure 10.4: Ordinary least squares regression (OLP) between Trial 1 (OS1) and Trial 2 (OS2). A. Scatter-plot between OS1 and OS2. Dashed line represent identity line, and black line represent estimated OLP regression. B. Residuals plot. Dashed lines represent upper and lower levels of agreement using RSE and 95% confidence level (or in other words, 95% of the residuals distribution will be within these two dashed lines). Blue line represent additional linear regression model (using OLS) between fitted and residual, used to indicate issue with the model. SESOI is represented with the grey band. Residuals within SESOI band are of no practical difference. Proportion of residuals within SESOI band represent PPER estimator. As can be seen from the Figure 10.4, this scale have less than perfect reliability to detect practically important changes in weight (given a priori defined SESOI of ±1kg). Estimated PPER, using RSE, is equal to 0.82. 10.2 Smallest Detectable Change In the case of reliability and validity analysis, we are mostly interested in the ability of a measure to detect signal from a noise. In this case, the signal is the true or real change and noise is the random error of the measure. Thus, we are interested in estimating what is the smallest detectable signal the measure can detect with certain level of confidence. This is called smallest detectable change (SDC) or minimal detectable change (MDC). If you check the Panel B in the Figure 10.4, SDC represents the spread of the residuals, visualized with the two horizontal dashed lines. If we assume that the residuals are normally distributed, we can estimate lower and upper thresholds that capture, for example 95% of the residuals distribution. This is done by multiplying \\(SD_{diff}\\) or RSE with appropriate critical value from a Student’s t-distribution (i.e. or simply with ±1.96). We thus get the Equation (10.5) that we can use to estimate SDC with 95% level of confidence36. \\[ \\begin{equation} \\begin{split} SDC &amp;= SD_{diff} \\times \\pm1.96 \\\\ SDC &amp;= RSE \\times \\pm1.96 \\\\ SDC &amp;= TE \\times \\sqrt{2} \\times \\pm1.96 \\\\ \\end{split} \\tag{10.5} \\end{equation} \\] Using OLP regression estimate RSE (equal to 0.72), and critical value to cover 95% of the Student’s t-distribution (DF=19) equal to ±2.09, SDC for our scale is equal to ±1.5kg. This implies that, with the 95% confidence, we are able to detect true signal (i.e. change in weight) as low as ±1.5kg. If SDC is lower than SESOI, reliability of the measure is (practically) perfect. This is not the case for our scale. We cannot use it to detect changes of ±1kg with satisficing level of confidence. SDC can also be used as SESOI in some other analysis utilizing this scale. For example, if we use nutrition intervention RCT utilizing this scale as a measure, we can use ±1.5kg as SESOI (in the absence of better defined SESOI) since that is the the minimum detectable effect size. 10.3 Interpreting individual changes using SESOI and SDC In order to showcase the interpretation of the individual changes by using SESOI and SDC (named observed outcome approach in Analysis of the individual residuals: responders vs non-responders section), let’s consider bench press example from the Comparing dependent groups (repeated in the Table 10.4). Table 10.4: Individual Pre and Post scores, as well as Change in the bench press 1RM Athlete Pre-test (kg) Post-test (kg) Change (kg) Athlete 01 111.80 121.42 9.62 Athlete 02 95.95 102.13 6.18 Athlete 03 105.87 125.56 19.69 Athlete 04 98.79 109.67 10.87 Athlete 05 95.81 108.11 12.30 Athlete 06 95.27 92.67 -2.60 Athlete 07 97.75 106.03 8.28 Athlete 08 106.50 109.51 3.01 Athlete 09 80.62 95.96 15.34 Athlete 10 100.40 94.30 -6.11 Athlete 11 82.71 78.91 -3.80 Athlete 12 102.89 93.98 -8.91 Athlete 13 91.34 105.21 13.87 Athlete 14 111.14 108.07 -3.07 Athlete 15 95.13 96.01 0.88 Athlete 16 109.12 112.12 3.00 Athlete 17 91.87 103.41 11.54 Athlete 18 92.16 103.93 11.77 Athlete 19 108.88 119.72 10.84 Athlete 20 97.94 95.91 -2.03 Before commencing this simple intervention, measurement error of the bench press 1RM test is estimated (using TE estimator), and is equal to 2.5kg (N=19). Practically, this means that due to the biological and instrumentation error, 1RM in the bench press would tend to vary normally distributed with TE equal to 2.5kg, given, of course, no real change in the strength (i.e. no change in TS). Expected SDC (with 95% confidence level) is thus equal to \\(\\sqrt{2}\\times TE \\times \\pm2.09\\) (±7.39kg). Please note that TE of the change score is equal to \\(\\sqrt{2}\\times TE\\), or (3.54kg). For the sake of example, ±5kg can be considered minimal important change, which will be used as SESOI. Since both TE and SESOI are known, the objective of the analysis is to estimate probability that the observed individual change score is practically significant (i.e. lower, equivalent, or higher compared to SESOI). This is because individual true changes are not known, but only observed changes. Change TE tells how much of observed change randomly varies around the true change score. The question trying to be answered is: “how likely individual’s true change is within lower, equivalent, or higher SESOI range, given the known change TE?” Panel A in the Figure 10.5 depicts individual Change scores probabilistically using the known change TE (3.54kg). Using the SESOI as equivalent change, we can estimate individual probability of lower, equivalent, and higher change. Panel B in the Figure 10.5 depicts individual change scores with error bars representing SDC. The numbers in brackets on Panel B in the Figure 10.5 represent estimated probabilities of the true change score being lower, equivalent, and higher compared to SESOI. To be more certain of individual changes, SDC needs to be smaller compared to SESOI. Ratio between SESOI and change TE can thus represent an estimate of the test sensitivity to detect practically meaningful changes (i.e. SESOI to RSE estimator). The smallest change that has at least 95% chance of being higher or lower than SESOI is equal to \\(SESOI \\pm SDC\\), or ±12.39kg. Graphically, bench press 1RM change of ±12.39kg is the smallest change, where 95% confidence intervals do not touch the SESOI band. Please note that inference from MET is slightly different, since METs use single-tailed tests, thus the critical value will be smaller that 2.09 (it will be 1.73 for single-tailed 95% confidence). This implies that 95% confidence intervals (i.e. SDC) can slightly cross SESOI threshold and still be considered “Higher” or \"Lower. Figure 10.5: Analysis of individual change scores using SESOI and SDC. A. Uncertainty around true change score can be depicted by using normal distribution whose SD is equal to change TE. Probability that the observed change is lower, equivalent or higher can be estimated using surface within lower, equivalent, and higher magnitude band. B. \\(95\\%\\) Confidence intervals around change scores represent SDC and are calculated using \\(\\pm 2.09\\times\\sqrt{2}\\times TE\\). Numbers in brackets represent proportion of the surface area in the lower, equivalent and higher magnitude band. These are interpreted as probabilities of true score being in lower, equivalent and higher magnitude band. See text for discussion why such interpretation is not statistically valid As explained in the Statistical inference section of this book, this method of individual analysis interprets the change TE and associated confidence intervals from the Bayesian perspective. This is not the correct interpretation, since we do not know individual’s true change scores, only the observed change scores. Change TE gives us the variance of the observed change scores around the true change score, not vice versa (i.e. Bayesian inverse probability). Thus, visual representation from the Figure 10.5 is not statistically valid. Since we do not know the true change scores, we are interested in probabilities of seeing the observed change score given the assumption of where we think the true change score is (i.e. null hypothesis). For this reason the question to be asked is “assuming individual’s true change scores are at ±SESOI, how likely are we to see the observed change score, given the known change TE?”. This question demands answer and interpretation of change TE from the frequentist perspective. Thus the correct interpretation of the individual changes involves the use of minimum effect tests (METs) discussed in the Statistical Inference section. METs approach to interpreting individual changes is depicted in the Figure 10.6. Figure 10.6: METs approach to interpreting individual change scores. A. Since true change scores are unknown, we can only test probability of seeing observed score or higher, given the known TE and assumed true score. In order to do this, minimal effect tests are performed, assuming two true score null-hypotheses: one at lower SESOI threshold (red color) and one at upper SESOI threshold (green color). Typical error can be interpreted as SD of the error distribution. Black dots indicate observed individual change. We are interested in estimating probability of observing this change, given two hypotheses. Five tests are performed as described in Minimum Effect Tests section: inferiority, non-superiority, equivalence, non-inferiority and superiority. B. \\(95\\%\\) Confidence intervals around change scores represent SDC and are calculated using \\(\\pm 2.09\\times\\sqrt{2}\\times TE\\) and depicted using error-bars. Final inference using five METs is reported. METs significance (assuming alpha=0.05), indicated by ’*’, are reported in the brackets, for each of the the five tests performed 10.4 What to do when we know the error? Statistical analysis covered in this book treats observed scores as true scores without measurement error. But measurement error is always involved and can introduce bias in the conclusion. How certain estimators and analyses are sensitive to measurement error can be estimated via simulations. But what can we do when we do know that there is measurement error involved in our observations and we actually have an estimate about it’s size (i.e. from validity or reliability study)? There are few adjustment techniques that can be implemented (Keogh et al. 2020; Lederer and Küchenhoff 2006; Shang 2012; Shaw et al. 2020; Wallace 2020), but here I will introduce Simulation extrapolation (SIMEX) approach since it is very intuitive. Let’s take the bench press example we used above: we do know that measurement error of 2.5kg is involved in the observed Pre- and Post-tests. How can we correct or adjust our estimators using that knowledge? Since we know that error is already inside our observations, we can call that error factor or error multiplier of 1. Then we add additional error to our observations and repeat the analysis. This is done for error multipliers from 1 to usually 3 (i.e. extra 2 x measurement error). Let’s do that using bench press data and calculate mean and SD of the Pre-test, Post-test, and Change. This single simulation is in Figure 10.7. Figure 10.7: Adding noise (measurement error) to observed Pre-test and Post-test scores and re-calculating estimators. Error multiplier equal to 1 is naive analysis where one magnitude of measurement error (i.e. 2.5kg) is already involved in observed data. Additional error is added using error multiplier (i.e. error multiplier 2 involves additional noise of 2.5 kg magnitude, thus 2 error magnitudes are involved in the data) from 1 to 3, using total of 20 steps We can’t conclude much since adding error multiplier once is stochastic. We need to repeat this numerous times, say 100 times. This is depicted in Figure 10.8. Figure 10.8: Result of SIMEX using 100 simulations What we are interested in, is calculating the average or expected estimator value for each error multiplier. This is depicted in Figure 10.9 Figure 10.9: Result of SIMEX using 100 simulations and addition simulation average. Blue line represents simulations average for a particular error multiplier Rather than using average across simulations, we can fit a particular model and then extrapolate to error multiplier of 0. This way we can get estimated estimator value when there is no measurement error involved in Pre-test and Post-test variables. Usually this is done using second order polynomial (i.e. \\(\\hat{y_i} = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2\\)), or quadratic equation (i.e. \\(\\hat{y_i} = \\beta_0 + \\beta_1x_i^2\\)). Extrapolating using quadratic equation is depicted in the Figure 10.10. Figure 10.10: Result of SIMEX using 100 simulations and addition quadratic extrapolation. Red line represents quadratic fit, extrapolated to error multiplier of 0 to estimate estimator value when there is no measurement error involved in the Pre-test and Post-test variables From 10.10 it is interesting to notice that mean is robust to measurement error, while SD is not, which is expected since normal error increase variance of the data. Using SIMEX for Change SD, we can estimate true Change SD, or in other words, when there is no measurement error. This can also be done using math rather than simulation and extrapolation (i.e. SIMEX) by using the same error propagation reasoning explained in the Example of randomized control trial section (Equation (10.6)). \\[ \\begin{equation} \\begin{split} \\sigma_{OS}^2 &amp;= \\sigma_{TS}^2 + \\epsilon^2 \\\\ SD_{observed \\; diff}^2 &amp;= SD_{true \\; diff}^2 + (\\sqrt{2}TE)^2 \\\\ SD_{true \\; diff} &amp;= \\sqrt{SD_{observed \\; diff}^2 - 2TE^2} \\end{split} \\tag{10.6} \\end{equation} \\] Observed Change SD is equal to 8.05kg, while the change TE is equal to \\(\\sqrt{2} \\times 2.5\\)kg or 3.53kg. True Change SD is thus equal to \\(\\sqrt{8.05^2 - 3.53^2 }\\), or 7.23kg. This is also done when estimating stochastic treatment effect in the RCT, if we assume that the Change SD in the Control group is mainly due to measurement error. 10.5 Extending the Classical Test Theory The theory behind true, observed scores, and measurement error introduced in this chapter is called Classical Test Theory (Borsboom 2009). Although it sounds simple, there are numerous assumptions and issues with it (Borsboom 2009, 44–45): \"Classical test theory was either one of the best ideas in twentieth-century psychology, or one of the worst mistakes. The theory is mathematically elegant and conceptually simple, and in terms of its acceptance by psychologists, it is a psychometric success story. However, as is typical of popular statistical procedures, classical test theory is prone to misinterpretation. One reason for this is the terminology used: if a competition for the misnomer of the century existed, the term ‘true score’ would be a serious contestant. The infelicitous use of the adjective ‘true’ invites the mistaken idea that the true score on a test must somehow be identical to the ‘real’, ‘valid’, or ‘construct’ score. This chapter has hopefully proved the inadequacy of this view beyond reasonable doubt. The problems with the platonic true score interpretation were, however, seen to run deeper than a confounding of validity and unreliability. Classical test theory is, ontologically speaking, ambiguous. In principle, it allows for both realist and constructivist interpretations, but sits well with neither. The constructivist interpretation of classical test theory is vacuous: although it is possible to specify how a true score could be constructed on the basis of observations (namely by averaging), the observations that are necessary for doing this are exactly the repeated measurements with intermediate brainwashing. The constructivist account of true scores can only be interpreted metaphorically, and it is not at all clear what such a metaphorical interpretation adds to the theory. On the other hand, a realist i terpretation of true scores leads to a metaphysical explosion of reality: a new true score has to be invoked for every thinkable testing procedure.\" Let’s take the bench press example yet again. What is individual’s true score? Something that one can manifest? What if I have bench pressed 150kg, but got a flu and was off for 3 weeks. When I came back, I was able to bench press 130kg. Is my true bench press still 150kg, but masked due to systematic effect of the illness? What if this happens in a reliability study, and few individuals demonstrate true systematic effects of fatigue, while some demonstrate biological variability from day to day? We do assume Ergodicity in this example. Figure 10.11 depicts my extension of the Classical Test Theory with performance specialist or sports scientist in mind (Jovanović 2020). As alluded multiple time thorough this book, all statistical models represent “Small Worlds”, or simplifications of the complex “Large World” we ought to understand and interact with. Figure 10.11: Circular Performance Model. Extended Classical Test Theory to include phenomenology of working with athletes Circular Performance Model depicted in the Figure 10.11 (Jovanović 2020) can be used to model and understand phenomena that sport practitioners and sports scientists wrestle with daily. References "],
["conclusion.html", "Chapter 11 Conclusion", " Chapter 11 Conclusion Statistical analysis should start with questions that we are trying to answer using the data. These questions of interest should not only guide statistical analysis, but also guide data collection and finally interpretations of the analysis and modelling results. In order to answer these questions with data, we are always representing the Large World with the Small World models. There is no entirely objective approach to do it, rather pluralism of approaches (Mitchell 2012, 2002) should be applied. The value of these models and Small World representations should be judged by qualities suggested by Gelman and Hennig (Gelman and Hennig 2017): transparency, consensus, impartiality, correspondence to observable reality, awareness of multiple perspectives, awareness of context-dependence, and investigation of stability. Finally, we need to accept that we must act based on cumulative knowledge rather than solely rely on single studies or even single lines of research (Amrhein, Trafimow, and Greenland 2019). For example, let’s take a question that a sport practitioner might ask: “From how many athletes I can expect to see positive improvements after this intervention? Will Johnny improve?” This question is predictive question, which is common in practice. Assume that I provide this coach with the estimate of the average causal effect and accompanying magnitude-based inference (MBI), using SESOI he provided (i.e. 20% harmful, 30% equivalent, and 50% beneficial) or frequentist p-value of p&lt;0.05. Will that answer the practitioner’s question? The accompanying MBIs might even confuse him and appear to answer the “proportion” question he asked. “So, 50% of athletes will show beneficial response to treatment?”. Unfortunately no - MBIs (or METs) answer different question about estimator (be it mean or Cohen’s d or some other), not about individual response proportions. Second part of his question also demands predictive modeling, that calls for taking into account Johnny’s known data and getting the best estimate for his response. If there is some historical data about Johnny, we might get better predictions (i.e. either through individual modeling or hierarchical model), but if not, then the average-based estimators might be our best guess of the most likely response that Johnny might manifest. Reporting proportions of responses on top of the average effect estimate might help answering questions about uncertainty regarding individual responses. I am not saying that these are not important. I am only saying that they should not be automatically selected as an answer to any question. CIs can provide us with the uncertainty interval around proportions (i.e. “Model gives us 90% confidence that the proportion of the beneficial responses will be 40-60%”), or even around predictive performance metrics. However, we need to make sure to start with the question asked, as well as to suit our analysis and conclusions so that practitioners can understand it, and finally act based on it. In the following part of this book, I will provide solution to the most common sport science problems and question using the material covered in this part, as well as bmbstats package written by the author. References "],
["bmbstats-bootstrap-magnitude-based-statistics-package.html", "Chapter 12 bmbstats: Bootstrap Magnitude-based Statistics package 12.1 Installation", " Chapter 12 bmbstats: Bootstrap Magnitude-based Statistics package In the first part of this book we have covered descriptive, predictive, and causal inference tasks, followed by the basics of statistical inference using frequentist, Bayesian, and bootstrap methods and concluded with the measurement error explanation. In this part of the book, we will turn to bmbstats, which is short of Bootstrap Magnitude-based Statistics, package to perform analysis of the most common sports science problems and utilize bootstrap as the inference method of choice. Since I strongly believe that the best way to understand statistical analysis is through simulations and smart visualizations, in this part of the book I will show the R code that you can use to reproduce the analysis, but more importantly, to understand the underlying DGP we are trying to explain (or estimate) with the analysis. 12.1 Installation You can install the development version from GitHub with: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;mladenjovanovic/bmbstats&quot;) require(bmbstats) "],
["descriptive-tasks-using-bmbstats.html", "Chapter 13 Descriptive tasks using bmbstats 13.1 Generating height data 13.2 Visualization and analysis of a single group/variable 13.3 Visualization and analysis of the two independent groups 13.4 NHST, METs and MBI functions 13.5 Comparing two dependent groups 13.6 Describing relationship between two groups", " Chapter 13 Descriptive tasks using bmbstats In this chapter I will demonstrate the analysis of the basic descriptive tasks using bmbstats package. Each data set that we will analyze will be generated with the R code, so the underlying DGP will be transparent and thus very useful for understanding what we are trying to do with the analysis. 13.1 Generating height data In Comparing two independent groups chapter we have used height data for 50 males and 50 females. This is how that data is generated: require(bmbstats) require(tidyverse) set.seed(1667) n_subjects &lt;- 50 # Generate height data height_data &lt;- data.frame( Male = rnorm( n = n_subjects, mean = 177.8, sd = 10.16 ), Female = rnorm( n = n_subjects, mean = 165.1, sd = 8.89 ) ) head(height_data) #&gt; Male Female #&gt; 1 193.9007 150.7703 #&gt; 2 172.4291 150.0221 #&gt; 3 186.3210 170.1512 #&gt; 4 177.4417 156.4366 #&gt; 5 167.5636 156.1961 #&gt; 6 172.9078 158.9467 13.2 Visualization and analysis of a single group/variable The simplest descriptive task is the description of a single group. Let’s use height of the females as an example. Function bmbstats::plot_raincloud37 can be used to plot the distribution and summary statistics (mean and SD as error bar): bmbstats::plot_raincloud( data = height_data, value = &quot;Female&quot;, control = plot_control(group_colors = &quot;pink&quot;) ) Functions in bmbstats package use control parameter to setup graphing or modeling parameters. For example, we can remove the quantile lines, resize points, change color by using bmbstats::plot_control function in the control parameter: bmbstats::plot_raincloud( data = height_data, value = &quot;Female&quot;, control = plot_control( group_colors = &quot;grey&quot;, cloud_quantile_lines = FALSE, summary_bar_color = &quot;red&quot;, summary_bar_nudge = -0.15, points_jitter_width = 0.1, points_size = 2 ) ) One of the core functions in bmbstats package is bmbstats::bmbstats, around which multiple wrapper functions are built, such as bmbstats::describe_data. bmbstats::describe_data performs bootstrap using the estimators provided in the estimator_function parameter. To modify bootstrap parameters, use control parameter and bmbstats::model_control function: female_analysis &lt;- bmbstats::describe_data( x = height_data$Female, estimator_function = bmbstats::data_estimators_simple, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) female_analysis #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 163.177916 161.200476 165.143065 #&gt; SD 8.199373 6.914061 9.136136 The above code uses bmbstats::data_estimators_simple function that returns mean and SD estimators. If you want to access the data frame containing estimators values and upper and lower confidence thresholds use the following code: female_analysis$estimators Since we have generated the data, we know the true population parameters for the mean (165.1cm) and SD (8.89cm), and we are hoping that our 90% confidence intervals capture those values in 90% of the cases in the long run (i.e. have 10% Type I error). To plot bootstrap distributions, use simple plot function: plot(female_analysis) The figure above depicts distribution of the bootstrap resamples with the error bar representing estimator value and upper and lower confidence thresholds (in this case 90% estimated using percentile method). To change the colors, use control and plot_control: plot( female_analysis, control = plot_control( group_colors = &quot;grey&quot;, summary_bar_color = &quot;red&quot; ) ) 13.2.1 Using your own estimators bmbstats functions are modular, implying that you can use different modules that you write yourself. This includes estimators function, but also performance functions (covered later when discussing prediction using bmbstats). Let’s say we are interested in the calculating mean, SD, and CV% (coefficient of variation): my_estimators &lt;- function(x, na.rm = FALSE) { x_mean &lt;- mean(x, na.rm = na.rm) x_sd &lt;- sd(x, na.rm = na.rm) x_cv &lt;- (x_sd / x_mean) * 100 return(c( mean = x_mean, SD = x_sd, CV = x_cv )) } If we apply this function to female heights, we get the following estimates: my_estimators(height_data$Female) #&gt; mean SD CV #&gt; 163.177916 8.199373 5.024806 Since we are interested in making statistical inference (by utilizing bootstrap method), we can simple replace bmbstats::data_estimators_simple with my_estimators: female_analysis_my_est &lt;- bmbstats::describe_data( x = height_data$Female, estimator_function = my_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) female_analysis_my_est #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 163.177916 161.200476 165.143065 #&gt; SD 8.199373 6.914061 9.136136 #&gt; CV 5.024806 4.240318 5.600490 plot(female_analysis_my_est) bmbstats::describe_data comes with three estimator functions: bmbstats::data_estimators, bmbstats::data_estimators_simple, and bmbstats::data_estimators_robust. Let’s run the bmbstats::data_estimators and bmbstats::data_estimators_robust, but this time using bca method of estimating 95% bootstrap confidence intervals (CIs): female_analysis_extensive &lt;- bmbstats::describe_data( x = height_data$Female, estimator_function = bmbstats::data_estimators, control = model_control( seed = 1667, boot_type = &quot;bca&quot;, boot_samples = 2000, confidence = 0.95 ) ) female_analysis_extensive #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 163.177916 160.913435 165.332557 #&gt; SD 8.199373 7.017929 9.874733 #&gt; CV % 5.024806 4.293769 6.039070 #&gt; median 164.003882 158.851885 165.294917 #&gt; mode 164.940229 156.333306 170.084573 #&gt; MAD 8.857033 6.340600 12.163640 #&gt; IQR 11.669487 8.447784 16.327453 #&gt; min 145.593392 NA NA #&gt; max 181.121685 175.163488 181.121685 #&gt; range 35.528292 28.764563 35.528292 plot(female_analysis_extensive) As can be seen from the bootstrap estimators distribution, some estimators, like mode, median, min, max and range have weird distribution and their CIs should not be trusted. Robust estimators is a family of estimators that are robust to outliers. Here I will use median and 10 and 20% trimmed mean. Trimming involves removing certain percentage of top and bottom observations from the sample, which removes potential outliers. female_analysis_robust &lt;- bmbstats::describe_data( x = height_data$Female, estimator_function = bmbstats::data_estimators_robust, control = model_control( seed = 1667, boot_type = &quot;bca&quot;, boot_samples = 2000, confidence = 0.95 ) ) female_analysis_robust #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; median 164.003882 158.851885 165.29492 #&gt; MAD 8.857033 6.340600 12.16364 #&gt; IQR 11.669487 8.447784 16.32745 #&gt; 10% trimmed mean 163.095407 160.630145 165.42765 #&gt; 20% trimmed mean 163.020558 160.356299 165.52687 plot(female_analysis_robust) The simplicity of the bootstrap is that it can provide CIs for any estimator you can think of. But as always, Devil is in the details and some CIs for certain estimators (or small sample sizes) cannot be trusted and can be biased. This topic is beyond this book. The easiest test you can do is to run a simulation and see if the Type I error rates are not inflated. To summarize the analysis of the single sample, let’s say that I am interested in estimating proportion of the females taller than 180cm in the population using the sample acquired. I can easily write my own estimator function and use normal or t-distribution to estimate the proportion, then plug that into the bootstrap to get CIs: # Estimators function prop_estimators &lt;- function(x, na.rm = false) { mean_x &lt;- mean(x, na.rm = na.rm) sd_x &lt;- sd(x, na.rm = na.rm) n_x &lt;- length(x) # Use t-distribution to calculate proportion over 180cm prop_t &lt;- 1 - pt((180 - mean_x) / sd_x, df = n_x - 1) # Use normal distribution to calculate proportion over 180cm prop_norm &lt;- 1 - pnorm(180, mean = mean_x, sd = sd_x) # Use `brute-force` (simple counting) to calculate proportion over 180cm prop_brute &lt;- sum(x &gt; 180) / n_x return(c( &quot;mean&quot; = mean_x, &quot;SD&quot; = sd_x, &quot;Over 180cm proportion (t-distr)&quot; = prop_t, &quot;Over 180cm proportion (norm-distr)&quot; = prop_norm, &quot;Over 180cm proportion (brute-force)&quot; = prop_brute )) } tall_females &lt;- bmbstats::describe_data( x = height_data$Female, estimator_function = prop_estimators, control = model_control( seed = 1667, boot_type = &quot;bca&quot;, boot_samples = 2000, confidence = 0.95 ) ) tall_females #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 163.17791620 1.609134e+02 165.33255732 #&gt; SD 8.19937316 7.017929e+00 9.87473324 #&gt; Over 180cm proportion (t-distr) 0.02278501 7.860132e-03 0.05965420 #&gt; Over 180cm proportion (norm-distr) 0.02010279 6.171549e-03 0.05644849 #&gt; Over 180cm proportion (brute-force) 0.02000000 0.000000e+00 0.06000000 plot(tall_females) 13.3 Visualization and analysis of the two independent groups Estimators for each independent group (e.g. males and females) can be visualized side by side. For example, we might be interested in visualizing mean, SD and proportion over 180cm for males vs. females: tall_males &lt;- bmbstats::describe_data( x = height_data$Male, estimator_function = prop_estimators, control = model_control( seed = 1667, boot_type = &quot;bca&quot;, boot_samples = 2000, confidence = 0.95 ) ) compare_groups &lt;- rbind( data.frame(group = &quot;females&quot;, tall_females$estimators), data.frame(group = &quot;males&quot;, tall_males$estimators) ) ggplot( compare_groups, aes(y = group, x = value) ) + theme_bw(8) + geom_errorbarh(aes(xmax = upper, xmin = lower), color = &quot;black&quot;, height = 0 ) + geom_point() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + facet_wrap(~estimator, scales = &quot;free_x&quot;) As can be seen from the figure, males have higher mean height, higher SD (but not sure if it is statistically significant nor practically significant - you can check this later with a few bmbstats functions) and higher proportion of individual over 180cm. Rather than comparing individual group estimates, we can perform independent group analysis using bmbstats::compare_independent_groups. But before we do that, let’s plot the groups using bmbstats::plot_raincloud function. To do that, we need to convert our wide height data to long format: height_data_long &lt;- gather( height_data, key = &quot;Gender&quot;, value = &quot;Height&quot; ) # Order factors height_data_long$Gender &lt;- factor( height_data_long$Gender, levels = c(&quot;Male&quot;, &quot;Female&quot;) ) head(height_data_long) #&gt; Gender Height #&gt; 1 Male 193.9007 #&gt; 2 Male 172.4291 #&gt; 3 Male 186.3210 #&gt; 4 Male 177.4417 #&gt; 5 Male 167.5636 #&gt; 6 Male 172.9078 And now we can plot the group height distribution: bmbstats::plot_raincloud( data = height_data_long, value = &quot;Height&quot;, group = &quot;Gender&quot; ) To perform descriptive analysis of the independent groups, we will use bmbstats::compare_independent_groups function. This function use estimator function bmbstats::independent_groups_estimators that provide all the major estimators introduced in the Comparing dependent groups section. For the SESOI we will use 2.5cm, like we have done in the Comparing dependent groups section as well: independent_groups_estimators( group_a = height_data$Female, group_b = height_data$Male, SESOI_lower = -2.5, SESOI_upper = 2.5 ) #&gt; SESOI lower SESOI upper SESOI range Mean diff #&gt; -2.50000000 2.50000000 5.00000000 12.72521903 #&gt; SD diff SD pooled %CV diff % diff #&gt; 12.41402468 8.77804103 97.55450689 7.79837084 #&gt; Ratio Cohen&#39;s d CLES OVL #&gt; 1.07798371 1.44966502 0.84733444 0.46855479 #&gt; Mean diff to SESOI SD diff to SESOI pLower pEquivalent #&gt; 2.54504381 2.48280494 0.11148343 0.09457649 #&gt; pHigher #&gt; 0.79394008 bmbstats::compare_independent_groups uses by default the bmbstats::independent_groups_estimators, but we can write our own estimators function a bit later: males_females_comp &lt;- compare_independent_groups( group_a = height_data$Female, group_b = height_data$Male, SESOI_lower = -2.5, SESOI_upper = 2.5 ) #&gt; [1] &quot;All values of t are equal to 2.5 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 5 \\n Cannot calculate confidence intervals&quot; males_females_comp #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -2.50000000 NA NA #&gt; SESOI upper 2.50000000 NA NA #&gt; SESOI range 5.00000000 NA NA #&gt; Mean diff 12.72521903 9.08380753 16.0686119 #&gt; SD diff 12.41402468 11.10081268 14.1287776 #&gt; SD pooled 8.77804103 7.84945992 9.9905544 #&gt; %CV diff 97.55450689 75.67596790 145.1388909 #&gt; % diff 7.79837084 5.48856008 9.9411055 #&gt; Ratio 1.07798371 1.05488560 1.0994111 #&gt; Cohen&#39;s d 1.44966502 0.97452326 1.8701976 #&gt; CLES 0.84733444 0.75460141 0.9069431 #&gt; OVL 0.46855479 0.34974837 0.6260860 #&gt; Mean diff to SESOI 2.54504381 1.81676151 3.2137224 #&gt; SD diff to SESOI 2.48280494 2.22016254 2.8257555 #&gt; pLower 0.11148343 0.06449905 0.1926671 #&gt; pEquivalent 0.09457649 0.06862640 0.1250760 #&gt; pHigher 0.79394008 0.68781913 0.8655902 plot(males_females_comp) You can notice that SESOI threshold doesn’t have any bootstrap distribution. That is because we have provide a priori SESOI. We can also estimate SESOI within the bootstrap loop. For SESOI we can use pooled SD of the group_a and group_b multiplied by 0.2, which represents Cohen’s trivial magnitude. This is the default behavior of the bmbstats::compare_independent_groups and other similar functions. You can write your own function for estimating SESOI by providing function argument to SESOI_lower and SESOI_upper parameters. For the sake of example, I will do that here, but only for the SESOI_upper and will stick to -2.5cm for the SESOI_lower: males_females_comp_est &lt;- compare_independent_groups( group_a = height_data$Female, group_b = height_data$Male, SESOI_lower = -2.5, SESOI_upper = function(group_a, group_b, na.rm) { sd(group_a, na.rm = na.rm) * 0.2 } ) males_females_comp_est #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -2.50000000 NA NA #&gt; SESOI upper 1.63987463 1.40101324 1.94141198 #&gt; SESOI range 4.13987463 3.90101324 4.44141198 #&gt; Mean diff 12.72521903 9.39979860 16.07945845 #&gt; SD diff 12.41402468 11.12171677 14.08901524 #&gt; SD pooled 8.77804103 7.86424134 9.96243822 #&gt; %CV diff 97.55450689 74.22144558 135.97174823 #&gt; % diff 7.79837084 5.72302737 9.95978160 #&gt; Ratio 1.07798371 1.05723027 1.09959782 #&gt; Cohen&#39;s d 1.44966502 1.04057626 1.90567697 #&gt; CLES 0.84733444 0.76901630 0.91107613 #&gt; OVL 0.46855479 0.34069164 0.60291087 #&gt; Mean diff to SESOI 3.07381749 2.26756237 3.95663372 #&gt; SD diff to SESOI 2.99864749 2.72442801 3.34078108 #&gt; pLower 0.11148343 0.06178746 0.17758243 #&gt; pEquivalent 0.07554712 0.05221579 0.09808881 #&gt; pHigher 0.81296945 0.72639255 0.88497505 plot(males_females_comp_est) You can now notice that SESOI upper and SESOI range have bootstrap distribution. It is important that if we estimate SESOI from the obtained sample, the SESOI estimation must be in the bootstrap loop, and our uncertainty about it’s estimate must be propagated to other estimators that uses SESOI (e.g. pLower, pEquivalent, pHigher). To demonstrate the difference, consider the following two analyses using different SESOI estimation. One estimates SESOI inside the bootstrap loop, and the other estimates SESOI outside the bootstrap loop. To make sure the same bootstrap is performed, we will set the same seed parameter: # SESOI estimated inside the bootstrap loop males_females_comp_inside &lt;- compare_independent_groups( group_a = height_data$Female, group_b = height_data$Male, SESOI_lower = function(group_a, group_b, na.rm) { -sd(group_a, na.rm = na.rm) * 0.2 }, SESOI_upper = function(group_a, group_b, na.rm) { sd(group_a, na.rm = na.rm) * 0.2 }, control = model_control(seed = 1667) ) # SESOI estimated outside the bootstrap loop males_females_comp_outside &lt;- compare_independent_groups( group_a = height_data$Female, group_b = height_data$Male, SESOI_lower = -sd(height_data$Female) * 0.2, SESOI_upper = sd(height_data$Female) * 0.2, control = model_control(seed = 1667) ) #&gt; [1] &quot;All values of t are equal to 1.63987463256618 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 3.27974926513235 \\n Cannot calculate confidence intervals&quot; # Plot the estimators compare_analyses &lt;- rbind( data.frame(group = &quot;inside&quot;, males_females_comp_inside$estimators), data.frame(group = &quot;outside&quot;, males_females_comp_outside$estimators) ) ggplot( compare_analyses, aes(y = group, x = value) ) + theme_bw(8) + geom_errorbarh(aes(xmax = upper, xmin = lower), color = &quot;black&quot;, height = 0 ) + geom_point() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + facet_wrap(~estimator, scales = &quot;free_x&quot;) The difference in CIs for the estimators that uses SESOI (i.e. magnitude-based estimators) is not staggering, but CIs are wider for the inside method (for example, compare Mean diff to SESOI estimator). 13.4 NHST, METs and MBI functions Is there are statistically significant difference in the mean height between males and females? As explained in the Statistical inference and Bootstrap chapters, we can derive p-value using bootstrap resamples. Let’s test the Null Hypothesis that mean difference between males and females is 0. To do that we will used bmbstats::bootstrap_NHST function that demands us to select estimator of interest using estimator parameter. bmbstats::bootstrap_NHST uses the result object from the bmbstats::compare_independent_groups function: males_females_NHST &lt;- bmbstats::bootstrap_NHST( males_females_comp, estimator = &quot;Mean diff&quot;, null_hypothesis = 0, test = &quot;two.sided&quot; ) males_females_NHST #&gt; Null-hypothesis significance test for the `Mean diff` estimator #&gt; Bootstrap result: Mean diff=12.725, 95% CI [9.084, 16.069] #&gt; H0=0, test: two.sided #&gt; p=0.000499750124937531 As can be seen, the p-value is below 0.05 (i.e. alpha), so we can conclude that the mean difference is statistically significant. What if our hypothesis is that males are taller by females by at least 10cm? We can also test that hypothesis using one-sided NHST: males_females_NHST &lt;- bmbstats::bootstrap_NHST( males_females_comp, estimator = &quot;Mean diff&quot;, null_hypothesis = 10, test = &quot;greater&quot; ) males_females_NHST #&gt; Null-hypothesis significance test for the `Mean diff` estimator #&gt; Bootstrap result: Mean diff=12.725, 95% CI [9.084, 16.069] #&gt; H0=10, test: greater #&gt; p=0.0535 Estimated p-value is slightly over 0.05, so we do not reject the Null Hypothesis. This is interesting result, particularly since we know that the true mean difference is around 12.7cm (true mean height for males is 177.8cm and for females is 165.1cm). It could be that this sample size is under-powered to detect this small difference between Null Hypothesis (i.e. &gt;10cm) and Alternative Hypothesis - true difference in this case - of 12.7cm. This topic is discussed in the Statistical Power. This could easily be tested with a simulation, althought that is beyond the scope of this chapter. Graphically, this test looks like this: plot(males_females_NHST) In the above graph, the estimator bootstrap distribution (i.e. mean difference) is centered around Null Hypothesis (i.e. 10cm). If we perform this same test, but using two sided NHST (which is default), we will the the following result and plot: males_females_NHST &lt;- bmbstats::bootstrap_NHST( males_females_comp, estimator = &quot;Mean diff&quot;, null_hypothesis = 10 ) plot(males_females_NHST) We have already decided that our SESOI in height is 2.5cm. But this SESOI is related to individual observations, not necessary to estimators (i.e. mean difference). This topic is discussed in the Individual vs. Parameter SESOI section of this book. Let’s use 10cm as SESOI for the mean difference estimator and perform METs using bmbstats::bootstrap_MET function and alpha level set to 0.05: males_females_MET &lt;- bmbstats::bootstrap_MET( males_females_comp, estimator = &quot;Mean diff&quot;, SESOI_lower = -10, SESOI_upper = 10, alpha = 0.05 ) males_females_MET #&gt; Minimum effect tests for the `Mean diff` estimator #&gt; Bootstrap result: Mean diff=12.725, 95% CI [9.084, 16.069] #&gt; SESOI: [-10, 10], alpha=0.05 #&gt; #&gt; Test p.value #&gt; inferiority 1.0000000000 #&gt; non-superiority 0.9465000000 #&gt; equivalence 0.9465000000 #&gt; non-inferiority 0.0004997501 #&gt; superiority 0.0535000000 #&gt; #&gt; Final inference: Not-Lower As can be seen from the result of the MET analysis, the final inference is that male height is “not lower” than female height using SESOI of 10cm. I personally prefer this to be conveyed visually by using plot function: plot(males_females_MET) We can also plot each individual MET test, for example equivalence and non-inferiority tests: plot(males_females_MET, type = &quot;equivalence&quot;) plot(males_females_MET, type = &quot;non-inferiority&quot;) What if I decide about different SESOI values or different Null Hypothesis or even alpha levels? Well, that is and example of p-harking discussed in the Statistical Power section. P-harking represents hypothesizing after results are known, or in other words tuning your analysis to be more acceptable for publications. That’s why it is important for the confirmatory studies to have all the threshold and the analysis a priori defined or pre-registered. To perform MBI, use bmbstats::bootstrap_MBI function: males_females_MBI &lt;- bmbstats::bootstrap_MBI( males_females_comp, estimator = &quot;Mean diff&quot;, SESOI_lower = -10, SESOI_upper = 10 ) males_females_MBI #&gt; Magnitude-based inference for the `Mean diff` estimator #&gt; Bootstrap result: Mean diff=12.725, 95% CI [9.084, 16.069] #&gt; SESOI: [-10, 10] #&gt; #&gt; Test prob #&gt; lower 0.00 #&gt; equivalent 0.06 #&gt; higher 0.94 #&gt; #&gt; Final inference: Likely higher The final inference of the MBI is “likely higher” mean difference. As always, plotting is much more informational: plot(males_females_MBI) 13.5 Comparing two dependent groups 13.5.1 Measurement error issues In the Description chapter, to showcase comparison between two dependent groups, bench press data involving Pre-test and Post-test observations were used. Let’s create that data-set here. To demonstrate measurement error effects, on top of true Pre- and Post- 1RM test scores, I will add measurement error that is normally distributed with SD equal to 2.5kg. In the first example, there will be no systematic nor random change between Pre-test and Post-test, thus measurement error will be solely responsible for the observed change (although there will be no true change). set.seed(1666) n_subjects &lt;- 20 measurement_error &lt;- 2.5 systematic_change &lt;- 0 random_change &lt;- 0 bench_press_data &lt;- tibble( # Generate athlete name Athlete = factor(paste( &quot;Athlete&quot;, str_pad( string = seq(1, n_subjects), width = 2, pad = &quot;0&quot; ) )), # True Pre-test `Pre-test (true)` = rnorm( n = n_subjects, mean = 100, sd = 7.5 ), # True Change `Change (true)` = rnorm( n = n_subjects, mean = systematic_change, sd = random_change ), # True Post-test `Post-test (true)` = `Pre-test (true)` + `Change (true)`, # Observed Pre-test `Pre-test (observed)` = `Pre-test (true)` + # Add measurement error rnorm( n = n_subjects, mean = 0, sd = measurement_error ), # Observed Post-test `Post-test (observed)` = `Post-test (true)` + # Add measurement error rnorm( n = n_subjects, mean = 0, sd = measurement_error ), # Observed Change `Change (observed)` = `Post-test (observed)` - `Pre-test (observed)` ) bench_press_data #&gt; # A tibble: 20 x 7 #&gt; Athlete `Pre-test (true… `Change (true)` `Post-test (tru… `Pre-test (obse… #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Athlet… 111. 0 111. 112. #&gt; 2 Athlet… 102. 0 102. 108. #&gt; 3 Athlet… 93.4 0 93.4 90.8 #&gt; 4 Athlet… 95.4 0 95.4 93.2 #&gt; 5 Athlet… 111. 0 111. 111. #&gt; 6 Athlet… 110. 0 110. 109. #&gt; 7 Athlet… 104. 0 104. 105. #&gt; 8 Athlet… 93.7 0 93.7 94.5 #&gt; 9 Athlet… 99.6 0 99.6 102. #&gt; 10 Athlet… 106. 0 106. 109. #&gt; 11 Athlet… 102. 0 102. 105. #&gt; 12 Athlet… 101. 0 101. 100. #&gt; 13 Athlet… 92.9 0 92.9 90.5 #&gt; 14 Athlet… 98.2 0 98.2 96.5 #&gt; 15 Athlet… 88.3 0 88.3 89.7 #&gt; 16 Athlet… 106. 0 106. 109. #&gt; 17 Athlet… 95.8 0 95.8 98.2 #&gt; 18 Athlet… 92.9 0 92.9 90.8 #&gt; 19 Athlet… 103. 0 103. 104. #&gt; 20 Athlet… 104. 0 104. 107. #&gt; # … with 2 more variables: `Post-test (observed)` &lt;dbl&gt;, `Change #&gt; # (observed)` &lt;dbl&gt; Let’s plot the true Pre-test and Post-test scores using a scatter plot and SESOI band of -5 to 5kg: plot_pair_changes( group_a = bench_press_data$`Pre-test (true)`, group_b = bench_press_data$`Post-test (true)`, group_a_label = &quot;True Pre-test&quot;, group_b_label = &quot;True Post-test&quot;, SESOI_lower = -5, SESOI_upper = 5 ) As can be seen, there is no true change. Let’s see what happens when we plot observed scores: plot_pair_changes( group_a = bench_press_data$`Pre-test (observed)`, group_b = bench_press_data$`Post-test (observed)`, group_a_label = &quot;Observed Pre-test&quot;, group_b_label = &quot;Observed Post-test&quot;, SESOI_lower = -5, SESOI_upper = 5 ) We can also plot distribution of the Change scores: plot_raincloud_SESOI( bench_press_data, value = &quot;Change (observed)&quot;, SESOI_lower = -5, SESOI_upper = 5 ) We would be very quick to claim that there are individuals that demonstrated higher or lower change (compared to SESOI). But remember that in this data set there is not true change - implying that all observed change is due to measurement error. Since we know that there is no true change, let’s do the summary of the observed Change score: mean(bench_press_data$`Change (observed)`) #&gt; [1] 0.1114846 sd(bench_press_data$`Change (observed)`) #&gt; [1] 3.699529 Since we know that the measurement error is 2.5kg, the SD of the change score is expected to be \\(2.5 \\times \\sqrt{2}\\) or 3.54kg. Using bmbstats::describe_data we can perform bootstrap CIs for the mean and SD of the Change score: obs_change_analysis &lt;- bmbstats::describe_data( x = bench_press_data$`Change (observed)`, estimator_function = bmbstats::data_estimators_simple, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) obs_change_analysis #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 0.1114846 -1.397619 1.360018 #&gt; SD 3.6995292 2.405751 4.775318 The 90% CIs for the SD of the observed Change score captures expected change measurement error of 3.54kg. We will come back to this in the Reliability section. Since we are aware of the measurement error involved in out observations, we can perform MBI and MET of the observed change scores. MBI can be performed using bmbstats::observations_MBI function. If we plot the result, confidence intervals (error bars) represent SDC (smallest detectable change), which is measurement error multiplied with appropriate critical value to get desired confidence level. On top of plotting MBI, I will also plot the true score using true_observations of the plot function (indicated by red line): obs_change_MBI &lt;- bmbstats::observations_MBI( observations = bench_press_data$`Change (observed)`, observations_label = bench_press_data$Athlete, measurement_error = 2.5 * sqrt(2), # Degrees of freedom from the reliability study. Use `Inf` for normal distribution df = Inf, SESOI_lower = -5, SESOI_upper = 5, confidence = 0.9 ) plot( obs_change_MBI, true_observations = bench_press_data$`Change (true)`, control = plot_control(points_size = 5) ) + xlim(-18, 20) To perform METs, use bmbstats::observations_MET function: obs_change_MET &lt;- bmbstats::observations_MET( observations = bench_press_data$`Change (observed)`, observations_label = bench_press_data$Athlete, measurement_error = 2.5 * sqrt(2), # Degrees of freedom from the reliability study. Use `Inf` for normal distribution df = Inf, SESOI_lower = -5, SESOI_upper = 5, alpha = 0.05, confidence = 0.9 ) plot( obs_change_MET, true_observations = bench_press_data$`Change (true)`, control = plot_control(points_size = 5) ) + xlim(-18, 20) It seems like the Athlete 02 showed true Change, but since we generated the data we know that there is no true Change (also check the vertical red lines for the true Change scores). Thus, if we conclude that this individual showed lower change, we would be making Type I error. Since we are performing multiple individual tests, we could/should adjust the alpha parameter (e.g. by dividing it by number of tests, or in this case athletes - Bonferroni correction) to avoid inflating family-wise error rates, since particular athlete can show significant change due to chance alone (due to multiple comparisons/test). To do that, simply divide alpha by the number of athletes: obs_change_MET &lt;- bmbstats::observations_MET( observations = bench_press_data$`Change (observed)`, observations_label = bench_press_data$Athlete, measurement_error = 2.5 * sqrt(2), # Degrees of freedom from the reliability study. Use `Inf` for normal distribution df = Inf, SESOI_lower = -5, SESOI_upper = 5, alpha = 0.05 / n_subjects, # Confidence could be adjusted as well # but it is used mainly for plotting confidence = 0.9 ) plot( obs_change_MET, true_observations = bench_press_data$`Change (true)`, control = plot_control(points_size = 5) ) + xlim(-18, 20) The point of this analysis is that we need to know measurement error to infer about true change in individuals. Since we do know that there is no real change in this example, we can see how measurement error cause cause wrong inferences about the true changes. Let’s now generate the data with true changes, where systematic change is 10kg and random change is 10kg as well: set.seed(1666) n_subjects &lt;- 20 measurement_error &lt;- 2.5 systematic_change &lt;- 10 random_change &lt;- 10 bench_press_data &lt;- tibble( # Generate athlete name Athlete = factor(paste( &quot;Athlete&quot;, str_pad( string = seq(1, n_subjects), width = 2, pad = &quot;0&quot; ) )), # True Pre-test `Pre-test (true)` = rnorm( n = n_subjects, mean = 100, sd = 7.5 ), # True Change `Change (true)` = rnorm( n = n_subjects, mean = systematic_change, sd = random_change ), # True Post-test `Post-test (true)` = `Pre-test (true)` + `Change (true)`, # Observed Pre-test `Pre-test (observed)` = `Pre-test (true)` + # Add measurement error rnorm( n = n_subjects, mean = 0, sd = measurement_error ), # Observed Post-test `Post-test (observed)` = `Post-test (true)` + # Add measurement error rnorm( n = n_subjects, mean = 0, sd = measurement_error ), # Observed Change `Change (observed)` = `Post-test (observed)` - `Pre-test (observed)` ) bench_press_data #&gt; # A tibble: 20 x 7 #&gt; Athlete `Pre-test (true… `Change (true)` `Post-test (tru… `Pre-test (obse… #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Athlet… 111. 13.8 125. 112. #&gt; 2 Athlet… 102. 34.3 136. 97.7 #&gt; 3 Athlet… 93.4 -0.428 93.0 92.0 #&gt; 4 Athlet… 95.4 1.27 96.7 94.0 #&gt; 5 Athlet… 111. 9.65 120. 110. #&gt; 6 Athlet… 110. 6.41 116. 111. #&gt; 7 Athlet… 104. 13.3 118. 110. #&gt; 8 Athlet… 93.7 13.4 107. 93.0 #&gt; 9 Athlet… 99.6 19.3 119. 99.8 #&gt; 10 Athlet… 106. 22.1 129. 106. #&gt; 11 Athlet… 102. 23.1 125. 103. #&gt; 12 Athlet… 101. 5.59 107. 102. #&gt; 13 Athlet… 92.9 0.190 93.1 96.2 #&gt; 14 Athlet… 98.2 3.17 101. 100. #&gt; 15 Athlet… 88.3 15.5 104. 92.5 #&gt; 16 Athlet… 106. 21.1 127. 111. #&gt; 17 Athlet… 95.8 19.6 115. 94.1 #&gt; 18 Athlet… 92.9 1.54 94.4 87.9 #&gt; 19 Athlet… 103. 14.6 117. 107. #&gt; 20 Athlet… 104. 19.1 123. 107. #&gt; # … with 2 more variables: `Post-test (observed)` &lt;dbl&gt;, `Change #&gt; # (observed)` &lt;dbl&gt; Let’s plot true scores: plot_pair_changes( group_a = bench_press_data$`Pre-test (true)`, group_b = bench_press_data$`Post-test (true)`, group_a_label = &quot;True Pre-test&quot;, group_b_label = &quot;True Post-test&quot;, SESOI_lower = -5, SESOI_upper = 5 ) Since there are true changes (systematic and random) in this DGP, estimating mean and SD fo the true Change scores will give us the estimate of the DGP parameters: mean(bench_press_data$`Change (true)`) #&gt; [1] 12.82946 sd(bench_press_data$`Change (true)`) #&gt; [1] 9.328114 To get bootstrap CI around these estimate, we can again use bmbstats::describe_data function: true_change_analysis &lt;- bmbstats::describe_data( x = bench_press_data$`Change (true)`, estimator_function = bmbstats::data_estimators_simple, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) true_change_analysis #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 12.829463 9.402614 16.51982 #&gt; SD 9.328114 6.781652 11.06663 The true DGP parameters (systematic effect of 10kg and random effect of 10kg) are captured with estimated CIs. Let’s turn to observed scores: plot_pair_changes( group_a = bench_press_data$`Pre-test (observed)`, group_b = bench_press_data$`Post-test (observed)`, group_a_label = &quot;Observed Pre-test&quot;, group_b_label = &quot;Observed Post-test&quot;, SESOI_lower = -5, SESOI_upper = 5 ) The image looks similar to true scores analysis. Let’s estimate mean and SD CIs: obs_change_analysis &lt;- bmbstats::describe_data( x = bench_press_data$`Change (observed)`, estimator_function = bmbstats::data_estimators_simple, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) obs_change_analysis #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; mean 13.26635 9.829395 17.26567 #&gt; SD 10.32696 7.533166 12.40584 As expected, the SD of the observed Change score is more inflated (than SD of the true Change score) due to measurement error. Before dealing with this issue, let’s plot MBI and MET analysis results: obs_change_MBI &lt;- bmbstats::observations_MBI( observations = bench_press_data$`Change (observed)`, observations_label = bench_press_data$Athlete, measurement_error = 2.5 * sqrt(2), # Degrees of freedom from the reliability study. Use `Inf` for normal distribution df = Inf, SESOI_lower = -5, SESOI_upper = 5, confidence = 0.9 ) plot( obs_change_MBI, true_observations = bench_press_data$`Change (true)`, control = plot_control(points_size = 5) ) + xlim(-10, 65) obs_change_MET &lt;- bmbstats::observations_MET( observations = bench_press_data$`Change (observed)`, observations_label = bench_press_data$Athlete, measurement_error = 2.5 * sqrt(2), # Degrees of freedom from the reliability study. Use `Inf` for normal distribution df = Inf, SESOI_lower = -5, SESOI_upper = 5, alpha = 0.05 / n_subjects, # Will adjust CI for plotting as well confidence = 1 - (0.05 / n_subjects) * 2 ) plot( obs_change_MET, true_observations = bench_press_data$`Change (true)`, control = plot_control(points_size = 5) ) + xlim(-15, 65) Before jumping on the responders vs. non-responders bandwagon, it would be wise to check the statistical error committed by Dankel and Loenneke (Dankel and Loenneke 2019) pointed out in the letter-to-the-editor by Tenan et al. (Tenan, Vigotsky, and Caldwell, n.d.). This is a lesson to us all who are trying to come up with a novel analyses, like myself, so I am trying to be very cautious in using any bold statements. 13.5.2 Analysis of the dependent groups using bmbstats::compare_dependent_groups To perform dependent group comparison, we will use bmbstats::compare_dependent_groups function, and bmbstats::dependent_groups_estimators estimator function. Let’s first do it with the true Pre- and Post- scores: true_pre_post &lt;- bmbstats::compare_dependent_groups( pre = bench_press_data$`Pre-test (true)`, post = bench_press_data$`Post-test (true)`, SESOI_lower = -5, SESOI_upper = 5, estimator_function = bmbstats::dependent_groups_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) #&gt; [1] &quot;All values of t are equal to 5 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 10 \\n Cannot calculate confidence intervals&quot; true_pre_post #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -5.00000000 -5.000000000 -5.00000000 #&gt; SESOI upper 5.00000000 NA NA #&gt; SESOI range 10.00000000 NA NA #&gt; Mean change 12.82946286 9.402614340 16.51981871 #&gt; SD change 9.32811444 6.781651565 11.06663336 #&gt; %CV change 72.70853459 49.485375080 95.21126362 #&gt; % change 12.62276799 9.213352403 16.25417940 #&gt; Ratio 1.12622768 1.092133524 1.16254179 #&gt; Cohen&#39;s d 1.95364779 1.371167791 3.03268427 #&gt; CLES 0.81092721 0.738420828 0.90342409 #&gt; OVL 0.32865634 0.129433313 0.49297559 #&gt; Mean change to SESOI 1.28294629 0.940261434 1.65198187 #&gt; SD change to SESOI 0.93281144 0.678165157 1.10666334 #&gt; pLower 0.03558066 0.007004211 0.06858572 #&gt; pEquivalent 0.17027690 0.087087118 0.24817063 #&gt; pHigher 0.79414243 0.691105965 0.90546802 plot(true_pre_post) And now with the observed Pre- and Post- scores: obs_pre_post &lt;- bmbstats::compare_dependent_groups( pre = bench_press_data$`Pre-test (observed)`, post = bench_press_data$`Post-test (observed)`, SESOI_lower = -5, SESOI_upper = 5, estimator_function = bmbstats::dependent_groups_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) #&gt; [1] &quot;All values of t are equal to 5 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 10 \\n Cannot calculate confidence intervals&quot; obs_pre_post #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -5.00000000 -5.00000000 -5.00000000 #&gt; SESOI upper 5.00000000 NA NA #&gt; SESOI range 10.00000000 NA NA #&gt; Mean change 13.26634977 9.82939542 17.26567091 #&gt; SD change 10.32696439 7.53316566 12.40584025 #&gt; %CV change 77.84329951 55.95245477 97.95459056 #&gt; % change 13.07536952 9.64149847 17.10620410 #&gt; Ratio 1.13075370 1.09641498 1.17106204 #&gt; Cohen&#39;s d 1.73487556 1.23093674 2.63074534 #&gt; CLES 0.80381999 0.73779303 0.88834207 #&gt; OVL 0.38570219 0.18838477 0.53824551 #&gt; Mean change to SESOI 1.32663498 0.98293954 1.72656709 #&gt; SD change to SESOI 1.03269644 0.75331657 1.24058402 #&gt; pLower 0.04648926 0.01380798 0.07714882 #&gt; pEquivalent 0.17017990 0.09737292 0.23894195 #&gt; pHigher 0.78333084 0.69453555 0.88551971 plot(obs_pre_post) Let’s plot the estimated CIs for all the estimators: # Plot the estimators compare_analyses &lt;- rbind( data.frame(group = &quot;true&quot;, true_pre_post$estimators), data.frame(group = &quot;observed&quot;, obs_pre_post$estimators) ) ggplot( compare_analyses, aes(y = group, x = value) ) + theme_bw(8) + geom_errorbarh(aes(xmax = upper, xmin = lower), color = &quot;black&quot;, height = 0 ) + geom_point() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + facet_wrap(~estimator, scales = &quot;free_x&quot;) As can be seen on the figure, some estimators (those depending on the SD) are more affected by the measurement error. Since we do not know true scores, we can perform SIMEX analysis on the observed scores, or adjust SD of the change using the change measurement error. Let’s do that by writing our own estimator equation that uses adjustment for the change SD: adjusted_estimators &lt;- function(pre, post, SESOI_lower = 0, SESOI_upper = 0, na.rm = FALSE) { SESOI_range &lt;- SESOI_upper - SESOI_lower change &lt;- post - pre mean_change &lt;- mean(change, na.rm = na.rm) sd_change &lt;- stats::sd(change, na.rm = na.rm) # Now we adjust the sd_change with the known measurement error change_measurement_error &lt;- measurement_error * sqrt(2) sd_change &lt;- sqrt(sd_change^2 - change_measurement_error^2) cv_change &lt;- 100 * sd_change / mean_change perc_change &lt;- mean(change / pre, na.rm = na.rm) * 100 ratio &lt;- mean(post / pre, na.rm = na.rm) cohen &lt;- cohens_d(pre, post, paired = TRUE, na.rm = na.rm) cles &lt;- CLES(pre, post, na.rm = na.rm) ovl &lt;- 2 * stats::pnorm(-abs(cohen) / 2) change_to_SESOI &lt;- mean_change / SESOI_range sd_change_to_SESOI &lt;- sd_change / SESOI_range # Calculate proportion of scores df &lt;- length(change) - 1 higher &lt;- 1 - stats::pt((SESOI_upper - mean_change) / sd_change, df = df) lower &lt;- stats::pt((SESOI_lower - mean_change) / sd_change, df = df) equivalent &lt;- 1 - (higher + lower) c( &quot;SESOI lower&quot; = SESOI_lower, &quot;SESOI upper&quot; = SESOI_upper, &quot;SESOI range&quot; = SESOI_range, &quot;Mean change&quot; = mean_change, &quot;SD change&quot; = sd_change, &quot;%CV change&quot; = cv_change, &quot;% change&quot; = perc_change, &quot;Ratio&quot; = ratio, &quot;Cohen&#39;s d&quot; = cohen, &quot;CLES&quot; = cles, &quot;OVL&quot; = ovl, &quot;Mean change to SESOI&quot; = change_to_SESOI, &quot;SD change to SESOI&quot; = sd_change_to_SESOI, &quot;pLower&quot; = lower, &quot;pEquivalent&quot; = equivalent, &quot;pHigher&quot; = higher ) } # ---------------------------------- adj_pre_post &lt;- bmbstats::compare_dependent_groups( pre = bench_press_data$`Pre-test (observed)`, post = bench_press_data$`Post-test (observed)`, SESOI_lower = -5, SESOI_upper = 5, estimator_function = adjusted_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) #&gt; [1] &quot;All values of t are equal to 5 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 10 \\n Cannot calculate confidence intervals&quot; adj_pre_post #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -5.00000000 -5.000000000 -5.00000000 #&gt; SESOI upper 5.00000000 NA NA #&gt; SESOI range 10.00000000 NA NA #&gt; Mean change 13.26634977 9.829395423 17.26567091 #&gt; SD change 9.70289614 6.651960703 11.89137806 #&gt; %CV change 73.13915511 50.327816706 93.01853927 #&gt; % change 13.07536952 9.641498467 17.10620410 #&gt; Ratio 1.13075370 1.096414985 1.17106204 #&gt; Cohen&#39;s d 1.73487556 1.230936741 2.63074534 #&gt; CLES 0.80381999 0.737793033 0.88834207 #&gt; OVL 0.38570219 0.188384768 0.53824551 #&gt; Mean change to SESOI 1.32663498 0.982939542 1.72656709 #&gt; SD change to SESOI 0.97028961 0.665196070 1.18913781 #&gt; pLower 0.03758233 0.007977625 0.06801839 #&gt; pEquivalent 0.16484497 0.085648118 0.23506532 #&gt; pHigher 0.79757270 0.708039046 0.90137633 Now we can add these estimated CI to the graph and compare it with estimates using true and observed scores: # Plot the estimators compare_analyses &lt;- rbind( data.frame(group = &quot;true&quot;, true_pre_post$estimators), data.frame(group = &quot;observed&quot;, obs_pre_post$estimators), data.frame(group = &quot;adjusted&quot;, adj_pre_post$estimators) ) ggplot( compare_analyses, aes(y = group, x = value) ) + theme_bw(8) + geom_errorbarh(aes(xmax = upper, xmin = lower), color = &quot;black&quot;, height = 0 ) + geom_point() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + facet_wrap(~estimator, scales = &quot;free_x&quot;) To be fair, some estimators like Cohen's d and those depending on it and Pre-test SD were not adjusted (which we can do that too as well), but SD changeand other estimators dependent on that it we adjusted and much closer to the estimates using the true scores. As explained in the What to do when we know the error? section, SIMEX procedure can be implemented as well. This simple example demonstrates the effect of the measurement error on the estimators and a simple adjustment that could be done to come closer to estimators using true scores for the dependent group analysis. This is very similar to the RCT analysis, where SD of the Control group change scores will be used instead of known measurement error. 13.5.3 Statistical tests For the sake of example, let’s perform NHST using the mean change estimator estimated using the observed scores: pre_vs_post_NHST &lt;- bmbstats::bootstrap_NHST( obs_pre_post, estimator = &quot;Mean change&quot;, null_hypothesis = 0, test = &quot;two.sided&quot; ) pre_vs_post_NHST #&gt; Null-hypothesis significance test for the `Mean change` estimator #&gt; Bootstrap result: Mean change=13.266, 90% CI [9.829, 17.266] #&gt; H0=0, test: two.sided #&gt; p=0.000999000999000999 plot(pre_vs_post_NHST) Using SESOI of ±5kg for the mean change estimator as well, let’s do the METs: pre_vs_post_MET &lt;- bmbstats::bootstrap_MET( obs_pre_post, estimator = &quot;Mean change&quot;, SESOI_lower = -5, SESOI_upper = 5, alpha = 0.05 ) pre_vs_post_MET #&gt; Minimum effect tests for the `Mean change` estimator #&gt; Bootstrap result: Mean change=13.266, 90% CI [9.829, 17.266] #&gt; SESOI: [-5, 5], alpha=0.05 #&gt; #&gt; Test p.value #&gt; inferiority 1.000000000 #&gt; non-superiority 0.999000000 #&gt; equivalence 0.999000000 #&gt; non-inferiority 0.000999001 #&gt; superiority 0.001000000 #&gt; #&gt; Final inference: Higher plot(pre_vs_post_MET) And finally MBI: pre_vs_post_MBI &lt;- bmbstats::bootstrap_MBI( obs_pre_post, estimator = &quot;Mean change&quot;, SESOI_lower = -5, SESOI_upper = 5 ) pre_vs_post_MBI #&gt; Magnitude-based inference for the `Mean change` estimator #&gt; Bootstrap result: Mean change=13.266, 90% CI [9.829, 17.266] #&gt; SESOI: [-5, 5] #&gt; #&gt; Test prob #&gt; lower 0 #&gt; equivalent 0 #&gt; higher 1 #&gt; #&gt; Final inference: Almost certainly higher plot(pre_vs_post_MBI) 13.6 Describing relationship between two groups In Describing relationship between two variables section we have used a relationship between YoYoIR1 and MAS. This is how that data is generated, but without rounding (i.e. YoYoIR1 should be rounded to 40m and MAS to 0.5km/h): set.seed(1667) n_subjects &lt;- 30 yoyo_mas_data &lt;- tibble( Athlete = paste( &quot;Athlete&quot;, str_pad( string = seq(1, n_subjects), width = 2, pad = &quot;0&quot; ) ), `YoYoIR1` = rnorm( n = n_subjects, mean = 1224, sd = 255 ), `MAS` = 3.6 * (0.456 * `YoYoIR1` / 1000 + 3.617) + rnorm(n = length(`YoYoIR1`), 0, 0.2) ) yoyo_mas_data #&gt; # A tibble: 30 x 3 #&gt; Athlete YoYoIR1 MAS #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Athlete 01 1628. 15.4 #&gt; 2 Athlete 02 1089. 15.0 #&gt; 3 Athlete 03 1438. 15.2 #&gt; 4 Athlete 04 1215. 15.0 #&gt; 5 Athlete 05 967. 14.3 #&gt; 6 Athlete 06 1101. 14.9 #&gt; 7 Athlete 07 1014. 14.5 #&gt; 8 Athlete 08 1424. 15.2 #&gt; 9 Athlete 09 633. 14.1 #&gt; 10 Athlete 10 1348. 14.9 #&gt; # … with 20 more rows Let’s create a scatter plot with linear regression model using bmbstats::plot_pair_lm, with YoYoIR1 being outcome variable and MAS being predictor, with SESOI being ±40m bmbstats::plot_pair_lm( predictor = yoyo_mas_data$MAS, outcome = yoyo_mas_data$YoYoIR1, predictor_label = &quot;MAS&quot;, outcome_label = &quot;YoYoIR1&quot;, SESOI_lower = -40, SESOI_upper = 40 ) To get bootstrapped CIs of the estimators, use bmbstats::describe_relationship and bmbstats::relationship_lm_estimators functions: boot_relationship &lt;- bmbstats::describe_relationship( predictor = yoyo_mas_data$MAS, outcome = yoyo_mas_data$YoYoIR1, SESOI_lower = -40, SESOI_upper = 40, estimator_function = bmbstats::relationship_lm_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) #&gt; [1] &quot;All values of t are equal to 40 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 80 \\n Cannot calculate confidence intervals&quot; boot_relationship #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -40.0000000 -40.0000000 -40.0000000 #&gt; SESOI upper 40.0000000 NA NA #&gt; SESOI range 80.0000000 NA NA #&gt; Intercept -7452.2295288 -8337.2111534 -6549.1633775 #&gt; Slope 580.8003907 519.2080676 640.5107552 #&gt; RSE 94.5402638 77.0626151 106.8726878 #&gt; Pearson&#39;s r 0.9314150 0.8828261 0.9597758 #&gt; R Squared 0.8675339 0.7793820 0.9211695 #&gt; SESOI to RSE 0.8462003 0.7485542 1.0381170 #&gt; PPER 0.3246566 0.2890790 0.3923412 plot(boot_relationship) Magnitude-based estimators SESOI to RSE and PPER are useful in judging practical significance of this model, which is this case very bad. For example, if we now use MAS as outcome and YoYoIR1 as predictor with SESOI equal to ±0.5kmh, R Squared and Pearson's r will stay the same, but SESOI to RSE and PPER will demonstrate model that now had much better practical significance: boot_relationship &lt;- bmbstats::describe_relationship( outcome = yoyo_mas_data$MAS, predictor = yoyo_mas_data$YoYoIR1, SESOI_lower = -0.5, SESOI_upper = 0.5, estimator_function = bmbstats::relationship_lm_estimators, control = model_control( seed = 1667, boot_type = &quot;perc&quot;, boot_samples = 1000, confidence = 0.9 ) ) #&gt; [1] &quot;All values of t are equal to 0.5 \\n Cannot calculate confidence intervals&quot; #&gt; [1] &quot;All values of t are equal to 1 \\n Cannot calculate confidence intervals&quot; boot_relationship #&gt; Bootstrap with 1000 resamples and 90% perc confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -0.500000000 -0.50000000 -0.500000000 #&gt; SESOI upper 0.500000000 NA NA #&gt; SESOI range 1.000000000 NA NA #&gt; Intercept 13.106232758 12.90779375 13.302068612 #&gt; Slope 0.001493687 0.00134178 0.001654143 #&gt; RSE 0.151611849 0.12399493 0.170447293 #&gt; Pearson&#39;s r 0.931415014 0.88282614 0.959775759 #&gt; R Squared 0.867533928 0.77938199 0.921169508 #&gt; SESOI to RSE 6.595790544 5.86691658 8.064845693 #&gt; PPER 0.997419105 0.99351216 0.999633678 plot(boot_relationship) In the next chapter we will continue with the prediction tasks. References "],
["predictive-tasks-using-bmbstats.html", "Chapter 14 Predictive tasks using bmbstats 14.1 How to implement different performance metrics? 14.2 How to use different prediction model? 14.3 Example of using tuning parameter 14.4 Plotting 14.5 Comparing models 14.6 Bootstrapping model", " Chapter 14 Predictive tasks using bmbstats Simple implementation of the prediction tasks in bmbstats is done with bmbstats::cv_model function, which is short of “cross-validate model”. This function is more of a teaching tool, than something more thorough like the caret (Kuhn 2020) or mlr and mlr3 packages (Bischl et al. 2016; Lang et al. 2019). But bmbstats::cv_model is still powerful and versatile for the sports science prediction tasks (e.g. bmbstats::rct_predict is a function that we will introduce later and it was built using bmbstats::cv_model results). bmbstats::cv_model is built using the outstanding hardhat package (Vaughan and Kuhn 2020). bmbstats::cv_model has three components: (1) fitting or modeling function (set using the model_func parameter with the bmbstats::lm_model(default) and bmbstats::baseline_model being implemented using stats::lm linear regression function), (2) prediction function; which is used for predicting on the new and training data (set using the predict_func parameter with bmbstats::generic_predict being the default that calls the default predict method, or stats::predict.lm for the lm classes), and (3) performance function; which is used to return performance estimators (set using the perf_func parameter with bmbstats::performance_metrics being the default). This will be much clearer once we start implementing the bmbstats::cv_model. To demonstrate bmbstats::cv_model function, let’s generate simple data for prediction: require(tidyverse) require(bmbstats) require(cowplot) set.seed(1667) # Model (DGP) random_error &lt;- 2 sinus_data &lt;- tibble( x = seq(0.8, 2.5, 0.05), observed_y = 30 + 15 * (x * sin(x)) + rnorm(n = length(x), mean = 0, sd = random_error), true_y = 30 + 15 * (x * sin(x)) ) head(sinus_data) #&gt; # A tibble: 6 x 3 #&gt; x observed_y true_y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.8 41.8 38.6 #&gt; 2 0.85 38.5 39.6 #&gt; 3 0.9 42.3 40.6 #&gt; 4 0.95 41.5 41.6 #&gt; 5 1 40.6 42.6 #&gt; 6 1.05 42.7 43.7 ggplot(sinus_data, aes(x = x)) + theme_cowplot(8) + geom_point(aes(y = observed_y)) + geom_line(aes(y = true_y)) This data has irreducible error with SD equal to 2 (a.u.). Let’s use simple linear regression to predict observed \\(y\\), but evaluate model performance using 10 repeats of 5-folds cross-validations: model1 &lt;- bmbstats::cv_model( observed_y ~ x, sinus_data, # These are default options, but I will list them here model_func = lm_model, predict_func = generic_predict, perf_func = performance_metrics, # CV parameters control = model_control( cv_folds = 5, cv_repeats = 10, seed = 1667 ) ) model1 #&gt; Training data consists of 2 predictors and 35 observations. Cross-Validation of the model was performed using 10 repeats of 5 folds. #&gt; #&gt; Model performance: #&gt; #&gt; metric training training.pooled testing.pooled mean #&gt; MBE -1.786530e-14 3.608730e-15 0.01201151 0.03878805 #&gt; MAE 3.307366e+00 3.281275e+00 3.42055845 3.40542550 #&gt; RMSE 3.830961e+00 3.811531e+00 4.00171168 3.91431323 #&gt; PPER 2.500597e-01 2.532192e-01 0.24118080 0.22872198 #&gt; SESOI to RMSE 6.520075e-01 6.461322e-01 0.61542490 0.67035639 #&gt; R-squared 6.125604e-01 6.164804e-01 0.57725715 0.59398198 #&gt; MinErr -5.833192e+00 -6.481721e+00 -6.66660654 -4.83585586 #&gt; MaxErr 9.531759e+00 1.069301e+01 10.89240388 5.74134976 #&gt; MaxAbsErr 9.531759e+00 1.069301e+01 10.89240388 6.54646863 #&gt; SD min max #&gt; 1.39091047 -3.56918463 2.8362928 #&gt; 0.71086848 1.90802477 4.6087168 #&gt; 0.78174879 2.19948265 5.4637498 #&gt; 0.05453147 0.15058656 0.3811788 #&gt; 0.16588651 0.47111531 1.1614533 #&gt; 0.20141946 -0.07623944 0.8893906 #&gt; 1.34488218 -6.66660654 -1.8826434 #&gt; 2.57512537 1.06931471 10.8924039 #&gt; 1.91850684 3.68446911 10.8924039 SESOI in the above example is calculated using 0.2 x SD of the outcome variable (i.e. observed_y) which represents Cohen’s trivial effect. SESOI constants of estimation function (that uses training data set to estimate SESOI) can be set up using the SESOI_lower and SESOI_upper parameters (to which the default are bmbstats::SESOI_lower_func and bmbstats::SESOI_upper_func respectively). The above output represents performance summary using the estimators returned by the bmbstats::performance_metrics. bmbstats has numerous cost functions implemented that could be called using the bmbstats::cost_ prefix. The above output can be plotted using the plot command and type = \"estimators\" parameter: plot(model1, &quot;estimators&quot;) Error bars represent range of estimator values across cross-validation folds, while the dashed line indicate training performance. There estimates can be accessed in the returned object, i.e. model1$performance for training, and model1$cross_validation$performance for cross-validation. 14.1 How to implement different performance metrics? To implement different performance metrics you need to write your own function that return named vector using the following template: # My performance metrics my_perf_metrics &lt;- function(observed, predicted, SESOI_lower = 0, SESOI_upper = 0, na.rm = FALSE) { c( RMSE = cost_RMSE( observed = observed, predicted = predicted, SESOI_lower = SESOI_lower, SESOI_upper = SESOI_upper, na.rm = na.rm ), PPER = cost_PPER( observed = observed, predicted = predicted, SESOI_lower = SESOI_lower, SESOI_upper = SESOI_upper, na.rm = na.rm ), `R-squared` = cost_R_squared( observed = observed, predicted = predicted, SESOI_lower = SESOI_lower, SESOI_upper = SESOI_upper, na.rm = na.rm ) ) } # Re-run the cv_model with my perf metrics model2 &lt;- bmbstats::cv_model( observed_y ~ x, sinus_data, # Use our performance metrics perf_func = my_perf_metrics, # CV parameters control = model_control( cv_folds = 5, cv_repeats = 10, seed = 1667 ) ) model2 #&gt; Training data consists of 2 predictors and 35 observations. Cross-Validation of the model was performed using 10 repeats of 5 folds. #&gt; #&gt; Model performance: #&gt; #&gt; metric training training.pooled testing.pooled mean SD #&gt; RMSE 3.8309607 3.8115312 4.0017117 3.914313 0.78174879 #&gt; PPER 0.2500597 0.2532192 0.2411808 0.228722 0.05453147 #&gt; R-squared 0.6125604 0.6164804 0.5772572 0.593982 0.20141946 #&gt; min max #&gt; 2.19948265 5.4637498 #&gt; 0.15058656 0.3811788 #&gt; -0.07623944 0.8893906 plot(model2, &quot;estimators&quot;) 14.2 How to use different prediction model? To use different prediction model instead of stats::lm, you will need to modify the model function using the following template. Let’s use Regression Tree using the rpart package (Therneau and Atkinson 2019): require(rpart) # My prediction model my_model_func &lt;- function(predictors, outcome, SESOI_lower = 0, SESOI_upper = 0, na.rm = FALSE, ...) { data &lt;- cbind(.outcome = outcome, predictors) rpart(.outcome ~ ., data = data, ...) } # Call the cv_model model3 &lt;- bmbstats::cv_model( observed_y ~ x, sinus_data, # Use our model function model_func = my_model_func, # CV parameters control = model_control( cv_folds = 5, cv_repeats = 10, seed = 1667 ), # Do not create intercept column intercept = TRUE ) model3 #&gt; Training data consists of 2 predictors and 35 observations. Cross-Validation of the model was performed using 10 repeats of 5 folds. #&gt; #&gt; Model performance: #&gt; #&gt; metric training training.pooled testing.pooled mean #&gt; MBE 2.030618e-16 -3.045865e-16 0.03760974 0.03641386 #&gt; MAE 2.023389e+00 2.268546e+00 2.60389737 2.57549008 #&gt; RMSE 2.591359e+00 2.765355e+00 3.29362285 3.19456487 #&gt; PPER 3.621870e-01 3.437038e-01 0.29087228 0.27713101 #&gt; SESOI to RMSE 9.639014e-01 8.905739e-01 0.74773376 0.83310652 #&gt; R-squared 8.227265e-01 7.981214e-01 0.71366153 0.74453263 #&gt; MinErr -5.699018e+00 -6.168662e+00 -9.88715278 -4.49333669 #&gt; MaxErr 6.097584e+00 6.427283e+00 7.22167137 4.47699323 #&gt; MaxAbsErr 6.097584e+00 6.427283e+00 9.88715278 5.86162382 #&gt; SD min max #&gt; 1.38472089 -3.5600378 2.4319242 #&gt; 0.60425150 0.9905252 3.7964691 #&gt; 0.67699316 1.1382304 4.6196434 #&gt; 0.07822901 0.1429604 0.6357025 #&gt; 0.26906599 0.5367524 2.2239463 #&gt; 0.13595473 0.3724261 0.9682861 #&gt; 2.25968230 -9.8871528 -0.4531138 #&gt; 1.95348134 -0.3225821 7.2216714 #&gt; 1.31129516 1.8902345 9.8871528 plot(model3, &quot;estimators&quot;) 14.3 Example of using tuning parameter Let’s use the same example from the Overfitting section of the Prediction chapter - polynomial fit. The objective is to select the polynomial degree that gives the best cross-validation performance. poly_tuning &lt;- seq(1, 12) my_model_func &lt;- function(predictors, outcome, SESOI_lower = 0, SESOI_upper = 0, na.rm = FALSE, poly_n = 1) { data &lt;- cbind(.outcome = outcome, predictors) lm(.outcome ~ poly(x, poly_n), data = data) } # Model performance across different tuning parameters poly_perf &lt;- map_df(poly_tuning, function(poly_n) { model &lt;- bmbstats::cv_model( observed_y ~ x, sinus_data, # CV parameters control = model_control( cv_folds = 5, cv_repeats = 10, seed = 1667 ), model_func = my_model_func, poly_n = poly_n ) data.frame( poly_n = poly_n, model$cross_validation$performance$summary$overall ) }) head(poly_perf) #&gt; poly_n metric training training.pooled testing.pooled mean #&gt; 1 1 MBE -1.339861e-14 1.081218e-15 0.01201151 0.03878805 #&gt; 2 1 MAE 3.307366e+00 3.281275e+00 3.42055845 3.40542550 #&gt; 3 1 RMSE 3.830961e+00 3.811531e+00 4.00171168 3.91431323 #&gt; 4 1 PPER 2.500597e-01 2.532192e-01 0.24118080 0.22872198 #&gt; 5 1 SESOI to RMSE 6.520075e-01 6.461322e-01 0.61542490 0.67035639 #&gt; 6 1 R-squared 6.125604e-01 6.164804e-01 0.57725715 0.59398198 #&gt; SD min max #&gt; 1 1.39091047 -3.56918463 2.8362928 #&gt; 2 0.71086848 1.90802477 4.6087168 #&gt; 3 0.78174879 2.19948265 5.4637498 #&gt; 4 0.05453147 0.15058656 0.3811788 #&gt; 5 0.16588651 0.47111531 1.1614533 #&gt; 6 0.20141946 -0.07623944 0.8893906 In the figure below the results of this analysis is depicted. Dashed red line represents cross-validated performance (using performance on the pooled testing data). ggplot(poly_perf, aes(x = poly_n)) + theme_cowplot(8) + geom_line(aes(y = training), color = &quot;blue&quot;, alpha = 0.8) + geom_line(aes(y = testing.pooled), color = &quot;red&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + facet_wrap(~metric, scales = &quot;free_y&quot;) As can be seen, the best predictive performance is with 3rd polynomial degrees. The figure below depicts RMSE estimator for higher resolution image. ggplot( filter( poly_perf, metric == &quot;RMSE&quot; ), aes(x = poly_n) ) + theme_cowplot(8) + geom_line(aes(y = training), color = &quot;blue&quot;, alpha = 0.8) + geom_line(aes(y = testing.pooled), color = &quot;red&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + ylab(&quot;RMSE&quot;) 14.4 Plotting Various diagnostic graphs can be easily generated using a generic plot function. Let’s fit a 3rd degree polynomial model first: model4 &lt;- bmbstats::cv_model( observed_y ~ poly(x, 3), sinus_data, # CV parameters control = model_control( cv_folds = 5, cv_repeats = 10, seed = 1667 ) ) model4 #&gt; Training data consists of 4 predictors and 35 observations. Cross-Validation of the model was performed using 10 repeats of 5 folds. #&gt; #&gt; Model performance: #&gt; #&gt; metric training training.pooled testing.pooled mean #&gt; MBE -1.299275e-14 2.060564e-15 -0.03724767 -0.02930855 #&gt; MAE 1.534398e+00 1.509623e+00 1.77622691 1.77362879 #&gt; RMSE 1.837539e+00 1.807043e+00 2.10648622 2.07925558 #&gt; PPER 4.925435e-01 5.041349e-01 0.44022003 0.40113906 #&gt; SESOI to RMSE 1.359326e+00 1.362863e+00 1.16912846 1.22806404 #&gt; R-squared 9.108623e-01 9.137963e-01 0.88289646 0.89017328 #&gt; MinErr -3.676547e+00 -4.335315e+00 -4.57284164 -2.99135803 #&gt; MaxErr 3.819068e+00 4.380287e+00 4.43506709 2.54927044 #&gt; MaxAbsErr 3.819068e+00 4.380287e+00 4.57284164 3.45414085 #&gt; SD min max #&gt; 0.83265732 -2.3250491 1.3681763 #&gt; 0.33144744 1.0364816 2.3883585 #&gt; 0.29737351 1.3940723 2.7092065 #&gt; 0.05824287 0.2221031 0.5513211 #&gt; 0.18072558 0.9198745 1.7145206 #&gt; 0.04568934 0.7776291 0.9626275 #&gt; 0.81846093 -4.5728416 0.2782154 #&gt; 1.17063338 -0.2033888 4.4350671 #&gt; 0.57631575 2.3829256 4.5728416 To plot the residuals, use type=\"residuals\" in the plot function: plot(model4, &quot;residuals&quot;) The following code will plot the training and testing residuals across cross-validation folds: plot(model4, &quot;training-residuals&quot;) plot(model4, &quot;testing-residuals&quot;) To plot bias-variance error decomposition across cross-validation folds use: plot(model4, &quot;bias-variance-index&quot;) The above figure depicts bias-variance for each observation index. To plot against the outcome variable value use: plot(model4, &quot;bias-variance-observed&quot;) Prediction error (i.e. residuals) can also be plotted as boxes, demonstrating the mean (i.e. bias) and spread (i.e. variance). To plot prediction error again outcome index use: plot(model4, &quot;prediction-index&quot;) Similar to plotting bias-variance, prediction error distribution (actually, a spread or range) can be plotted against outcome variable value: plot(model4, &quot;prediction-observed&quot;) And finally, to plot the performance estimator use: plot(model4, &quot;estimators&quot;) Each of these plots can be customized using the control=bmbstats::plot_control argument and function. 14.5 Comparing models Comparing models is beyond the scope of this book, but I will provide a short introduction using bmbstats. Let’s use our model3 that used rpart Regression Tree, and model4 that used 3rd degree polynomial fit, to plot the estimators range across cross-validation folds. plot_data &lt;- rbind( data.frame(model = &quot;rpart&quot;, model3$cross_validation$performance$summary$overall), data.frame(model = &quot;poly&quot;, model4$cross_validation$performance$summary$overall) ) plot_data #&gt; model metric training training.pooled testing.pooled mean #&gt; 1 rpart MBE 2.030618e-16 -3.045865e-16 0.03760974 0.03641386 #&gt; 2 rpart MAE 2.023389e+00 2.268546e+00 2.60389737 2.57549008 #&gt; 3 rpart RMSE 2.591359e+00 2.765355e+00 3.29362285 3.19456487 #&gt; 4 rpart PPER 3.621870e-01 3.437038e-01 0.29087228 0.27713101 #&gt; 5 rpart SESOI to RMSE 9.639014e-01 8.905739e-01 0.74773376 0.83310652 #&gt; 6 rpart R-squared 8.227265e-01 7.981214e-01 0.71366153 0.74453263 #&gt; 7 rpart MinErr -5.699018e+00 -6.168662e+00 -9.88715278 -4.49333669 #&gt; 8 rpart MaxErr 6.097584e+00 6.427283e+00 7.22167137 4.47699323 #&gt; 9 rpart MaxAbsErr 6.097584e+00 6.427283e+00 9.88715278 5.86162382 #&gt; 10 poly MBE -1.299275e-14 2.060564e-15 -0.03724767 -0.02930855 #&gt; 11 poly MAE 1.534398e+00 1.509623e+00 1.77622691 1.77362879 #&gt; 12 poly RMSE 1.837539e+00 1.807043e+00 2.10648622 2.07925558 #&gt; 13 poly PPER 4.925435e-01 5.041349e-01 0.44022003 0.40113906 #&gt; 14 poly SESOI to RMSE 1.359326e+00 1.362863e+00 1.16912846 1.22806404 #&gt; 15 poly R-squared 9.108623e-01 9.137963e-01 0.88289646 0.89017328 #&gt; 16 poly MinErr -3.676547e+00 -4.335315e+00 -4.57284164 -2.99135803 #&gt; 17 poly MaxErr 3.819068e+00 4.380287e+00 4.43506709 2.54927044 #&gt; 18 poly MaxAbsErr 3.819068e+00 4.380287e+00 4.57284164 3.45414085 #&gt; SD min max #&gt; 1 1.38472089 -3.5600378 2.4319242 #&gt; 2 0.60425150 0.9905252 3.7964691 #&gt; 3 0.67699316 1.1382304 4.6196434 #&gt; 4 0.07822901 0.1429604 0.6357025 #&gt; 5 0.26906599 0.5367524 2.2239463 #&gt; 6 0.13595473 0.3724261 0.9682861 #&gt; 7 2.25968230 -9.8871528 -0.4531138 #&gt; 8 1.95348134 -0.3225821 7.2216714 #&gt; 9 1.31129516 1.8902345 9.8871528 #&gt; 10 0.83265732 -2.3250491 1.3681763 #&gt; 11 0.33144744 1.0364816 2.3883585 #&gt; 12 0.29737351 1.3940723 2.7092065 #&gt; 13 0.05824287 0.2221031 0.5513211 #&gt; 14 0.18072558 0.9198745 1.7145206 #&gt; 15 0.04568934 0.7776291 0.9626275 #&gt; 16 0.81846093 -4.5728416 0.2782154 #&gt; 17 1.17063338 -0.2033888 4.4350671 #&gt; 18 0.57631575 2.3829256 4.5728416 ggplot( plot_data, aes(y = model, x = mean) ) + theme_bw(8) + geom_errorbarh(aes(xmax = max, xmin = min), color = &quot;black&quot;, height = 0 ) + geom_point() + geom_point(aes(x = training), color = &quot;red&quot;, shape = &quot;|&quot;, size = 3) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + facet_wrap(~metric, scales = &quot;free_x&quot;) As can be seen from the figure, polynomial fit represents a more predictive model. But we can also perform statistical significance test using the cross-validation performance, for let’s say RMSE estimator. Since the CV folds are the same (which we achieved by using the same seed number), we can do dependent groups analysis. But before, let’s plot the CV estimates of the RMSE: rmse_rpart &lt;- filter( model3$cross_validation$performance$folds$testing, metric == &quot;RMSE&quot; ) head(rmse_rpart) #&gt; fold metric value #&gt; RMSE...1 Fold1.Rep01 RMSE 3.535807 #&gt; RMSE...2 Fold2.Rep01 RMSE 1.138230 #&gt; RMSE...3 Fold3.Rep01 RMSE 3.175505 #&gt; RMSE...4 Fold4.Rep01 RMSE 2.995586 #&gt; RMSE...5 Fold5.Rep01 RMSE 3.739492 #&gt; RMSE...6 Fold1.Rep02 RMSE 3.931772 rmse_poly &lt;- filter( model4$cross_validation$performance$folds$testing, metric == &quot;RMSE&quot; ) head(rmse_poly) #&gt; fold metric value #&gt; RMSE...1 Fold1.Rep01 RMSE 1.745409 #&gt; RMSE...2 Fold2.Rep01 RMSE 2.408312 #&gt; RMSE...3 Fold3.Rep01 RMSE 2.039587 #&gt; RMSE...4 Fold4.Rep01 RMSE 2.670888 #&gt; RMSE...5 Fold5.Rep01 RMSE 2.070925 #&gt; RMSE...6 Fold1.Rep02 RMSE 2.082951 rmse_data &lt;- rbind( data.frame(model = &quot;rpart&quot;, value = rmse_rpart$value), data.frame(model = &quot;poly&quot;, value = rmse_poly$value) ) bmbstats::plot_raincloud( rmse_data, value = &quot;value&quot;, value_label = &quot;RMSE&quot;, groups = &quot;model&quot; ) And we can finally perform the dependent groups analysis: rmse_perf &lt;- bmbstats::compare_dependent_groups( pre = rmse_rpart$value, post = rmse_poly$value ) rmse_perf #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -0.13539863 -0.17607368 -0.10927225 #&gt; SESOI upper 0.13539863 0.10927225 0.17607368 #&gt; SESOI range 0.27079726 0.21854450 0.35214735 #&gt; Mean change -1.11530929 -1.32646984 -0.89455705 #&gt; SD change 0.76022031 0.60218574 0.99219147 #&gt; %CV change -68.16228649 -108.17299822 -50.50157994 #&gt; % change -30.38499807 -35.93194876 -19.10373522 #&gt; Ratio 0.69615002 0.64068051 0.80896265 #&gt; Cohen&#39;s d -1.64744544 -2.17427538 -1.02403852 #&gt; CLES 0.06573315 0.02725296 0.16669235 #&gt; OVL 0.41009713 0.27810102 0.60868406 #&gt; Mean change to SESOI -4.11861361 -5.43568845 -2.56009630 #&gt; SD change to SESOI 2.80734121 2.51010263 3.14489890 #&gt; pLower 0.89827180 0.76960554 0.95994450 #&gt; pEquivalent 0.04856260 0.02183432 0.09330646 #&gt; pHigher 0.05316560 0.01748864 0.13895348 # Do the statistical significance test rmse_NHST &lt;- bmbstats::bootstrap_NHST( rmse_perf, estimator = &quot;Mean change&quot;, test = &quot;two.sided&quot;, null_hypothesis = 0 ) rmse_NHST #&gt; Null-hypothesis significance test for the `Mean change` estimator #&gt; Bootstrap result: Mean change=-1.115, 95% CI [-1.326, -0.895] #&gt; H0=0, test: two.sided #&gt; p=0.000499750124937531 plot(rmse_NHST) So we can conclude that polynomial model has significantly better performance, using RMSE estimator, than Regression Tree model that cannot be explained as a pure (sampling) chance. But to be honest, model comparison is beyond my current knowledge and I am not sure that comparing the estimators is the right approach. We could instead compare model residuals. Since CV folds are identical, we can perform dependent groups analysis as well. But before doing that, let’s plot the CV residuals: resid_data &lt;- rbind( data.frame(model = &quot;rpart&quot;, value = model3$cross_validation$data$testing$residual), data.frame(model = &quot;poly&quot;, value = model4$cross_validation$data$testing$residual) ) bmbstats::plot_raincloud( resid_data, value = &quot;value&quot;, value_label = &quot;CV residuals&quot;, groups = &quot;model&quot; ) Or their difference: bmbstats::plot_raincloud( data.frame(diff = model4$cross_validation$data$testing$residual - model3$cross_validation$data$testing$residual), value = &quot;diff&quot;, value_label = &quot;CV residuals difference&quot; ) And we can finally perform the dependent groups analysis: resid_perf &lt;- bmbstats::compare_dependent_groups( pre = model3$cross_validation$data$testing$residual, post = model4$cross_validation$data$testing$residual ) resid_perf #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; SESOI lower -6.596246e-01 -7.094456e-01 -6.153383e-01 #&gt; SESOI upper 6.596246e-01 6.153383e-01 7.094456e-01 #&gt; SESOI range 1.319249e+00 1.230677e+00 1.418891e+00 #&gt; Mean change -7.485741e-02 -3.678773e-01 2.267774e-01 #&gt; SD change 2.853680e+00 2.711641e+00 3.019229e+00 #&gt; %CV change -3.812154e+03 -1.203382e+06 -1.112797e+03 #&gt; % change -1.816303e+01 -8.898433e+01 6.621511e+01 #&gt; Ratio 8.183697e-01 1.101567e-01 1.662151e+00 #&gt; Cohen&#39;s d -2.269697e-02 -1.118320e-01 6.659056e-02 #&gt; CLES 4.923722e-01 4.625656e-01 5.225228e-01 #&gt; OVL 9.909454e-01 9.689477e-01 9.999341e-01 #&gt; Mean change to SESOI -5.674243e-02 -2.795801e-01 1.664764e-01 #&gt; SD change to SESOI 2.163109e+00 2.042661e+00 2.298416e+00 #&gt; pLower 4.188782e-01 3.767829e-01 4.605721e-01 #&gt; pEquivalent 1.826035e-01 1.721664e-01 1.933616e-01 #&gt; pHigher 3.985183e-01 3.604887e-01 4.386698e-01 # Do the statistical significance test resid_NHST &lt;- bmbstats::bootstrap_NHST( resid_perf, estimator = &quot;Mean change&quot;, test = &quot;two.sided&quot;, null_hypothesis = 0 ) resid_NHST #&gt; Null-hypothesis significance test for the `Mean change` estimator #&gt; Bootstrap result: Mean change=-0.075, 95% CI [-0.368, 0.227] #&gt; H0=0, test: two.sided #&gt; p=0.617 plot(resid_NHST) According to Cross-Validation residuals analysis, the two models didn’t perform statistically different that can be attributed to chance. It bears repeating that I am not sure either of these are valid model comparison methods, so use this only as an example. Model comparison also involves deciding about model complexity and selecting the simpler model. 14.6 Bootstrapping model model4.boot.coef &lt;- bmbstats::bmbstats( data = sinus_data, estimator_function = function(data, SESOI_lower, SESOI_upper, na.rm, init_boot) { model &lt;- lm(observed_y ~ poly(x, 3), data) coef(model) } ) model4.boot.coef #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; (Intercept) 51.094279 48.87153 52.96739 #&gt; poly(x, 3)1 28.497988 19.59403 36.45944 #&gt; poly(x, 3)2 -18.194924 -23.94497 -13.14120 #&gt; poly(x, 3)3 -8.027118 -13.13937 -4.52746 plot(model4.boot.coef) Performance metrics model4.boot.perf &lt;- bmbstats::bmbstats( data = sinus_data, estimator_function = function(data, SESOI_lower, SESOI_upper, na.rm, init_boot) { model &lt;- lm(observed_y ~ poly(x, 3), data) # Return model performance metrics bmbstats::performance_metrics( observed = data$observed_y, predicted = predict(model), SESOI_lower = SESOI_lower, SESOI_upper = SESOI_upper, na.rm = na.rm ) } ) model4.boot.perf #&gt; Bootstrap with 2000 resamples and 95% bca confidence intervals. #&gt; #&gt; estimator value lower upper #&gt; MBE -1.299275e-14 -4.125793e-14 1.055661e-14 #&gt; MAE 1.534398e+00 1.290075e+00 2.119985e+00 #&gt; RMSE 1.837539e+00 1.620042e+00 2.370531e+00 #&gt; PPER 0.000000e+00 NA NA #&gt; SESOI to RMSE 0.000000e+00 NA NA #&gt; R-squared 9.108623e-01 8.286065e-01 9.396944e-01 #&gt; MinErr -3.676547e+00 -5.001168e+00 -3.035433e+00 #&gt; MaxErr 3.819068e+00 2.958894e+00 5.388544e+00 #&gt; MaxAbsErr 3.819068e+00 3.001014e+00 5.097689e+00 plot(model4.boot.perf) References "],
["validity-and-reliability.html", "Chapter 15 Validity and Reliability 15.1 Data generation 15.2 Validity 15.3 Reliability", " Chapter 15 Validity and Reliability 15.1 Data generation require(tidyverse) require(bmbstats) n_subjects &lt;- 20 criterion_random &lt;- 0.3 practical_fixed &lt;- 1 practical_proportional &lt;- 1.1 practical_random &lt;- 1 set.seed(1667) agreement_data &lt;- tibble( Athlete = paste( &quot;Athlete&quot;, str_pad( string = seq(1, n_subjects), width = 2, pad = &quot;0&quot; ) ), True_score = rnorm(n_subjects, 45, 5), Criterion_score.trial1 = 0 + True_score * 1 + rnorm(n_subjects, 0, criterion_random), Criterion_score.trial2 = 0 + True_score * 1 + rnorm(n_subjects, 0, criterion_random), Practical_score.trial1 = practical_fixed + True_score * practical_proportional + rnorm(n_subjects, 0, practical_random), Practical_score.trial2 = practical_fixed + True_score * practical_proportional + rnorm(n_subjects, 0, practical_random) ) head(agreement_data) #&gt; # A tibble: 6 x 6 #&gt; Athlete True_score Criterion_score… Criterion_score… Practical_score… #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Athlet… 52.9 52.9 52.9 59.2 #&gt; 2 Athlet… 42.4 42.3 42.2 47.7 #&gt; 3 Athlet… 49.2 49.1 49.5 55.0 #&gt; 4 Athlet… 44.8 44.7 44.8 50.5 #&gt; 5 Athlet… 40.0 40.4 40.1 44.7 #&gt; 6 Athlet… 42.6 42.5 42.2 48.7 #&gt; # … with 1 more variable: Practical_score.trial2 &lt;dbl&gt; 15.2 Validity 15.2.1 True vs Criterion bmbstats::plot_pair_lm( predictor = agreement_data$True_score, outcome = agreement_data$Criterion_score.trial1, predictor_label = &quot;True Score&quot;, outcome_label = &quot;Criterion Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) bmbstats::plot_pair_BA( predictor = agreement_data$True_score, outcome = agreement_data$Criterion_score.trial1, predictor_label = &quot;True Score&quot;, outcome_label = &quot;Criterion Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) 15.2.2 True vs Practical bmbstats::plot_pair_lm( predictor = agreement_data$True_score, outcome = agreement_data$Practical_score.trial1, predictor_label = &quot;True Score&quot;, outcome_label = &quot;Practical Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) bmbstats::plot_pair_BA( predictor = agreement_data$True_score, outcome = agreement_data$Practical_score.trial1, predictor_label = &quot;True Score&quot;, outcome_label = &quot;Practical Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) 15.2.3 Criterion vs Practical bmbstats::plot_pair_lm( predictor = agreement_data$Criterion_score.trial1, outcome = agreement_data$Practical_score.trial1, predictor_label = &quot;Criterion Score&quot;, outcome_label = &quot;Practical Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) bmbstats::plot_pair_BA( predictor = agreement_data$Criterion_score.trial1, outcome = agreement_data$Practical_score.trial1, predictor_label = &quot;Criterion Score&quot;, outcome_label = &quot;Practical Score&quot;, SESOI_lower = -2.5, SESOI_upper = 2.5 ) 15.3 Reliability "],
["rct-analysis-and-prediction-in-bmbstats.html", "Chapter 16 RCT analysis and prediction in bmbstats", " Chapter 16 RCT analysis and prediction in bmbstats "],
["appendix-a-dorem-package.html", "Appendix A: dorem package", " Appendix A: dorem package "],
["appendix-b-shorts-package.html", "Appendix B: shorts package", " Appendix B: shorts package "],
["appendix-c-recommended-material.html", "Appendix C: Recommended material", " Appendix C: Recommended material General Spiegelhalter D. 2019. The art of statistics: how to learn from data. New York: Basic Books, an imprint of Perseus Books, a subsidiary of Hachette Book Group. Foreman JW. 2014. Data smart: using data science to transform information into insight. Hoboken, New Jersey: John Wiley &amp; Sons. Dienes Z. 2008. Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. New York: Red Globe Press. Bayesian analysis Kruschke JK. 2015. Doing Bayesian data analysis: a tutorial with R, JAGS, and Stan. Boston: Academic Press. McElreath R. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton: Chapman and Hall/CRC. Lambert B. 2018. A student’s guide to Bayesian statistics. Los Angeles: SAGE. Stanton JM. 2017. Reasoning with data: an introduction to traditional and Bayesian statistics using R. New York: The Guilford Press. Predictive analysis and Machine Learning Kuhn M, Johnson K. 2018. Applied Predictive Modeling. New York: Springer. Kuhn M, Johnson K. 2019. Feature Engineering and Selection: a Practical Approach for Predictive Models. Milton: CRC Press LLC. Lantz B. 2019. Machine learning with R: expert techniques for predictive modeling. James G, Witten D, Hastie T, Tibshirani R. 2017. An Introduction to Statistical Learning: with Applications in R. New York: Springer. Causal inference Kleinberg S. 2015. Why: A Guide to Finding and Using Causes. Beijing; Boston: O’Reilly Media. Pearl J, Mackenzie D. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books. Hernán MA, Robins J. 2019. Causal Inference. Boca Raton: Chapman &amp; Hall/CRC. R Language and Visualization Kabacoff R. 2015. R in action: data analysis and graphics with R. Shelter Island: Manning. Matloff NS. 2011. The art of R programming: tour of statistical software design. San Francisco: No Starch Press. Wickham H, Grolemund G. 2016. R for data science: import, tidy, transform, visualize, and model data. Sebastopol, CA: O’Reilly. Wilke C. 2019. Fundamentals of data visualization: a primer on making informative and compelling figures. Sebastopol, CA: O’Reilly Media. Healy K. 2018. Data visualization: a practical introduction. Princeton, NJ: Princeton University Press. Simulation and statistical inference Carsey T, Harden J. 2013. Monte Carlo Simulation and Resampling Methods for Social Science. Los Angeles: Sage Publications, Inc. Efron B, Hastie T. 2016. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. New York, NY: Cambridge University Press. Online Courses Hastie T, Tibshirani R. 2016.Statistical Learning. Available at https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about (accessed October 16, 2019). Lakens D. 2017.Improving your statistical inferences. Available at https://www.coursera.org/learn/statistical-inferences (accessed October 16, 2019). Lakens D. 2019.Improving your statistical questions. Available at https://www.coursera.org/learn/improving-statistical-questions (accessed October 16, 2019). "],
["references.html", "References", " References Albanese, Davide, Michele Filosi, Roberto Visintainer, Samantha Riccadonna, Giuseppe Jurman, and Cesare Furlanello. 2012a. “Minerva and Minepy: A c Engine for the Mine Suite and Its R, Python and Matlab Wrappers.” Bioinformatics, bts707. ———. 2012b. “Minerva and Minepy: A C Engine for the MINE Suite and Its R, Python and MATLAB Wrappers.” Bioinformatics, bts707. Allaire, JJ, Jeffrey Horner, Yihui Xie, Vicent Marti, and Natacha Porte. 2019. Markdown: Render Markdown with the c Library ’Sundown’. https://CRAN.R-project.org/package=markdown. Allen, Mary J., and Wendy M. Yen. 2001. Introduction to Measurement Theory. 1 edition. Long Grove, Ill: Waveland Pr Inc. Allen, Micah, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and Rogier Kievit. 2018. “Raincloudplots Tutorials and Codebase.” Zenodo. https://doi.org/10.5281/zenodo.1402959. Allen, Micah, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and Rogier A. Kievit. 2019. “Raincloud Plots: A Multi-Platform Tool for Robust Data Visualization.” Wellcome Open Research 4 (April): 63. https://doi.org/10.12688/wellcomeopenres.15191.1. Altmann, Thomas, Jakob Bodensteiner, Cord Dankers, Thommy Dassen, Nikolas Fritz, Sebastian Gruber, Philipp Kopper, Veronika Kronseder, Moritz Wagner, and Emanuel Renkl. 2019. Limitations of Interpretable Machine Learning Methods. Amrhein, Valentin, David Trafimow, and Sander Greenland. 2019. “Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis If We Don’t Expect Replication.” The American Statistician 73 (sup1): 262–70. https://doi.org/10.1080/00031305.2018.1543137. Angrist, Joshua David, and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton ; Oxford: Princeton University Press. Anvari, Farid, and Daniel Lakens. 2019. “Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest,” March. https://doi.org/10.31234/osf.io/syp5a. Barker, Richard J., and Matthew R. Schofield. 2008. “Inference About Magnitudes of Effects.” International Journal of Sports Physiology and Performance 3 (4): 547–57. https://doi.org/10.1123/ijspp.3.4.547. Barron, Jonathan T. 2019. “A General and Adaptive Robust Loss Function.” arXiv:1701.03077 [Cs, Stat], April. http://arxiv.org/abs/1701.03077. Batterham, Alan M., and William G. Hopkins. 2006. “Making Meaningful Inferences About Magnitudes.” International Journal of Sports Physiology and Performance 1 (1): 50–57. https://doi.org/10.1123/ijspp.1.1.50. Beaujean, A. Alexander. 2014. Latent Variable Modeling Using R: A Step by Step Guide. New York: Routledge/Taylor &amp; Francis Group. Biecek, Przemyslaw, and Tomasz Burzykowski. 2019. Predictive Models: Explore, Explain, and Debug. Binmore, Ken. 2011. Rational Decisions. Fourth Impression edition. Princeton, NJ: Princeton University Press. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. Bischl, Bernd, Michel Lang, Jakob Richter, Jakob Bossek, Daniel Horn, and Pascal Kerschke. 2020. ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning. https://CRAN.R-project.org/package=ParamHelpers. Bischl, Bernd, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas, and Michel Lang. 2017. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions.” arXiv Preprint arXiv:1703.03373. Borg, David N., Geoffrey M. Minett, Ian B. Stewart, and Christopher C. Drovandi. 2018. “Bayesian Methods Might Solve the Problems with Magnitude-Based Inference:” Medicine &amp; Science in Sports &amp; Exercise 50 (12): 2609–10. https://doi.org/10.1249/MSS.0000000000001736. Borsboom, Denny. 2009. Measuring the Mind: Conceptual Issues in Modern Psychometrics. Cambridge: Cambridge University Press. ———. 2008. “Latent Variable Theory.” Measurement: Interdisciplinary Research &amp; Perspective 6 (1-2): 25–53. https://doi.org/10.1080/15366360802035497. Borsboom, Denny, Gideon J. Mellenbergh, and Jaap van Heerden. 2003. “The Theoretical Status of Latent Variables.” Psychological Review 110 (2): 203–19. https://doi.org/10.1037/0033-295X.110.2.203. Botchkarev, Alexei. 2019. “A New Typology Design of Performance Metrics to Measure Errors in Machine Learning Regression Algorithms.” Interdisciplinary Journal of Information, Knowledge, and Management 14: 045–076. https://doi.org/10.28945/4184. Breheny, Patrick, and Woodrow Burchett. 2017. “Visualization of Regression Models Using Visreg.” The R Journal 9 (2): 56–71. Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–215. Buchheit, Martin, and Alireza Rabbani. 2014. “The 3015 Intermittent Fitness Test Versus the Yo-Yo Intermittent Recovery Test Level 1: Relationship and Sensitivity to Training.” International Journal of Sports Physiology and Performance 9 (3): 522–24. https://doi.org/10.1123/ijspp.2012-0335. Caldwell, Aaron R., and Samuel N. Cheuvront. 2019. “Basic Statistical Considerations for Physiology: The Journal Temperature Toolbox.” Temperature, June, 1–30. https://doi.org/10.1080/23328940.2019.1624131. Canty, Angelo, and B. D. Ripley. 2017. Boot: Bootstrap R (S-Plus) Functions. Carsey, Thomas, and Jeffrey Harden. 2013. Monte Carlo Simulation and Resampling Methods for Social Science. 1 edition. Los Angeles: Sage Publications, Inc. Casalicchio, Giuseppe, Jakob Bossek, Michel Lang, Dominik Kirchhoff, Pascal Kerschke, Benjamin Hofner, Heidi Seibold, Joaquin Vanschoren, and Bernd Bischl. 2017. “OpenML: An R Package to Connect to the Machine Learning Platform Openml.” Computational Statistics, 1–15. Chai, T., and R. R. Draxler. 2014. “Root Mean Square Error (RMSE) or Mean Absolute Error (MAE)? Arguments Against Avoiding RMSE in the Literature.” Geoscientific Model Development 7 (3): 1247–50. https://doi.org/10.5194/gmd-7-1247-2014. Cohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates. Cumming, Geoff. 2014. “The New Statistics: Why and How.” Psychological Science 25 (1): 7–29. https://doi.org/10.1177/0956797613504966. Curran-Everett, Douglas. 2018. “Magnitude-Based Inference: Good Idea but Flawed Approach.” Medicine &amp; Science in Sports &amp; Exercise 50 (10): 2164–5. https://doi.org/10.1249/MSS.0000000000001646. Dankel, Scott J., and Jeremy P. Loenneke. 2019. “A Method to Stop Analyzing Random Error and Start Analyzing Differential Responders to Exercise.” Sports Medicine, June. https://doi.org/10.1007/s40279-019-01147-0. Davison, A. C., and D. V. Hinkley. 1997a. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/. ———. 1997b. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. Dienes, Zoltan. 2008. Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. 2008 edition. New York: Red Globe Press. Efron, Bradley. 2005. “Bayesians, Frequentists, and Scientists.” Journal of the American Statistical Association 100 (469): 1–5. https://doi.org/10.1198/016214505000000033. Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. 1 edition. New York, NY: Cambridge University Press. Estrada, Eduardo, Emilio Ferrer, and Antonio Pardo. 2019. “Statistics for Evaluating Pre-Post Change: Relation Between Change in the Distribution Center and Change in the Individual Scores.” Frontiers in Psychology 9 (January). https://doi.org/10.3389/fpsyg.2018.02696. Everitt, Brian, and Torsten Hothorn. 2011. An Introduction to Applied Multivariate Analysis with R. Use R! New York: Springer. Finch, W. Holmes, and Brian F. French. 2015. Latent Variable Modeling with R. New York: Routledge, Taylor &amp; Francis Group. Fisher, Aaron J., John D. Medaglia, and Bertus F. Jeronimus. 2018. “Lack of Group-to-Individual Generalizability Is a Threat to Human Subjects Research.” Proceedings of the National Academy of Sciences 115 (27): E6106–E6115. https://doi.org/10.1073/pnas.1711978115. Foreman, John W. 2014. Data Smart: Using Data Science to Transform Information into Insight. Hoboken, New Jersey: John Wiley &amp; Sons. Fox, John. 2003. “Effect Displays in R for Generalised Linear Models.” Journal of Statistical Software 8 (15): 1–27. http://www.jstatsoft.org/v08/i15/. Fox, John, and Jangman Hong. 2009. “Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package.” Journal of Statistical Software 32 (1): 1–24. http://www.jstatsoft.org/v32/i01/. Fox, John, and Sanford Weisberg. 2018. “Visualizing Fit and Lack of Fit in Complex Regression Models with Predictor Effect Plots and Partial Residuals.” Journal of Statistical Software 87 (9): 1–27. https://doi.org/10.18637/jss.v087.i09. Fox, John, Sanford Weisberg, and Brad Price. 2019. CarData: Companion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1–22. Gelman, Andrew. 2011. “Causality and Statistical Learning.” American Journal of Sociology 117 (3): 955–66. https://doi.org/10.1086/662659. Gelman, Andrew, and Sander Greenland. 2019. “Are Confidence Intervals Better Termed ‘Uncertainty Intervals’?” BMJ, September, l5381. https://doi.org/10.1136/bmj.l5381. Gelman, Andrew, and Christian Hennig. 2017. “Beyond Subjective and Objective in Statistics.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 180 (4): 967–1033. https://doi.org/10.1111/rssa.12276. Gigerenzer, Gerd, Ralph Hertwig, and Thorsten Pachur. 2015. Heuristics: The Foundations of Adaptive Behavior. Reprint edition. Oxford University Press. Glazier, Paul S., and Sina Mehdizadeh. 2018. “Challenging Conventional Paradigms in Applied Sports Biomechanics Research.” Sports Medicine, December. https://doi.org/10.1007/s40279-018-1030-1. Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2013. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” arXiv:1309.6392 [Stat], September. http://arxiv.org/abs/1309.6392. Greenwell, Brandon, Brad Boehmke, and Bernie Gray. 2020. Vip: Variable Importance Plots. https://CRAN.R-project.org/package=vip. Greenwell, Brandon M. 2017a. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://journal.r-project.org/archive/2017/RJ-2017-016/index.html. ———. 2017b. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://doi.org/10.32614/RJ-2017-016. Hamner, Ben, and Michael Frasco. 2018. Metrics: Evaluation Metrics for Machine Learning. https://CRAN.R-project.org/package=Metrics. Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer. Heckman, James J. 2005. “Rejoinder: Response to Sobel.” Sociological Methodology 35 (1): 135–50. https://doi.org/10.1111/j.0081-1750.2006.00166.x. Hecksteden, Anne, Jochen Kraushaar, Friederike Scharhag-Rosenberger, Daniel Theisen, Stephen Senn, and Tim Meyer. 2015. “Individual Response to Exercise Training - a Statistical Perspective.” Journal of Applied Physiology 118 (12): 1450–9. https://doi.org/10.1152/japplphysiol.00714.2014. Hecksteden, Anne, Werner Pitsch, Friederike Rosenberger, and Tim Meyer. 2018. “Repeated Testing for the Assessment of Individual Response to Exercise Training.” Journal of Applied Physiology 124 (6): 1567–79. https://doi.org/10.1152/japplphysiol.00896.2017. Henry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr. Henry, Lionel, Hadley Wickham, and Winston Chang. 2020. Ggstance: Horizontal ’Ggplot2’ Components. https://CRAN.R-project.org/package=ggstance. Hernan, M. A. 2002. “Causal Knowledge as a Prerequisite for Confounding Evaluation: An Application to Birth Defects Epidemiology.” American Journal of Epidemiology 155 (2): 176–84. https://doi.org/10.1093/aje/155.2.176. Hernan, M. A., and S. R. Cole. 2009. “Invited Commentary: Causal Diagrams and Measurement Bias.” American Journal of Epidemiology 170 (8): 959–62. https://doi.org/10.1093/aje/kwp293. Hernán, M A, and S L Taubman. 2008. “Does Obesity Shorten Life? The Importance of Well-Defined Interventions to Answer Causal Questions.” International Journal of Obesity 32 (S3): S8–S14. https://doi.org/10.1038/ijo.2008.82. Hernán, Miguel A. 2016. “Does Water Kill? A Call for Less Casual Causal Inferences.” Annals of Epidemiology 26 (10): 674–80. https://doi.org/10.1016/j.annepidem.2016.08.016. ———. 2017. “Causal Diagrams: Draw Your Assumptions Before Your Conclusions Course | PH559x | edX.” edX. https://courses.edx.org/courses/course-v1:HarvardX+PH559x+3T2017/course/. ———. 2018. “The C-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data.” American Journal of Public Health 108 (5): 616–19. https://doi.org/10.2105/AJPH.2018.304337. Hernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” CHANCE 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578. Hernán, Miguel A., and JM Robins. n.d. Causal Inference. Boca Raton: Chapman &amp; Hall/CRC. Hesterberg, Tim C. 2015. “What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum.” The American Statistician 69 (4): 371–86. https://doi.org/10.1080/00031305.2015.1089789. Hocking, Toby Dylan. 2020. Directlabels: Direct Labels for Multicolor Plots. https://CRAN.R-project.org/package=directlabels. Hopkins, Will, and Alan Batterham. 2018. “The Vindication of Magnitude-Based Inference,” 12. Hopkins, Will G. 2004a. “How to Interpret Changes in an Athletic Performance Test,” November, 2. ———. 2006. “New View of Statistics: Effect Magnitudes.” https://www.sportsci.org/resource/stats/effectmag.html. Hopkins, Will G. 2015. “Individual Responses Made Easy.” Journal of Applied Physiology 118 (12): 1444–6. https://doi.org/10.1152/japplphysiol.00098.2015. ———. 2000. “Measures of Reliability in Sports Medicine and Science.” Sports Med, 15. ———. 2004b. “Bias in Bland-Altman but Not Regression Validity Analyses.” Sportscience.org. https://sportsci.org/jour/04/wghbias.htm. ———. 2007. “Understanding Statistics by Using Spreadsheets to Generate and Analyze Samples.” Sportscience.org. https://www.sportsci.org/2007/wghstats.htm. ———. 2010. “A Socratic Dialogue on Comparison of Measures.” Sportscience.org. http://www.sportsci.org/2010/wghmeasures.htm. Hopkins, William G., Stephen W. Marshall, Alan M. Batterham, and Juri Hanin. 2009. “Progressive Statistics for Studies in Sports Medicine and Exercise Science:” Medicine &amp; Science in Sports &amp; Exercise 41 (1): 3–13. https://doi.org/10.1249/MSS.0b013e31818cb278. J, Twisk, Bosman L, Hoekstra T, Rijnhart J, Welten M, and Heymans M. 2018. “Different Ways to Estimate Treatment Effects in Randomised Controlled Trials.” Contemporary Clinical Trials Communications 10 (June): 80–85. https://doi.org/10.1016/j.conctc.2018.03.008. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning: With Applications in R. 1st ed. 2013, Corr. 7th printing 2017 edition. New York: Springer. Jovanović, Mladen. 2020. bmbstats: Bootstrap Magnitude-Based Statistics. Belgrade, Serbia. https://github.com/mladenjovanovic/bmbstats. Jovanović, Mladen. 2020. “Extending the Classical Test Theory with Circular Performance Model.” Complementary Training. Kabacoff, Robert. 2015. R in Action: Data Analysis and Graphics with R. Second edition. Shelter Island: Manning. Keogh, Ruth H., Pamela A. Shaw, Paul Gustafson, Raymond J. Carroll, Veronika Deffner, Kevin W. Dodd, Helmut Küchenhoff, et al. 2020. “STRATOS Guidance Document on Measurement Error and Misclassification of Variables in Observational Epidemiology: Part 1-Basic Theory and Simple Methods of Adjustment.” Statistics in Medicine, April. https://doi.org/10.1002/sim.8532. King, Madeleine T. 2011. “A Point of Minimal Important Difference (MID): A Critique of Terminology and Methods.” Expert Review of Pharmacoeconomics &amp; Outcomes Research 11 (2): 171–84. https://doi.org/10.1586/erp.11.9. Kleinberg, Jon, Annie Liang, and Sendhil Mullainathan. 2017. “The Theory Is Predictive, but Is It Complete? An Application to Human Perception of Randomness.” arXiv:1706.06974 [Cs, Stat], June. http://arxiv.org/abs/1706.06974. Kleinberg, Samantha. 2018. Causality, Probability, and Time. ———. 2015. Why: A Guide to Finding and Using Causes. 1 edition. Beijing ; Boston: O’Reilly Media. Kruschke, John K. 2013. “Bayesian Estimation Supersedes the T Test.” Journal of Experimental Psychology: General 142 (2): 573–603. https://doi.org/10.1037/a0029146. Kruschke, John K., and Torrin M. Liddell. 2018a. “Bayesian Data Analysis for Newcomers.” Psychonomic Bulletin &amp; Review 25 (1): 155–77. https://doi.org/10.3758/s13423-017-1272-1. ———. 2018b. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” Psychonomic Bulletin &amp; Review 25 (1): 178–206. https://doi.org/10.3758/s13423-016-1221-4. Kuhn, Max. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret. Kuhn, Max, and Kjell Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Milton: CRC Press LLC. ———. 2018. Applied Predictive Modeling. 1st ed. 2013, Corr. 2nd printing 2016 edition. New York: Springer. Kuhn, Max, Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, et al. 2018. Caret: Classification and Regression Training. Lakens, Daniël. 2017. “Equivalence Tests: A Practical Primer for T Tests, Correlations, and Meta-Analyses.” Social Psychological and Personality Science 8 (4): 355–62. https://doi.org/10.1177/1948550617697177. Lakens, Daniël, Anne M. Scheel, and Peder M. Isager. 2018. “Equivalence Testing for Psychological Research: A Tutorial.” Advances in Methods and Practices in Psychological Science 1 (2): 259–69. https://doi.org/10.1177/2515245918770963. Lang, Kyle M., Shauna J. Sweet, and Elizabeth M. Grandfield. 2017. “Getting Beyond the Null: Statistical Modeling as an Alternative Framework for Inference in Developmental Science.” Research in Human Development 14 (4): 287–304. https://doi.org/10.1080/15427609.2017.1371567. Lang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903. Lang, Michel, Helena Kotthaus, Peter Marwedel, Claus Weihs, Joerg Rahnenfuehrer, and Bernd Bischl. 2014. “Automatic Model Selection for High-Dimensional Survival Analysis.” Journal of Statistical Computation and Simulation 85 (1): 62–76. Lantz, Brett. 2019. Machine Learning with R: Expert Techniques for Predictive Modeling. Lederer, David J., Scott C. Bell, Richard D. Branson, James D. Chalmers, Rachel Marshall, David M. Maslove, David E. Ost, et al. 2019. “Control of Confounding and Reporting of Results in Causal Inference Studies. Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals.” Annals of the American Thoracic Society 16 (1): 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS. Lederer, Wolfgang, and Helmut Küchenhoff. 2006. “A Short Introduction to the SIMEX and MCSIMEX.” R News 6 (January). Ludbrook, John. 1997. “SPECIAL ARTICLE COMPARING METHODS OF MEASUREMENT.” Clinical and Experimental Pharmacology and Physiology 24 (2): 193–203. https://doi.org/10.1111/j.1440-1681.1997.tb01807.x. ———. 2002. “Statistical Techniques for Comparing Measurers and Methods of Measurement: A Critical Review.” Clinical and Experimental Pharmacology and Physiology 29 (7): 527–36. https://doi.org/10.1046/j.1440-1681.2002.03686.x. ———. 2010. “Linear Regression Analysis for Comparing Two Measurers or Methods of Measurement: But Which Regression?: Linear Regression for Comparing Methods.” Clinical and Experimental Pharmacology and Physiology 37 (7): 692–99. https://doi.org/10.1111/j.1440-1681.2010.05376.x. ———. 2012. “A Primer for Biomedical Scientists on How to Execute Model II Linear Regression Analysis: Model II Linear Regression Analysis.” Clinical and Experimental Pharmacology and Physiology 39 (4): 329–35. https://doi.org/10.1111/j.1440-1681.2011.05643.x. Lübke, Karsten, Matthias Gehrke, Jörg Horst, and Gero Szepannek. 2020. “Why We Should Teach Causal Inference: Examples in Linear Regression with Simulated Data.” Journal of Statistics Education, May, 1–7. https://doi.org/10.1080/10691898.2020.1752859. Makowski, Dominique, Mattan Ben-Shachar, and Daniel Lüdecke. 2019a. “bayestestR: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” Journal of Open Source Software 4 (40): 1541. https://doi.org/10.21105/joss.01541. Makowski, Dominique, Mattan S. Ben-Shachar, SH Annabel Chen, and Daniel Lüdecke. n.d. “Indices of Effect Existence and Significance in the Bayesian Framework.” https://doi.org/10.31234/osf.io/2zexr. Makowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019. “BayestestR: Describing Effects and Their Uncertainty, Existence and Significance Within the Bayesian Framework.” Journal of Open Source Software 4 (40): 1541. https://doi.org/10.21105/joss.01541. Makowski, Dominique, Mattan S. Ben-Shachar, and Daniel Lüdecke. 2019b. “Understand and Describe Bayesian Models and Posterior Distributions Using bayestestR.” CRAN. https://doi.org/10.5281/zenodo.2556486. McElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 1 edition. Boca Raton: Chapman and Hall/CRC. McGraw, Kenneth O., and S. P. Wong. 1992. “A Common Language Effect Size Statistic.” Psychological Bulletin 111 (2): 361–65. https://doi.org/10.1037/0033-2909.111.2.361. Miller, Tim. 2017. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv:1706.07269 [Cs], June. http://arxiv.org/abs/1706.07269. Mitchell, Sandra. 2012. Unsimple Truths: Science, Complexity, and Policy. Paperback ed. Chicago, Mich.: The Univ. of Chicago Press. Mitchell, Sandra D. 2002. “Integrative Pluralism.” Biology &amp; Philosophy 17 (1): 55–70. https://doi.org/10.1023/A:1012990030867. Molenaar, Peter C. M. 2004. “A Manifesto on Psychology as Idiographic Science: Bringing the Person Back into Scientific Psychology, This Time Forever.” Measurement: Interdisciplinary Research &amp; Perspective 2 (4): 201–18. https://doi.org/10.1207/s15366359mea0204_1. Molenaar, Peter C. M., and Cynthia G. Campbell. 2009. “The New Person-Specific Paradigm in Psychology.” Current Directions in Psychological Science 18 (2): 112–17. https://doi.org/10.1111/j.1467-8721.2009.01619.x. Molnar, Christoph. 2018. Interpretable Machine Learning. Leanpub. Molnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “Iml: An R Package for Interpretable Machine Learning.” JOSS 3 (26): 786. https://doi.org/10.21105/joss.00786. Morey, Richard D., Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers. 2016. “The Fallacy of Placing Confidence in Confidence Intervals.” Psychonomic Bulletin &amp; Review 23 (1): 103–23. https://doi.org/10.3758/s13423-015-0947-8. Mullineaux, David R., Christopher A. Barnes, and Alan M. Batterham. 1999. “Assessment of Bias in Comparing Measurements: A Reliability Example.” Measurement in Physical Education and Exercise Science 3 (4): 195–205. https://doi.org/10.1207/s15327841mpee0304_1. Müller, Kirill, and Hadley Wickham. 2020. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble. Nevill, Alan M., A. Mark Williams, Colin Boreham, Eric S. Wallace, Gareth W. Davison, Grant Abt, Andrew M. Lane, and Edward M. Winter EDITORIAL BOARD. 2018. “Can We Trust ‘Magnitude-Based Inference’?” Journal of Sports Sciences 36 (24): 2769–70. https://doi.org/10.1080/02640414.2018.1516004. Norman, Geoffrey R., Femida Gwadry Sridhar, Gordon H. Guyatt, and Stephen D. Walter. 2001. “Relation of Distribution- and Anchor-Based Approaches in Interpretation of Changes in Health-Related Quality of Life:” Medical Care 39 (10): 1039–47. https://doi.org/10.1097/00005650-200110000-00002. Novick, Melvin R. 1966. “The Axioms and Principal Results of Classical Test Theory.” Journal of Mathematical Psychology 3 (1): 1–18. https://doi.org/10.1016/0022-2496(66)90002-2. O’Hagan, Tony. 2004. “Dicing with the Unknown.” Significance 1 (3): 132–33. https://doi.org/10.1111/j.1740-9713.2004.00050.x. Page, Scott E. 2018. The Model Thinker: What You Need to Know to Make Data Work for You. Basic Books. Pearl, Judea. 2009. “Causal Inference in Statistics: An Overview.” Statistics Surveys 3 (0): 96–146. https://doi.org/10.1214/09-SS057. ———. 2019. “The Seven Tools of Causal Inference, with Reflections on Machine Learning.” Communications of the ACM 62 (3): 54–60. https://doi.org/10.1145/3241036. Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. 1 edition. Chichester, West Sussex: Wiley. Pearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. 1 edition. New York: Basic Books. Probst, Philipp, Quay Au, Giuseppe Casalicchio, Clemens Stachl, and Bernd Bischl. 2017. “Multilabel Classification with R Package Mlr.” arXiv Preprint arXiv:1703.08991. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. ———. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reshef, D. N., Y. A. Reshef, H. K. Finucane, S. R. Grossman, G. McVean, P. J. Turnbaugh, E. S. Lander, M. Mitzenmacher, and P. C. Sabeti. 2011. “Detecting Novel Associations in Large Data Sets.” Science 334 (6062): 1518–24. https://doi.org/10.1126/science.1205438. Revelle, William. 2019. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University. https://CRAN.R-project.org/package=psych. Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier.” arXiv:1602.04938 [Cs, Stat], February. http://arxiv.org/abs/1602.04938. Rohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629. Rousselet, Guillaume A., Cyril R. Pernet, and Rand R. Wilcox. 2017. “Beyond Differences in Means: Robust Graphical Methods to Compare Two Groups in Neuroscience.” European Journal of Neuroscience 46 (2): 1738–48. https://doi.org/10.1111/ejn.13610. Rousselet, Guillaume A, Cyril R Pernet, and Rand R. Wilcox. 2019a. “A Practical Introduction to the Bootstrap: A Versatile Method to Make Inferences by Using Data-Driven Simulations.” https://doi.org/10.31234/osf.io/h8ft7. ———. 2019b. “The Percentile Bootstrap: A Teaser with Step-by-Step Instructions in R.” https://doi.org/10.31234/osf.io/kxarf. RStudio Team. 2016. RStudio: Integrated Development Environment for R. Boston, MA: RStudio, Inc. Saddiki, Hachem, and Laura B. Balzer. 2018. “A Primer on Causality in Data Science.” arXiv:1809.02408 [Stat], September. http://arxiv.org/abs/1809.02408. Sainani, Kristin L. 2012. “Clinical Versus Statistical Significance.” PM&amp;R 4 (6): 442–45. https://doi.org/10.1016/j.pmrj.2012.04.014. ———. 2018. “The Problem with \"Magnitude-Based Inference\".” Medicine and Science in Sports and Exercise 50 (10): 2166–76. https://doi.org/10.1249/MSS.0000000000001645. Sainani, Kristin L., Keith R. Lohse, Paul Remy Jones, and Andrew Vickers. 2019. “Magnitude-Based Inference Is Not Bayesian and Is Not a Valid Method of Inference.” Scandinavian Journal of Medicine &amp; Science in Sports, May. https://doi.org/10.1111/sms.13491. Sarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with R. New York: Springer. http://lmdvr.r-forge.r-project.org. Savage, Leonard J. 1972. The Foundations of Statistics. 2nd Revised ed. edition. New York: Dover Publications. Shang, Yi. 2012. “Measurement Error Adjustment Using the SIMEX Method: An Application to Student Growth Percentiles: Measurement Error Adjustment Using the SIMEX Method.” Journal of Educational Measurement 49 (4): 446–65. https://doi.org/10.1111/j.1745-3984.2012.00186.x. Shaw, Pamela A., Paul Gustafson, Raymond J. Carroll, Veronika Deffner, Kevin W. Dodd, Ruth H. Keogh, Victor Kipnis, et al. 2020. “STRATOS Guidance Document on Measurement Error and Misclassification of Variables in Observational Epidemiology: Part 2-More Complex Methods of Adjustment and Advanced Topics.” Statistics in Medicine, April. https://doi.org/10.1002/sim.8531. Shmueli, Galit. 2010. “To Explain or to Predict?” Statistical Science 25 (3): 289–310. https://doi.org/10.1214/10-STS330. Shrier, Ian, and Robert W Platt. 2008. “Reducing Bias Through Directed Acyclic Graphs.” BMC Medical Research Methodology 8 (1). https://doi.org/10.1186/1471-2288-8-70. Swinton, Paul A., Ben Stephens Hemingway, Bryan Saunders, Bruno Gualano, and Eimear Dolan. 2018. “A Statistical Framework to Interpret Individual Response to Intervention: Paving the Way for Personalized Nutrition and Exercise Prescription.” Frontiers in Nutrition 5 (May). https://doi.org/10.3389/fnut.2018.00041. Tenan, Matthew, Andrew David Vigotsky, and Aaron R Caldwell. n.d. “On the Statistical Properties of the Dankel-Loenneke Method.” https://doi.org/10.31236/osf.io/8ndhg. Textor, Johannes, Benito van der Zander, Mark S. Gilthorpe, Maciej Li’skiewicz, and George T. H. Ellison. 2017. “Robust Causal Inference Using Directed Acyclic Graphs: The R Package ‘Dagitty’.” International Journal of Epidemiology, January, dyw341. https://doi.org/10.1093/ije/dyw341. Therneau, Terry, and Beth Atkinson. 2019. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. Turner, Anthony, Jon Brazier, Chris Bishop, Shyam Chavda, Jon Cree, and Paul Read. 2015. “Data Analysis for Strength and Conditioning Coaches: Using Excel to Analyze Reliability, Differences, and Relationships.” Strength and Conditioning Journal 37 (1): 76–83. https://doi.org/10.1519/SSC.0000000000000113. Vaughan, Davis, and Max Kuhn. 2020. Hardhat: Construct Modeling Packages. https://CRAN.R-project.org/package=hardhat. Wagenmakers, Eric-Jan. 2007. “A Practical Solution to the Pervasive Problems Ofp Values.” Psychonomic Bulletin &amp; Review 14 (5): 779–804. https://doi.org/10.3758/BF03194105. Wallace, Michael. 2020. “Analysis in an Imperfect World.” Significance 17 (1): 14–19. https://doi.org/10.1111/j.1740-9713.2020.01353.x. Watts, Duncan J, Emorie D Beck, Elisa Jayne Bienenstock, Jake Bowers, Aaron Frank, Anthony Grubesic, Jake Hofman, Julia Marie Rohrer, and Matthew Salganik. 2018. “Explanation, Prediction, and Causality: Three Sides of the Same Coin?” October. https://doi.org/10.31219/osf.io/u6vz5. Weinberg, Gabriel, and Lauren McCann. 2019. Super Thinking: The Big Book of Mental Models. New York: Portfolio/Penguin. Welsh, Alan H., and Emma J. Knight. 2015. “‘Magnitude-Based Inference’: A Statistical Review.” Medicine &amp; Science in Sports &amp; Exercise 47 (4): 874–84. https://doi.org/10.1249/MSS.0000000000000451. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr. ———. 2020. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, and Lionel Henry. 2020. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr. Wickham, Hadley, Jim Hester, and Romain Francois. 2018. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr. Wikipedia contributors. 2019. “Causal Model.” Wilcox, Rand, Travis J. Peterson, and Jill L. McNitt-Gray. 2018. “Data Analyses When Sample Sizes Are Small: Modern Advances for Dealing with Outliers, Skewed Distributions, and Heteroscedasticity.” Journal of Applied Biomechanics 34 (4): 258–61. https://doi.org/10.1123/jab.2017-0269. Wilcox, Rand R. 2016. Introduction to Robust Estimation and Hypothesis Testing. 4th edition. Waltham, MA: Elsevier. Wilcox, Rand R., and Guillaume A. Rousselet. 2017. “A Guide to Robust Statistical Methods in Neuroscience.” bioRxiv, June. https://doi.org/10.1101/151811. Wilke, Claus O. 2019. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://CRAN.R-project.org/package=cowplot. ———. 2020. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges. Willmott, Cj, and K Matsuura. 2005. “Advantages of the Mean Absolute Error (MAE) over the Root Mean Square Error (RMSE) in Assessing Average Model Performance.” Climate Research 30: 79–82. https://doi.org/10.3354/cr030079. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/. ———. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown. Yarkoni, Tal, and Jacob Westfall. 2017. “Choosing Prediction over Explanation in Psychology: Lessons from Machine Learning.” Perspectives on Psychological Science 12 (6): 1100–1122. https://doi.org/10.1177/1745691617693393. Zhao, Qingyuan, and Trevor Hastie. 2019. “Causal Interpretations of Black-Box Models.” Journal of Business &amp; Economic Statistics, July, 1–10. https://doi.org/10.1080/07350015.2019.1624293. Zhu, Hao. 2019. KableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra. "]
]
